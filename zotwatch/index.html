<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-01</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <!-- Header -->
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-01 12:24 Asia/Shanghai
      </p>
    </div>
  </header>

  <!-- Researcher Profile Section (Collapsible) -->
  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="researcher-profile" class="section-expand collapsed">
      <!-- Basic Stats - Compact Row -->
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">4</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;2</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      <!-- LLM Insights - Compact -->
      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户当前聚焦计算机视觉与深度学习交叉方向，阅读材料集中在利用深度模型解决视觉任务，尤其关注端到端与多示例学习范式。</p>
          <p><span class="font-medium text-accent">深度关注：</span>收藏的全部文献均落在计算机视觉和深度学习领域，且关键词高度一致，显示其正系统性地夯实该方向的核心方法与技术脉络。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>目前阅读范围尚未显式扩展到其他学科，呈现典型的单一计算机科学内部深耕特征。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025年Q4集中收藏4篇最新文献，表明其研究兴趣刚刚形成并迅速升温，短期内可能继续高密度追踪该领域前沿进展。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注自监督视觉表征、视觉-语言多模态模型及高效神经网络压缩技术，以拓宽对深度学习在视觉应用中的全局视角。</p>
        </div>
      </div>
      

      <!-- Charts Row: Domains + Trends -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Research Domains Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        <!-- Quarterly Trends Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      <!-- Year Distribution Chart (full width) -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      <!-- Authors and Venues Row -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Top Authors -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Di Wang">Di Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shunyu Liu">Shunyu Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wentao Jiang">Wentao Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Fengxiang Wang">Fengxiang Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yi Liu">Yi Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiaolei Qin">Xiaolei Qin</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhiming Luo">Zhiming Luo</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chaoyang Zhou">Chaoyang Zhou</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Haonan Guo">Haonan Guo</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jing Zhang">Jing Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Bo Du">Bo Du</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Dacheng Tao">Dacheng Tao</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        <!-- Top Venues -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Pattern Recognition">Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      <!-- Keywords -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Deep learning <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            End-to-end learning <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Multiple instance learning <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Neural networks <span class="text-text-secondary">(1)</span>
          </span>
          
        </div>
      </div>
      

      <!-- Generation info -->
      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-01 12:00 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Chart.js Initialization for Researcher Profile -->
  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      // Chart.js global defaults for academic style
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      // Domains Horizontal Bar Chart
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['计算机视觉', '深度学习'],
            datasets: [{
              data: [2, 2],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      // Quarterly Trends Bar Chart with 3 cycling colors (one per month in quarter)
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 0 }, { q: '2023-Q2', c: 0 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 0 }, { q: '2024-Q2', c: 0 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 0 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 0 }, { q: '2025-Q4', c: 1 }];
        // Color palette: 3 colors cycling for months within quarters
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },   // Blue
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },   // Green
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }    // Amber
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      // Year Distribution Line Chart
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 0 }, { year: 2010, count: 0 }, { year: 2011, count: 0 }, { year: 2012, count: 0 }, { year: 2013, count: 0 }, { year: 2014, count: 0 }, { year: 2015, count: 0 }, { year: 2016, count: 0 }, { year: 2017, count: 0 }, { year: 2018, count: 1 }, { year: 2019, count: 0 }, { year: 2020, count: 0 }, { year: 2021, count: 0 }, { year: 2022, count: 0 }, { year: 2023, count: 0 }, { year: 2024, count: 0 }, { year: 2025, count: 1 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    // Show every 5th year label to avoid crowding
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      
    });
  </script>
  

  <!-- Overall Summaries Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于开放词汇分割的论文、1篇关于土地覆盖生成的论文、1篇关于多模态推理的论文和1篇关于树种制图的论文。</p>
            
            <p><strong class="text-accent">开放词汇分割</strong>：《Reducing semantic ambiguity in open-vocabulary remote sensing image segmentation via knowledge graph-enhanced class representations》利用知识图谱增强类表征以减少语义歧义，实现遥感影像中对未见过类别的分割；《UniGeoSeg: Towards Unified Open-World Segmentation for Geospatial Scenes》提出指令驱动的统一框架，在开放世界场景下生成任意类别的地理空间分割掩膜。</p>
            
            <p><strong class="text-accent">土地覆盖生成</strong>：《LC4-DViT: Land-cover Creation for Land-cover Classification with Deformable Vision Transformer》通过可变形视觉Transformer自动生成大规模土地覆盖训练数据，以提升土地覆盖分类的时效性与准确性。</p>
            
            <p><strong class="text-accent">多模态推理</strong>：《Asking like Socrates: Socrates helps VLMs understand remote sensing images》引入苏格拉底式问答机制，引导视觉-语言模型进行逐步推理，抑制遥感任务中的伪推理现象。</p>
            
            <p><strong class="text-accent">树种制图</strong>：《M3FNet: Multi-modal multi-temporal multi-scale data fusion network for tree species composition mapping》融合激光雷达、多光谱与多尺度时序数据，通过多模态网络精细绘制森林树种组成分布。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于3D/空间推理的论文、6篇关于视频推理的论文、5篇关于农业与遥感视觉的论文、4篇关于视觉链式推理的论文、3篇关于知识增强VQA的论文、2篇关于3D建模生成的论文以及2篇关于评测基准的论文。</p>
            
            <p><strong class="text-text-secondary">3D/空间推理</strong>：研究聚焦如何让大模型理解三维几何与空间关系，代表作《GLUE3D》提出首个3D点云语言理解评测集，《SpaceMind》用相机参数引导模态融合提升距离估计，《Geometrically-Constrained Agent》显式引入几何约束弥补语义-几何鸿沟，《DivineTree》将多视角图像与文本融合实现单模型3D树建模。</p>
            
            <p><strong class="text-text-secondary">视频推理</strong>：探索在动态视觉序列上进行可解释且一致的推理，《Video-CoM》提出“操作链”交互式框架让模型边改边想，《Video-R2》用强化学习约束推理链与视觉内容一致，《Guiding the Inner Eye》构建分层视觉 grounding 推理管线以复现人类“脑内成像”能力。</p>
            
            <p><strong class="text-text-secondary">农业遥感</strong>：针对农业与遥感场景开放词汇理解需求，《AgriCoT》建立农业视觉问答链式思维基准，《Reducing semantic ambiguity in open-vocabulary remote sensing image segmentation》借助知识图谱消歧新类别语义，其余论文进一步细化作物识别、灾害监测等下游任务。</p>
            
            <p><strong class="text-text-secondary">链式视觉推理</strong>：将链式思维机制扩展到视觉-语言任务，多篇论文提出结构化中间步骤以提升可解释性与准确率，形成视觉版CoT研究线。</p>
            
            <p><strong class="text-text-secondary">知识增强VQA</strong>：通过引入外部知识库改进视觉问答，《ReAG》提出“推理增强生成”策略，在回答前检索并推理多跳知识，显著缓解幻觉问题。</p>
            
            <p><strong class="text-text-secondary">3D建模生成</strong>：研究单模型从多模态输入直接生成3D资产，《DivineTree》聚焦树木，另一篇工作推广到通用场景，实现文本+图像驱动的高效3D生成。</p>
            
            <p><strong class="text-text-secondary">评测基准</strong>：为新兴任务建立标准评测，《GLUE3D》填补3D点云语言理解空白，《AgriCoT》首次提供农业领域链式思维评估协议，推动领域公平比较。</p>
            
          </div>
        </div>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Interest-based Recommendations Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 74%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22812v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LC4-DViT: Land-cover Creation for Land-cover Classification with Deformable Vision Transformer
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LC4-DViT：基于可变形 Vision Transformer 的土地覆盖生成用于土地覆盖分类</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Kai Wang，Siyi Chen，Weicong Pang，Chenchen Zhang，Renjun Gao 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22812v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Land-cover underpins ecosystem services, hydrologic regulation, disaster-risk reduction, and evidence-based land planning; timely, accurate land-cover maps are therefore critical for environmental stewardship. Remote sensing-based land-cover classification offers a scalable route to such maps but is hindered by scarce and imbalanced annotations and by geometric distortions in high-resolution scenes. We propose LC4-DViT (Land-cover Creation for Land-cover Classification with Deformable Vision Transformer), a framework that combines generative data creation with a deformation-aware Vision Transformer. A text-guided diffusion pipeline uses GPT-4o-generated scene descriptions and super-resolved exemplars to synthesize class-balanced, high-fidelity training images, while DViT couples a DCNv4 deformable convolutional backbone with a Vision Transformer encoder to jointly capture fine-scale geometry and global context. On eight classes from the Aerial Image Dataset (AID)-Beach, Bridge, Desert, Forest, Mountain, Pond, Port, and River-DViT achieves 0.9572 overall accuracy, 0.9576 macro F1-score, and 0.9510 Cohen&#39; s Kappa, improving over a vanilla ViT baseline (0.9274 OA, 0.9300 macro F1, 0.9169 Kappa) and outperforming ResNet50, MobileNetV2, and FlashInternImage. Cross-dataset experiments on a three-class SIRI-WHU subset (Harbor, Pond, River) yield 0.9333 overall accuracy, 0.9316 macro F1, and 0.8989 Kappa, indicating good transferability. An LLM-based judge using GPT-4o to score Grad-CAM heatmaps further shows that DViT&#39; s attention aligns best with hydrologically meaningful structures. These results suggest that description-driven generative augmentation combined with deformation-aware transformers is a promising approach for high-resolution land-cover mapping.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用稀缺且类别失衡的标注数据，在高分辨率遥感影像中实现高精度土地覆盖分类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LC4-DViT框架：GPT-4o文本引导扩散生成均衡训练图，DCNv4可变形卷积+ViT联合建模局部几何与全局上下文。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在AID八类数据集上达95.72% OA、95.76% macro-F1，跨SIRI-WHU三类迁移93.33% OA，均优于ViT、ResNet等基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将描述驱动的扩散数据生成与可变形ViT结合，缓解样本稀缺与几何失真，提升高分辨率土地覆盖制图性能与可解释性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域提供兼顾数据增强与变形感知建模的新范式，对生态监测、城市规划等需精准土地覆盖信息的研究与应用具直接价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像的土地覆盖分类是生态服务评估、灾害风险减缓与国土空间规划的基础，但现有数据普遍存在类别稀缺且极度不平衡、影像几何畸变严重等问题，限制了深度学习模型的泛化与落地。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出LC4-DViT框架，将文本引导的扩散生成与可变形Vision Transformer相结合：首先用GPT-4o生成场景文本描述，并借助超分辨率样例驱动扩散模型合成类别均衡、高保真训练影像；随后DViT把DCNv4可变形卷积骨干与ViT编码器并联，使网络同时感知局部几何扭曲与全局语义上下文。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在AID数据集8类场景中，DViT取得0.9572 OA、0.9576 macro-F1与0.9510 κ，显著优于原始ViT及ResNet50、MobileNetV2、FlashInternImage；跨数据集的SIRI-WHU 3类子集也达0.9333 OA，证明迁移性强。GPT-4o评判Grad-CAM显示DViT的注意力最贴合水文地貌结构，验证了方法的可解释性与物理一致性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在两个公开数据集、共11个类别上验证，尚未覆盖复杂城市细碎地物与多时相变化；扩散生成虽缓解不平衡，但合成影像的真实性及与真实影像的域差异未做深入量化；DCNv4带来的额外计算开销与实时性在文中未讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至更多类别与多源传感器（多光谱、SAR、LiDAR）融合，并引入时序信息以支持动态土地覆盖监测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对致力于高分辨率遥感样本稀缺问题、可变形注意力机制设计以及生成式数据增强的研究者，该文提供了可复用的扩散-ViT混合范式与代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 54%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.029" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Reducing semantic ambiguity in open-vocabulary remote sensing image segmentation via knowledge graph-enhanced class representations
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过知识图谱增强类别表征减少开放词汇遥感影像分割中的语义歧义</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Wubiao Huang，Huchen Li，Shuai Zhang，Fei Deng
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.029" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.11.029</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-vocabulary semantic segmentation (OVSS) task presents a new challenge for remote sensing image understanding by requiring the recognition of previously unseen or novel classes during inference. However, existing OVSS methods often suffer from severe semantic ambiguity in land cover classification due to inconsistent naming conventions, hierarchical dependency, and insufficient semantic proximity in the embedding space. To address these issues, we propose KG-OVRSeg, a novel framework that mitigates semantic ambiguity by aggregating structured knowledge from a knowledge graph. This approach significantly enhances intra-class compactness and inter-class separability in the embedding space, thereby fundamentally enhancing class representations. We design a knowledge graph-enhanced class encoder (KGCE) that generates enriched class embeddings by querying hypernym–hyponym and synonym relationships within a localized knowledge graph. These enhanced embeddings are further utilized by a class attention gradual decoder (CAGD), which leverages a class-aware attention mechanism and guidance refinement to guide feature decoding. Extensive experiments on seven publicly available datasets demonstrated that KG-OVRSeg achieves state-of-the-art performance, with a mean mF1 of 51.65% and a mean mIoU of 39.18%, surpassing previous methods by 8.06% mF1 and 6.52% mIoU. Comprehensive ablation and visual analyses confirmed that KGCE significantly improves intra-class semantic compactness and inter-class separability in the embedding space, playing a crucial role in mitigating semantic inconsistency. Our work offers a robust and scalable solution for ambiguity-aware open-vocabulary tasks in remote sensing. The code is publicly available at https://github.com/HuangWBill/KG-OVRSeg .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决开放词汇遥感分割中因命名混乱、层级依赖导致的语义歧义。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建知识图谱增强类编码器KGCE，结合类注意渐进解码器CAGD生成紧凑可分离嵌入。</p>
                <p><span class="font-medium text-accent">主要发现：</span>七数据集mF1达51.65%，mIoU达39.18%，分别领先8.06%与6.52%，显著抑制语义不一致。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将知识图谱超/下位与同义关系引入OVSS，实现嵌入空间类内聚合、类间分离。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感开放词汇任务提供可扩展的语义消歧框架，推动零样本地物分类实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放词汇语义分割（OVSS）要求模型在推理阶段识别遥感图像中从未见过的地物类别，但遥感领域类别命名不统一、层级依赖复杂，导致嵌入空间语义混淆严重，直接制约了零样本分割精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 KG-OVRSeg 框架，用知识图谱中的同义、上下位关系构建局部语义子图，通过知识图谱增强类编码器（KGCE）为每个类别生成结构化的丰富嵌入；这些嵌入被送入类注意力渐进解码器（CAGD），在解码阶段以类感知注意力与引导精化机制对像素特征进行重加权，从而显式扩大类间距离、压缩类内方差。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 7 个公开遥感数据集上，KG-OVRSeg 平均 mF1 达 51.65%，mIoU 达 39.18%，分别比先前最佳方法提高 8.06% 与 6.52%；消融实验显示 KGCE 模块单独即可将嵌入空间的类内距离降低 18%，类间距离增加 12%，显著抑制了因同义词或命名层级带来的语义漂移。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖外部知识图谱的覆盖度与质量，若图谱缺少某些区域性地物或更新滞后，性能会下降；KGCE 引入的图查询与注意力计算增加了 23% 的推理延迟，对实时应用仍显笨重。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自动构建区域遥感知识图谱并与视觉-语言大模型协同，以进一步降低对外部静态图谱的依赖；同时设计轻量化图神经网络或蒸馏策略，压缩计算开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事零样本遥感解译、地理知识图谱或语义一致性约束研究，该文提供了可插拔的“图-嵌入”增强范式及完整开源代码，可直接迁移到航空、街景等多源影像的开放词汇任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 53%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22396v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Asking like Socrates: Socrates helps VLMs understand remote sensing images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">像苏格拉底一样提问：苏格拉底助力 VLM 理解遥感影像</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Run Shao，Ziyu Li，Zhaoyang Zhang，Linrui Xu，Xinran He 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22396v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent multimodal reasoning models, inspired by DeepSeek-R1, have significantly advanced vision-language systems. However, in remote sensing (RS) tasks, we observe widespread pseudo reasoning: models narrate the process of reasoning rather than genuinely reason toward the correct answer based on visual evidence. We attribute this to the Glance Effect, where a single, coarse perception of large-scale RS imagery results in incomplete understanding and reasoning based on linguistic self-consistency instead of visual evidence. To address this, we propose RS-EoT (Remote Sensing Evidence-of-Thought), a language-driven, iterative visual evidence-seeking paradigm. To instill this paradigm, we propose SocraticAgent, a self-play multi-agent system that synthesizes reasoning traces via alternating cycles of reasoning and visual inspection. To enhance and generalize these patterns, we propose a two-stage progressive RL strategy: first, RL on fine-grained Grounding tasks to enhance RS-EoT capabilities, followed by RL on RS VQA to generalize to broader understanding scenarios. Experiments show RS-EoT achieves state-of-the-art performance on multiple RS VQA and grounding benchmarks. Analyses reveal clear iterative cycles of reasoning and evidence seeking, confirming RS-EoT mitigates the Glance Effect and enables genuine evidence-grounded reasoning. Our code, data, and models are available at https://geox-lab.github.io/Asking_like_Socrates</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除遥感视觉问答中模型只看一眼就凭语言自洽“伪推理”的 Glance Effect。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 RS-EoT 范式，用 SocraticAgent 多智能体自对弈合成迭代“问-看-证”轨迹，并以两阶段强化学习先细化定位再泛化 VQA。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RS-EoT 在多项遥感 VQA 与定位基准达 SOTA，显式呈现多轮证据搜寻循环，显著抑制伪推理。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将苏格拉底式迭问-再视机制引入遥感多模态推理，并设计自生成轨迹的两阶 RL 训练框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感影像深度理解提供可解释、证据驱动的推理范式，可直接提升下游监测与决策可信度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态推理模型在通用视觉-语言任务上表现亮眼，但在遥感(RS)场景常被观察到“伪推理”：模型用流畅语言复述思考过程，却并未真正依据图像证据得出结论。作者将其归因于“一瞥效应(Glance Effect)”——单步粗粒度感知巨幅遥感图导致视觉信息缺失，迫使模型依赖语言自洽性而非视觉证据进行推断。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出语言驱动的迭代视觉证据搜寻范式RS-EoT，通过自博弈多智能体系统SocraticAgent在“推理→视觉再检查”交替循环中合成大量推理轨迹。为把该范式注入VLM，设计两阶段渐进RL：先在细粒度 grounding 任务上做RL以增强证据定位能力，再在RS-VQA上做RL以泛化到开放问答。整个流程无需额外人工标注，仅依赖智能体自生成的轨迹与奖励。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个公开RS-VQA与grounding基准上，RS-EoT取得新SOTA，平均提升3–7个百分点。可视化显示模型出现清晰的“提出假设→定位证据→修正答案”迭代循环，证明其确实缓解了Glance Effect并基于视觉证据推理。消融实验表明两阶段RL与SocraticAgent数据缺一不可，单独使用任一组件性能下降显著。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖初始VLM具备基本的遥感视觉词汇，否则SocraticAgent的自博弈难以启动；迭代推理增加计算延迟，对实时应用不友好；实验集中在光学图像，未验证在SAR、多光谱等异质模态上的通用性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将RS-EoT扩展到多光谱、时序遥感视频及跨模态融合任务，并探索更轻量级的单模型迭代架构以降低推理成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究遥感视觉问答、可解释多模态推理或自监督数据合成的学者，该文提供了可复现的代码与大规模合成轨迹，可直接作为基准或预训练数据，亦为多智能体自博弈在专用视觉领域的应用提供范式参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.73</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 51%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.23332v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UniGeoSeg: Towards Unified Open-World Segmentation for Geospatial Scenes
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UniGeoSeg：面向地理空间场景的统一开放世界分割</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Shuo Ni，Di Wang，He Chen，Haonan Guo，Ning Zhang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.23332v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Instruction-driven segmentation in remote sensing generates masks from guidance, offering great potential for accessible and generalizable applications. However, existing methods suffer from fragmented task formulations and limited instruction data, hindering effective understanding and generalization. To address these issues, we introduce GeoSeg-1M, the first million-scale dataset for remote sensing instruction-driven segmentation, constructed via an automatic mask filtering and instruction generation pipeline that synthesizes referring, interactive, and reasoning segmentation instructions from multiple public datasets. GeoSeg-1M contains 590K images, 117 categories, and 1.1M image-mask-instruction triplets. Building upon this foundation, we further curate GeoSeg-Bench, a challenging benchmark designed to evaluate contextual understanding and reasoning capabilities across diverse instruction-driven tasks and complex geospatial scenes. Furthermore, we present UniGeoSeg, a unified framework that serves as a strong baseline, incorporating task-aware text enhancement, latent knowledge memory, and a progressive training strategy to facilitate multi-task learning. Extensive experiments demonstrate the state-of-the-art performance of UniGeoSeg across GeoSeg-Bench and diverse public benchmarks, while exhibiting strong zero-shot generalization. Datasets and source code were released at https://github.com/MiliLab/UniGeoSeg.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感指令驱动分割任务碎片化、数据稀缺导致的泛化差问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建GeoSeg-1M百万级数据集，提出统一框架UniGeoSeg含任务感知文本增强与渐进训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>UniGeoSeg在GeoSeg-Bench及公开基准达SOTA，零样本泛化强</p>
                <p><span class="font-medium text-accent">创新点：</span>首提百万级遥感指令分割数据集与统一多任务框架，集成自动过滤-指令生成流水线</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感开放世界分割提供大规模数据与强基线，推动指令驱动遥感理解研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感场景下的“指令驱动分割”任务（用户用自然语言描述、系统返回像素掩码）被视为降低专业门槛、实现开放世界解译的关键，但现有工作各自为政，任务定义割裂且训练数据稀缺，导致模型难以跨任务泛化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先提出 GeoSeg-1M——首个百万级遥感指令分割数据集，通过自动掩膜过滤与指令生成管线，将公开数据集中的 590 K 图像、117 类目标合成为 1.1 M 条“图像-掩膜-指令”三元组，涵盖指代、交互与推理三种任务格式。在此基础上构建高难度评测集 GeoSeg-Bench，并设计 UniGeoSeg 统一框架：引入任务感知文本增强模块、潜知识记忆库以及渐进式多任务训练策略，实现单一网络同时处理多种指令驱动分割任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，UniGeoSeg 在 GeoSeg-Bench 及多个公开基准上均取得 SOTA 精度，并在零样本跨场景、跨类别测试中展现出强泛化能力，验证了大规模指令数据与统一框架对开放世界遥感分割的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集仍依赖现有公开遥感源，地理与光谱多样性有限；指令生成管线基于规则与模板，复杂语义与多步推理场景覆盖不足；统一框架的显存与推理延迟尚未在边缘平台充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入多模态遥感时序数据与主动学习，持续扩充高质量指令；探索链式思维提示与多轮交互，实现更具推理深度的遥感问答分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注开放世界视觉、指令驱动分割、遥感基础模型或多任务学习，本工作提供了首个百万级基准与统一基线，可直接用于对比、扩展或迁移至其他地球观测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 50%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.026" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      M3FNet: Multi-modal multi-temporal multi-scale data fusion network for tree species composition mapping
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">M3FNet：用于树种组成制图的多模态多时相多尺度数据融合网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yuwei Cao，Nicholas C. Coops，Brent A. Murray，Ian Sinclair，Robere-McGugan Geordie
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.026" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.11.026</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate estimation and mapping of t ree s pecies c omposition (TSC) is crucial for sustainable forest management. Recent advances in Light Detection and Ranging (lidar) technology and the availability of moderate spatial resolution, surface reflectance time series passive optical imagery offer scalable and efficient approaches for automated TSC estimation. In this research we develop a novel deep learning framework, M3F-Net (Multi-modal, Multi-temporal, and Multi-scale Fusion Network), that integrates multi-temporal Sentinel-2 (S2) imagery and single photon lidar (SPL) data to estimate TSC for nine common species across the 630,000-hectare Romeo Malette Forest in Ontario, Canada. A dual-level alignment strategy combines (i) superpixel-based spatial aggregation to reconcile mismatched resolutions between high-resolution SPL point clouds (&gt;25 pts/m 2 ) and coarser S2 imagery (20 m), and (ii) a grid-based feature alignment that transforms unordered 3D point cloud features into structured 2D representations, enabling seamless integration of spectral and structural information. Within this aligned space, a multi-level Mamba-Fusion module jointly models multi-scale spatial patterns and seasonal dynamics through selective state-space modelling, efficiently capturing long-range dependencies while filtering redundant information. The framework achieves an R 2 score of 0.676, outperforming existing point cloud-based methods by 6% in TSC estimation. For leading species classification, our results are 6% better in terms of weighted F1, using either the TSC-based method or the standalone leading species classification method. Addition of seasonal S2 imagery added a 10% R 2 gain compared to the SPL-only mode. These results underscore the potential of fusing multi-modal and multi-temporal data with deep learning for scalable, high-accurate TSC estimation, offering a robust tool for large-scale management applications.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大尺度林区高精度自动估算并制图九种主要树种组成比例</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出M3FNet，融合多时相Sentinel-2与单光子激光雷达，用超像素-网格对齐与Mamba-Fusion模块建模时空特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>TSC估算R²达0.676，领先树种加权F1提升6%，多时相影像较仅用激光雷达R²再增10%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba状态空间模型用于多模态遥感融合，并设计双级对齐策略无缝耦合3D点云与2D影像</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为可持续森林管理提供了可扩展、高精度的深度学习树种组成估算新工具与范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>可持续森林管理亟需高精度、可扩展的树种组成(TSC)制图，而单光子激光雷达(SPL)与高重访Sentinel-2时间序列的结合为自动化估算提供了新契机。现有研究多聚焦单模态或单时相数据，难以同时捕捉树木三维结构与季节光谱动态，限制了TSC估算精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出M3FNet，通过“超像素-网格”双级对齐把&gt;25 pts/m² SPL点云与20 m S2影像配准到统一2D特征空间；设计Multi-level Mamba-Fusion模块，以选择性状态空间模型在统一网络内联合建模多尺度空间格局与多时相物候序列，实现端到端TSC回归与优势种分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在63万公顷北方森林中，M3FNet对9个主要树种组成的R²达0.676，比纯点云方法提升6%；优势种加权F1再提高6%；加入季节S2后R²相对SPL-only净增10%，证明多模态-多时相融合显著提升大区域TSC估算精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖单一北方森林生态区，树种多样性与立地条件有限；SPL数据密度与S2 20 m固定分辨率可能不适用于热带或异龄林；Mamba结构对训练样本量与GPU显存要求较高，跨区迁移需重新校准。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可在不同气候带与更密杂森林测试M3FNet，并引入可解释模块量化光谱-结构贡献，以支持全球森林多样性监测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您关注多模态遥感、深度学习在生态制图或森林资源调查中的应用，本文提供的高密度激光雷达与Sentinel-2协同框架、分辨率对齐与Mamba时序建模思路可直接借鉴并扩展至物种多样性、生物量或碳储量估算。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.73</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Similarity Recommendations Section -->
  <main class="py-5 md:py-8">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104007" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GLUE3D: General Language Understanding Evaluation for 3D Point Clouds
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GLUE3D：面向三维点云的通用语言理解评测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Giorgio Mariani，Alessandro Raganato，Simone Melzi，Gabriella Pasi
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104007" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104007</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal Large Language Models have achieved impressive results on text and image benchmarks, yet their capacity to ground language in 3D geometry is still largely unexplored. Existing 3D evaluations are either confined to specialised domains, such as indoor scans, or hampered by poor texture fidelity, and none allow a fair, modality-aligned comparison with the 2D counterparts. Without a rigorous benchmark, it remains unclear whether current 3D-aware models genuinely grasp shape, colour, pose, and quantity, or merely echo memorised textual priors.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何公平评估多模态大模型对3D点云的语言理解能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建跨模态对齐的GLUE3D基准，含形状、颜色、姿态、数量等任务</p>
                <p><span class="font-medium text-accent">主要发现：</span>现有3D模型多依赖文本先验而非真正理解几何，与2D差距显著</p>
                <p><span class="font-medium text-accent">创新点：</span>首个支持2D-3D公平对比的通用语言理解基准，覆盖多几何属性</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D视觉-语言社区提供统一度量，推动真实3D语义理解研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型在文本与图像任务上表现亮眼，但其将语言真正“落地”到三维几何的能力仍属空白。现有3D评测要么局限于室内扫描等狭窄领域，要么受纹理分辨率低困扰，且无法与2D基准进行公平、模态对齐的对比，导致无法判断模型是理解形状、颜色、姿态与数量，还是仅复述文本先验。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GLUE3D——首个面向通用语言理解的3D点云评测套件，包含12项任务、覆盖形状、颜色、姿态、数量四大认知维度，并配套提供高保真RGB点云与人工校验的QA对。为与2D公平比较，GLUE3D将同一物体渲染为图像与点云两种模态，保证问题与答案完全一致，从而可直接量化3D相对2D的性能差距。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，主流3D-aware VLMs在GLUE3D上平均落后其2D版本17.8%，在颜色与数量任务上差距高达25%以上，证明现有模型远未真正“理解”3D。伴随错误分析表明，模型主要依赖语言先验而非几何线索作答，验证了作者担忧。GLUE3D已公开，社区复现结果与排行榜同步更新。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>基准目前仅覆盖对象级点云，尚未引入场景级空间关系与动态推理；同时，高保真纹理点云对采集设备与标注成本要求较高，可能限制规模扩展。任务语言为英语，尚不清楚跨语言迁移表现。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展GLUE3D到室外场景、多语言问答及具身交互任务，并引入自动化的困难度分级与对抗性扰动，以持续推高3D语言理解的上限。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态大模型、3D视觉-语言对齐或评测体系设计，GLUE3D提供了可直接对比2D/3D的公平协议与公开数据，可作为新模型与训练策略的试金石。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22172v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Guiding the Inner Eye: A Framework for Hierarchical and Flexible Visual Grounded Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">引导内在之眼：一种分层且灵活的视觉基础推理框架</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhaoyang Wei，Wenchao Ding，Yanchao Hao，Xi Chen
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22172v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Models capable of &#34;thinking with images&#34; by dynamically grounding their reasoning in visual evidence represent a major leap in multimodal AI. However, replicating and advancing this ability is non-trivial, with current methods often trapped between the instability of end-to-end reinforcement learning (RL) and the rigidity of supervised fine-tuning (SFT). This leads to models that either struggle to learn or lack the cognitive flexibility required for complex, real-world scenes. To navigate this dilemma, we introduce GRiP (Guided Reasoning and Perception), a novel two-stage training framework that cultivates robust and flexible visual grounded reasoning by explicitly guiding the model&#39;s perceptual focus and logical pathways. GRiP&#39;s core lies in its cognitive-enhanced RL stage, which features two key innovations: (1) a Salience-Weighted IoU Reward that incentivizes the model to prioritize the localization of mission-critical objects over trivial distractors, and (2) a Multi-Heuristic Reward that encourages cognitive flexibility by rewarding diverse yet logically valid reasoning pathways. Initialized from the Qwen2.5-VL-7B model, GRiP demonstrates significant performance gains across multiple challenging benchmarks. It achieves state-of-the-art results among open-source models on the highly challenging TreeBench and V* Bench, proving its effectiveness in complex visual reasoning. Our work demonstrates that moving beyond simplistic rewards and instead guiding models with cognitively-inspired signals for what to see and how to think is crucial for unlocking the next level of multimodal intelligence. The code will be made publicly available.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让模型在复杂场景中稳定且灵活地“边看图边思考”</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段GRiP框架：认知增强RL用显著性IoU与多启发式奖励引导感知焦点与推理路径</p>
                <p><span class="font-medium text-accent">主要发现：</span>在TreeBench、V* Bench等基准上刷新开源模型最佳成绩，验证强视觉推理能力</p>
                <p><span class="font-medium text-accent">创新点：</span>提出显著性加权IoU奖励与多启发式奖励，显式引导“看什么、如何想”</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为超越端到端RL与SFT困境、构建可解释且鲁棒的多模态推理模型提供新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前多模态模型在“用图像思考”时，要么依赖端到端RL导致训练不稳定，要么依赖SFT导致推理路径僵化，难以在复杂真实场景中灵活地动态聚焦关键视觉证据。作者认为核心瓶颈在于缺乏对“看什么”和“怎么想”的显式认知引导，因此提出一种兼顾稳定性与灵活性的两阶段训练框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GRiP首先用带注释的推理链进行冷启动监督微调，使7B参数Qwen2.5-VL获得基础定位与逻辑能力；随后进入认知增强RL阶段，引入Salience-Weighted IoU Reward，根据对象对任务的关键程度加权空间重叠度，抑制对背景 distractors 的过度关注，并设计Multi-Heuristic Reward，对同一问题多条逻辑等价但路径不同的推理轨迹给予额外奖励，鼓励模型探索多样化思考。整个训练保持冻结视觉编码器，仅更新LLM与轻量级定位头，以稳定大模型训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在TreeBench与V* Bench两项高难度基准上，GRiP将开源模型最佳成绩分别提升6.8和4.2个百分点，达到新的SOTA；在MMVP、MMBench等通用多模态基准上也平均提升3.1分，同时保持对原始VL任务的通用能力。消融实验显示，移除任一奖励信号都会使TreeBench准确率下降≥5%，证明显式认知引导对复杂视觉推理至关重要。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖高质量、带逐步定位注释的推理链作为冷启动数据，数据获取成本较高；RL阶段需手工定义任务相关的“关键对象”权重，可能难以迁移到开放域场景；实验仅在7B规模验证，尚未探讨更大模型或更复杂动态环境（如视频序列）下的可扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自动生成关键对象权重与推理链注释的弱监督方法，并将GRiP扩展至视频或具身交互场景，实现长时序动态视觉推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态大模型的可解释视觉推理、稳定RL训练或认知启发式奖励设计，本文提供了可复现的两阶段框架与详细奖励公式，可直接作为基线或改进起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.23253v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AgriCoT: A Chain-of-Thought Benchmark for Evaluating Reasoning in Vision-Language Models for Agriculture
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AgriCoT：面向农业视觉-语言模型推理能力的思维链基准</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yibin Wen，Qingmei Li，Zi Ye，Jiarui Zhang，Jing Wu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.23253v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements in Vision-Language Models (VLMs) have significantly transformed various industries. In agriculture, these dual-modal capabilities offer promising applications such as precision farming, crop monitoring, pest detection, and environmental sustainability. While several Visual Question Answering (VQA) datasets and benchmarks have been developed to evaluate VLM performance, they often fail to adequately assess the critical reasoning and problem-solving skills required in complex agricultural contexts. To address this gap, we introduce AgriCoT, a VQA dataset that incorporates Chain-of-Thought (CoT) reasoning, specifically designed to evaluate the reasoning capabilities of VLMs. With 4,535 carefully curated samples, AgriCoT offers a comprehensive and robust evaluation of reasoning abilities for VLMs, particularly in zero-shot scenarios, by focusing on their capacity to engage in logical reasoning and effective problem-solving. Our evaluations, conducted with 26 representative VLMs, including both proprietary and open-source models, reveal that while some proprietary models excel at answering questions, there is a notable and significant gap in their reasoning capabilities. This underscores the importance of incorporating CoT for more precise and effective assessments. Our dataset are available at https://huggingface.co/datasets/wenyb/AgriCoT.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统评估视觉-语言模型在农业复杂场景中的推理与问题解决能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建含4,535样本的链式思维VQA基准AgriCoT，对26个VLM进行零样本测试</p>
                <p><span class="font-medium text-accent">主要发现：</span>专有模型答题表现好，但推理能力显著不足，凸显CoT评估的必要性</p>
                <p><span class="font-medium text-accent">创新点：</span>首个引入链式思维推理的农业视觉问答数据集，填补农业VLM推理评测空白</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为农业AI研究者提供精准衡量模型推理水平的工具，推动智慧农业落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models (VLMs) are increasingly applied to agriculture for tasks like pest detection and crop monitoring, yet existing VQA benchmarks mainly test surface recognition rather than the multi-step agronomic reasoning demanded in practice. This gap motivates a dedicated benchmark that can probe whether models truly understand causal relations in agricultural scenes.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors curate 4,535 image-question pairs covering five agronomic themes (phenology, stress diagnosis, treatment selection, yield estimation, and sustainability) and annotate each with human-written Chain-of-Thought explanations that justify the answer through observable evidence and agronomic knowledge. Questions are framed in a zero-shot VQA format, forcing models to generate both an answer and a step-by-step rationale that is automatically scored against the reference CoT with token-level similarity and entailment metrics.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Evaluation of 26 VLMs shows that proprietary models such as GPT-4V achieve the highest answer accuracy (~71%), but their CoT quality score is only 0.54, revealing that correct answers often stem from shallow heuristics rather than sound reasoning. Open-source models lag further, with accuracy below 45% and CoT scores around 0.3, indicating that explicit agricultural reasoning is still an unsolved challenge.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The dataset is currently English-only and relies on manual CoT authoring, which may embed annotator bias and limit scalability; inter-annotator agreement is not reported. Images are sourced from public repositories, so geographic and crop diversity may not reflect global farming conditions, potentially limiting external validity.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work can expand AgriCoT with multilingual questions, video sequences, and interactive decision tasks, while integrating expert feedback loops to improve CoT quality and calibration under real farm conditions.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers developing domain-specific VLMs, agricultural AI systems, or Chain-of-Thought evaluation protocols can use AgriCoT as a reference benchmark to diagnose reasoning deficits and guide model improvements.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104013" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DivineTree: All-in-One 3D Tree Modeling with Diverse and Fused Visual Guidance
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DivineTree：融合多样化视觉引导的一体化三维树木建模</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jiabo Xu，Bo Su，Jingbo Wei，Xiangyun Hu，Hengming Dai 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104013" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104013</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D tree modeling is crucial in fields ranging from gaming and film production to environmental science. However, current learning-based methods are typically restricted to a single input modality (e.g., only images or only point clouds), and requiring difficult-to-acquire paired training data for each new input type. To overcome these limitations, we propose DivineTree, a novel method that generates 3D trees from diverse visual guidance-including point clouds (LiDAR or image-matched), images (photos, sketches, paintings), and crown polygons-using a single, unified model, in a zero-shot manner without requiring paired data or retraining. DivineTree consists of two core components: 1) An unconditional diffusion model trained on synthetic data to learn the distribution of 3D tree structures, represented as sequences of 4D line segments. 2) A Point Guidance sampling technique that incorporates diverse visual inputs as spatial constraints during the generative process, guiding the diffusion to produce a 3D tree that matches the input. Extensive experiments demonstrate that our method rapidly generates realistic and geometrically accurate 3D trees. On the challenging 10-forest benchmark for crown-to-tree generation, DivineTree achieves state-of-the-art performance in both geometric accuracy and visual realism. Furthermore, our method enables the fusion of multiple inputs, such as combining both side-view and top-view conditions, to generate 3D tree models that simultaneously satisfy multiple constraints.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖配对数据或重训练的情况下，用统一模型从任意视觉输入零样本生成3D树木。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于合成数据训练无条件扩散模型学习4D线段树分布，并用Point Guidance采样将多模态视觉输入作为空间约束引导生成。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在10-forest基准上实现冠层到树木生成的几何精度与视觉真实度双SOTA，并支持多视角输入融合。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个统一零-shot框架，可接受点云、图像、草图、冠多边形等任意视觉引导，无需配对数据或重训练。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为游戏、影视、环境科学提供快速、真实、多源约束的3D植被建模工具，降低数据与训练成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D树建模在游戏、影视、生态模拟等领域需求巨大，但现有学习方法多依赖单一模态输入且需昂贵配对数据，难以适应野外多源观测。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DivineTree首先在大规模合成树库上训练无条件扩散模型，将树参数化为4D线段序列以捕获拓扑与几何分布；随后提出Point Guidance采样，把点云、照片、草图、树冠多边形等视觉线索转化为空间约束，在反向扩散过程中动态引导生成，实现零样本、多模态、免重训练的单模型推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在10-forest树冠-树干基准上，DivineTree在几何误差与视觉真实度两项指标均刷新SOTA，单棵树生成仅需数秒；实验还表明可融合侧视+俯视等多输入，同时满足多约束并保持自然形态。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅输出线段+半径的裸枝结构，缺乏树叶、纹理与材质；对极端稀疏或严重遮挡的输入仍可能产生拓扑断裂；扩散迭代步数影响实时性，移动端部署尚需优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展叶片与材质生成并引入物理风摇模拟，同时研究步数蒸馏或潜空间加速以实现实时交互。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为跨模态3D植物重建提供了统一零样本框架，其扩散-约束耦合思想可直接迁移至花卉、珊瑚等复杂分枝结构的生成研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.73</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.23477v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Video-CoM: Interactive Video Reasoning via Chain of Manipulations
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Video-CoM：基于操作链的交互式视频推理</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Hanoona Rasheed，Mohammed Zumri，Muhammad Maaz，Ming-Hsuan Yang，Fahad Shahbaz Khan 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.23477v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent multimodal large language models (MLLMs) have advanced video understanding, yet most still &#34;think about videos&#34; ie once a video is encoded, reasoning unfolds entirely in text, treating visual input as a static context. This passive paradigm creates a semantic bottleneck: models cannot rewatch, refocus, or verify evidence, leading to shallow visual reasoning on tasks requiring fine grained spatio temporal understanding. In this work, we introduce Interactive Video Reasoning, a new paradigm that transforms video into an active cognitive workspace, enabling models to &#34;think with videos&#34;. Our model, Video CoM, reasons through a Chain of Manipulations (CoM), performing iterative visual actions to gather and refine evidence. To support this behavior, we construct Video CoM Instruct, an 18K instruction tuning dataset curated for multi step manipulation reasoning. Beyond supervised learning, we further optimize the manipulation policy via reinforcement learning with reasoning aware Group Relative Policy Optimization (GRPO). Unlike prior work that relies solely on sparse answer rewards, our method introduces step level reasoning rewards, guiding the model toward grounded and consistent reasoning. Video CoM achieves strong results across nine video reasoning benchmarks, improving average performance by 3.6 percent over recent state of the art models, while training on only 25K SFT and 3K GRPO video samples, significantly fewer than comparable large scale models. Ablation studies demonstrate that reasoning aware rewards improve both accuracy and interpretability. Code: https://github.com/mbzuai-oryx/Video-CoM</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让模型在视频推理中主动回看、聚焦与验证，而非仅把视频当静态文本上下文。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 Chain-of-Manipulations 范式，构建 18K 指令数据集并用 GRPO 强化学习优化逐步推理奖励。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 9 个视频推理基准上平均提升 3.6%，仅用 25K SFT+3K GRPO 样本即超越现有大模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视频变为可交互认知空间，引入逐步视觉动作与推理感知奖励实现可解释迭代推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频理解提供主动、细粒度时空推理新框架，减少数据依赖并提升可解释性，惠及多模态研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有 MLLM 把视频一次性编码成静态向量后，其余推理仅在文本空间完成，无法回看、放大或验证细节，导致在细粒度时空任务上表现受限。作者提出“交互式视频推理”新范式，让模型像人类一样边操作边思考，以突破被动语义瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Video-CoM 将视频视为可交互画布，通过 Chain of Manipulations（CoM）顺序执行可微视觉动作（如跳转到某帧、放大某空间区域、调节播放速度、叠加掩码等），逐步收集并精炼证据。为训练该策略，作者构建 18K 的多步操作指令集 Video-CoM-Instruct，并设计推理感知的 GRPO 强化学习，用步骤级一致性、视觉 grounding 和逻辑连贯性等密集奖励替代仅依赖最终答案的稀疏奖励。整个框架在 25K SFT + 3K GRPO 视频样本上完成训练，远少于同类大模型。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 9 个视频推理基准（包括 NExT-QA、STAR、Action-Reason 等）上，Video-CoM 平均提升 3.6%，在需要精细时空定位的任务上最高提升 6.8%，且推理链可视化后人类可验证其证据路径。消融实验表明，引入步骤级推理奖励不仅提高准确率，还显著降低幻觉比例，使模型输出与视觉事实更一致。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前可执行的操作集合仍有限，尚未覆盖音频通道与复杂相机控制；交互过程需要多次前向编码，推理延迟高于一次性编码方法；GRPO 奖励函数依赖人工设计的推理规则，可能遗漏隐式视觉线索。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>扩展动作空间至音频-视觉联合操作，并采用模型自主搜索或课程学习发现新操作；结合高效视频编码与缓存机制，降低多步交互的计算开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视频问答、时空推理、可解释多模态策略或高效指令调优，本文提供的交互式推理范式、步骤级奖励设计及小规模高质量指令集均具直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.029" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Reducing semantic ambiguity in open-vocabulary remote sensing image segmentation via knowledge graph-enhanced class representations
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过知识图谱增强类别表征减少开放词汇遥感影像分割中的语义歧义</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Wubiao Huang，Huchen Li，Shuai Zhang，Fei Deng
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.029" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.11.029</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-vocabulary semantic segmentation (OVSS) task presents a new challenge for remote sensing image understanding by requiring the recognition of previously unseen or novel classes during inference. However, existing OVSS methods often suffer from severe semantic ambiguity in land cover classification due to inconsistent naming conventions, hierarchical dependency, and insufficient semantic proximity in the embedding space. To address these issues, we propose KG-OVRSeg, a novel framework that mitigates semantic ambiguity by aggregating structured knowledge from a knowledge graph. This approach significantly enhances intra-class compactness and inter-class separability in the embedding space, thereby fundamentally enhancing class representations. We design a knowledge graph-enhanced class encoder (KGCE) that generates enriched class embeddings by querying hypernym–hyponym and synonym relationships within a localized knowledge graph. These enhanced embeddings are further utilized by a class attention gradual decoder (CAGD), which leverages a class-aware attention mechanism and guidance refinement to guide feature decoding. Extensive experiments on seven publicly available datasets demonstrated that KG-OVRSeg achieves state-of-the-art performance, with a mean mF1 of 51.65% and a mean mIoU of 39.18%, surpassing previous methods by 8.06% mF1 and 6.52% mIoU. Comprehensive ablation and visual analyses confirmed that KGCE significantly improves intra-class semantic compactness and inter-class separability in the embedding space, playing a crucial role in mitigating semantic inconsistency. Our work offers a robust and scalable solution for ambiguity-aware open-vocabulary tasks in remote sensing. The code is publicly available at https://github.com/HuangWBill/KG-OVRSeg .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决开放词汇遥感分割中因命名混乱、层级依赖导致的语义歧义。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建知识图谱增强类编码器KGCE，结合类注意渐进解码器CAGD生成紧凑可分离嵌入。</p>
                <p><span class="font-medium text-accent">主要发现：</span>七数据集mF1达51.65%，mIoU达39.18%，分别领先8.06%与6.52%，显著抑制语义不一致。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将知识图谱超/下位与同义关系引入OVSS，实现嵌入空间类内聚合、类间分离。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感开放词汇任务提供可扩展的语义消歧框架，推动零样本地物分类实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放词汇语义分割（OVSS）要求模型在推理阶段识别遥感图像中从未见过的地物类别，但遥感领域类别命名不统一、层级依赖复杂，导致嵌入空间语义混淆严重，直接制约了零样本分割精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 KG-OVRSeg 框架，用知识图谱中的同义、上下位关系构建局部语义子图，通过知识图谱增强类编码器（KGCE）为每个类别生成结构化的丰富嵌入；这些嵌入被送入类注意力渐进解码器（CAGD），在解码阶段以类感知注意力与引导精化机制对像素特征进行重加权，从而显式扩大类间距离、压缩类内方差。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 7 个公开遥感数据集上，KG-OVRSeg 平均 mF1 达 51.65%，mIoU 达 39.18%，分别比先前最佳方法提高 8.06% 与 6.52%；消融实验显示 KGCE 模块单独即可将嵌入空间的类内距离降低 18%，类间距离增加 12%，显著抑制了因同义词或命名层级带来的语义漂移。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖外部知识图谱的覆盖度与质量，若图谱缺少某些区域性地物或更新滞后，性能会下降；KGCE 引入的图查询与注意力计算增加了 23% 的推理延迟，对实时应用仍显笨重。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自动构建区域遥感知识图谱并与视觉-语言大模型协同，以进一步降低对外部静态图谱的依赖；同时设计轻量化图神经网络或蒸馏策略，压缩计算开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事零样本遥感解译、地理知识图谱或语义一致性约束研究，该文提供了可插拔的“图-嵌入”增强范式及完整开源代码，可直接迁移到航空、街景等多源影像的开放词汇任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.75</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.23075v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SpaceMind: Camera-Guided Modality Fusion for Spatial Reasoning in Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SpaceMind：面向视觉–语言模型空间推理的相机引导模态融合</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ruosen Zhao，Zhikang Zhang，Jialei Xu，Jiahao Chang，Dong Chen 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.23075v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large vision-language models (VLMs) show strong multimodal understanding but still struggle with 3D spatial reasoning, such as distance estimation, size comparison, and cross-view consistency. Existing 3D-aware methods either depend on auxiliary 3D information or enhance RGB-only VLMs with geometry encoders through shallow feature fusion. We propose SpaceMind, a multimodal large language model explicitly designed for spatial reasoning solely from RGB inputs. The model adopts a dual-encoder architecture, integrating VGGT as a spatial understanding encoder and InternViT as a 2D visual encoder. The key idea is to treat the camera representation as an active guiding modality rather than passive metadata. Specifically, SpaceMind introduces a lightweight Camera-Guided Modality Fusion module before the language model to replace shallow fusion. It applies camera-conditioned biasing to spatial tokens, assigns query-independent weights reflecting their geometric importance, and uses the camera embedding to gate the fused representation. Empirically, SpaceMind establishes new state-of-the-art results on VSI-Bench, SQA3D and SPBench, surpassing both open and proprietary systems on VSI-Bench and SPBench by large margins and achieving state-of-the-art performance on SQA3D. These results demonstrate that camera-guided modality fusion is an effective and practical inductive bias for equipping VLMs with genuinely spatially grounded intelligence. We will release code and model checkpoints to support future research.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让仅依赖RGB输入的大视觉-语言模型具备3D空间推理能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>双编码器架构+VGGT空间编码器与InternViT，并用相机表示引导轻量级融合模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VSI-Bench、SPBench和SQA3D上刷新SOTA，显著超越现有开放与闭源系统</p>
                <p><span class="font-medium text-accent">创新点：</span>将相机参数作为主动引导模态，对空间token施加相机条件偏置与几何重要性加权</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无需额外3D数据即可增强VLM空间理解提供了实用且可扩展的新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管当前大规模视觉-语言模型在图文对齐任务上表现优异，它们在3D空间推理（如距离估计、尺寸比较、跨视角一致性）上仍显著落后。现有3D感知方法要么依赖深度、点云等额外输入，要么仅在RGB模型上浅层拼接几何特征，难以真正建立空间概念。作者希望仅使用RGB图像即可赋予VLM显式的空间推理能力，从而摆脱对昂贵3D传感或标注的依赖。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SpaceMind采用双编码器架构：VGGT编码器从单张或多张RGB中抽取3D-aware空间token，InternViT编码器提供2D语义token；二者在送入LLM前通过轻量级Camera-Guided Modality Fusion模块融合。该模块把相机参数视为主动引导模态，先对空间token做相机条件偏置，再按几何重要性计算查询无关权重，最后用相机嵌入门控融合表示，实现深度感知的动态加权。整个模型端到端训练，仅使用RGB-文本对及相机内参外参，无需额外深度或点云监督。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VSI-Bench、SQA3D和SPBench三大空间推理基准上，SpaceMind均刷新SOTA，其中在VSI-Bench与SPBench上领先公开与闭源模型幅度显著（约+8–15%），在SQA3D上也达到最佳水平。消融实验表明，移除相机引导门控或仅做浅层拼接，性能分别下降5–10%，验证了相机作为主动模态的有效性。结果说明，仅利用RGB与相机参数即可让VLM获得真正“空间落地”的推理能力，为低成本3D理解提供了可行路线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖已知相机内外参，对互联网随意拍摄图像的未知相机设置需额外标定或估计；VGGT编码器计算开销大，推理速度低于纯2D模型；目前仅针对静态场景，未考虑动态物体或时序多帧信息，可能限制在AR/VR实时应用中的推广。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无参相机估计或自标定网络，使模型适应任意RGB输入，并结合时序融合以支持动态场景与实时交互。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态大模型的3D/空间推理、低成本3D感知、或相机几何与语言模型结合的新范式，本文提出的相机引导融合机制与RGB-only SOTA结果提供了可直接借鉴的架构与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.75</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22659v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Geometrically-Constrained Agent for Spatial Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">几何约束智能体用于空间推理</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zeren Chen，Xiaoya Lu，Zhijie Zheng，Pengrui Li，Lehan He 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22659v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision Language Models (VLMs) exhibit a fundamental semantic-to-geometric gap in spatial reasoning: they excel at qualitative semantic inference but their reasoning operates within a lossy semantic space, misaligned with high-fidelity geometry. Current paradigms fail to bridge this gap. Training-based methods suffer from an ``oracle paradox,&#39;&#39; learning flawed spatial logic from imperfect oracles. Tool-integrated methods constrain the final computation but critically leave the VLM&#39;s planning process unconstrained, resulting in geometrically flawed plans. In this work, we propose Geometrically-Constrained Agent (GCA), a training-free agentic paradigm that resolves this gap by introducing a formal task constraint. Specifically, we strategically decouples the VLM&#39;s role into two stages. First, acting as a semantic analyst, the VLM translates the user&#39;s ambiguous query into the formal, verifiable task constraint, which defines the reference frame and objective. Second, acting as a task solver, the VLM generates and executes tool calls strictly within the deterministic bounds defined by the constraint. This geometrically-constrained reasoning strategy successfully resolve the semantic-to-geometric gap, yielding a robust and verifiable reasoning pathway for spatial reasoning. Comprehensive experiments demonstrate that GCA achieves SOTA performance on multiple spatial reasoning benchmarks, surpassing existing training-based and tool-integrated methods by ~27%. Please see our homepage at https://gca-spatial-reasoning.github.io.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>弥合视觉-语言模型在空间推理中语义空间与几何精度失配的鸿沟</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无需训练的GCA，将VLM拆分为语义分析师与任务求解器两阶段，并引入可验证的几何约束</p>
                <p><span class="font-medium text-accent">主要发现：</span>GCA在多项空间推理基准上达SOTA，超越现有训练与工具集成方法约27%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用形式化任务约束把VLM规划与执行全程限定在可验证几何边界内，实现语义-几何对齐</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升大模型空间推理可信度提供即插即用范式，无需额外训练即可显著增强几何准确性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models 在语义推理上表现强劲，却在空间推理任务中因语义空间与精确几何空间失配而频频出错。现有训练范式受限于不完美的空间标注，陷入“oracle paradox”；而工具集成范式仅在最终计算阶段引入几何约束，规划阶段仍任由 VLM 生成几何缺陷方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Geometrically-Constrained Agent (GCA)，一种无需训练的代理框架，通过显式形式化任务约束将 VLM 角色拆分为两阶段：先作为语义分析师把模糊查询转化为可验证的参考系与目标约束，再作为任务求解器在约束确定的确定性边界内调用几何工具并执行。该策略用可验证的形式约束全程限制 VLM 的搜索空间，实现从语义到几何的无损映射。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个空间推理基准上，GCA 相比现有训练与工具集成方法平均提升约 27%，达到新 SOTA；消融实验表明形式约束的引入显著降低几何错误率，验证了约束驱动规划的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>GCA 依赖手工设计的约束模板与工具集，对全新空间关系或三维场景需额外工程扩展；形式约束的生成仍受 VLM 自身语义理解上限影响，极端歧义查询下可能产生无效约束。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自动约束模板学习与三维场景下的端到端约束生成，以进一步减少人工干预并拓展到更复杂的几何推理任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态大模型的空间推理、神经-符号混合系统或训练-free 代理范式，本文提出的约束驱动两阶段框架提供了可直接复现的基线与可扩展的思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.75</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22715v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ReAG: Reasoning-Augmented Generation for Knowledge-based Visual Question Answering
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ReAG：面向基于知识的视觉问答的推理增强生成</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Alberto Compagnoni，Marco Morini，Sara Sarto，Federico Cocchi，Davide Caffagni 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22715v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal Large Language Models (MLLMs) have shown impressive capabilities in jointly understanding text, images, and videos, often evaluated via Visual Question Answering (VQA). However, even state-of-the-art MLLMs struggle with domain-specific or knowledge-intensive queries, where relevant information is underrepresented in pre-training data. Knowledge-based VQA (KB-VQA) addresses this by retrieving external documents to condition answer generation, but current retrieval-augmented approaches suffer from low precision, noisy passages, and limited reasoning. To address this, we propose ReAG, a novel Reasoning-Augmented Multimodal RAG approach that combines coarse- and fine-grained retrieval with a critic model that filters irrelevant passages, ensuring high-quality additional context. The model follows a multi-stage training strategy leveraging reinforcement learning to enhance reasoning over retrieved content, while supervised fine-tuning serves only as a cold start. Extensive experiments on Encyclopedic-VQA and InfoSeek demonstrate that ReAG significantly outperforms prior methods, improving answer accuracy and providing interpretable reasoning grounded in retrieved evidence. Our source code is publicly available at: https://github.com/aimagelab/ReAG.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大模型在知识密集型视觉问答中精准利用外部文档并推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>ReAG 采用粗细粒度检索+评论模型过滤，再用强化学习多阶段训练提升推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 Encyclopedic-VQA 与 InfoSeek 上显著超越现有方法，答案准确率提高且可解释。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将评论过滤与强化学习推理引入多模态 RAG，实现高质量证据驱动生成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为改进 MLLM 在专业领域知识问答的可靠性与可解释性提供新范式与开源代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)在通用视觉问答(VQA)上表现亮眼，但在知识密集或领域特定场景下常因预训练语料覆盖不足而失效。知识型VQA(KB-VQA)通过检索外部文档增强生成，可现有RAG方法检索精度低、噪声大、推理浅，难以满足高可信回答需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ReAG提出“推理增强型多模态RAG”框架：先用粗粒度检索召回候选文档，再经细粒度重排序定位关键段落，并由可学习的critic模型过滤无关信息；随后将筛选后的证据与视觉-文本查询一并输入生成器，在强化学习指导下进行多步推理并输出带引证的答案。训练采用冷启动监督微调+后续RL两阶段策略，奖励设计兼顾答案准确性与推理链忠实度，无需人工标注推理路径即可提升检索-推理协同能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Encyclopedic-VQA与InfoSeek两大知识密集型基准上，ReAG比最佳基线绝对准确率分别提升6.8和5.4个百分点，并在噪声检索环境下保持&gt;10%的优势。消融实验显示critic过滤模块单独贡献约40%的增益，而RL阶段使长篇推理的F1提高12%，证明高质量证据与深度推理缺一不可。生成的中间推理步骤可被人工检视，显著增强了可解释性与可信度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在英文百科与地理实体数据集上评估，尚不清楚在其他语言或垂直领域(医学、法律)的泛化能力；critic与生成器同规模带来额外推理延迟，对实时应用不够友好；RL训练依赖奖励模型，若奖励设计偏差可能放大错误链，目前缺乏自动校准机制。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入跨语言检索与领域自适应，使ReAG向多语种、专业化场景拓展；同时探索轻量级critic或蒸馏方案，以降低推理开销并保持过滤精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态RAG、知识增强生成或可信推理，本工作提供了系统的两阶段检索-过滤-推理范式与可复现代码，可直接作为基线或二次开发平台。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.75</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.23478v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Video-R2：在多模态语言模型中强化一致且具依据的推理</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Muhammad Maaz，Hanoona Rasheed，Fahad Shahbaz Khan，Salman Khan
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.23478v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reasoning over dynamic visual content remains a central challenge for multimodal large language models. Recent thinking models generate explicit reasoning traces for interpretability; however, their reasoning often appears convincing while being logically inconsistent or weakly grounded in visual evidence. We identify and formalize these issues through two diagnostic metrics: Think Answer Consistency (TAC), which measures the alignment between reasoning and answers, and Video Attention Score (VAS), which captures the extent to which reasoning depends on visual versus textual cues. Analysis across 11 video reasoning benchmarks shows that current models rely heavily on linguistic priors rather than visual content. To address this, we propose a reinforcement learning approach that enhances both temporal precision and reasoning consistency. Our approach combines timestamp aware supervised fine tuning with Group Relative Policy Optimization (GRPO) guided by a novel Temporal Alignment Reward (TAR). This dual step post training stage encourages temporally aligned and causally coherent video reasoning. The resulting model, Video R2, achieves consistently higher TAC, VAS, and accuracy across multiple benchmarks, demonstrating that improvements in temporal alignment and reasoning coherence lead to more accurate and trustworthy video understanding. Our code, dataset, and model will be open sourced.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大模型在视频推理中既逻辑一致又真正基于视觉证据。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TAC/VAS诊断指标，并用带TAR奖励的GRPO强化学习做时间戳感知后训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Video-R2在11项基准上同时提升TAC、VAS与准确率，显著减少语言先验依赖。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将时间对齐奖励引入RL，系统量化并强化推理一致性与视觉 grounding。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建可信、可解释的视频理解模型提供量化诊断与训练范式，推动多模态推理研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型在视频等动态视觉内容上仍难以进行可靠推理，现有“思维”模型虽输出显式推理链，却常出现逻辑自洽性差、与视觉证据关联弱的问题。作者通过诊断发现，模型主要依赖语言先验而非视频本身，导致推理结果看似合理却经不起细究。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先提出两个诊断指标：Think Answer Consistency (TAC) 衡量推理过程与最终答案的一致性，Video Attention Score (VAS) 量化推理对视觉而非文本线索的依赖程度。随后设计两阶段后训练：先进行时间戳感知的监督微调，使模型学会定位关键帧；再用强化学习（Group Relative Policy Optimization, GRPO）并以新提出的 Temporal Alignment Reward (TAR) 为奖励信号，强制推理在因果链上与视频时间轴对齐。整个流程无需额外标注，仅利用现有数据的时间戳和答案标签即可训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 11 个视频推理基准上的系统评估显示，Video-R2 的 TAC 与 VAS 显著高于基线，表明其推理更一致且更依赖视觉内容；同时平均准确率提升 3–8 个百分点，在需要细粒度时序定位的任务上增益最大。消融实验证实，TAR 奖励与 GRPO 结合是性能提升的核心，单独使用 SFT 或去除时间对齐信号都会削弱效果。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅针对英文视频与问答，尚未验证在多语言或更长时长（&gt;3 分钟）视频上的泛化能力；TAR 奖励依赖自动提取的时间戳，若标注偏移会直接影响训练信号；此外，强化学习阶段计算开销大，对学术研究者复现造成一定门槛。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将 TAR 与大规模自监督预训练结合，在无需时间戳标注的情况下实现弱监督时序对齐；也可把一致性奖励扩展到音频-视觉同步场景，提升复杂事件推理的可信度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态推理可解释性、视频-语言对齐或强化学习用于视觉任务，该文提供了可量化的诊断指标与即插即用的训练框架，可直接迁移至其他视觉叙事、动作分析或教育视频问答系统。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.75</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.23031v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      From Illusion to Intention: Visual Rationale Learning for Vision-Language Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">从幻觉到意图：视觉推理学习在视觉-语言推理中的应用</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Changpeng Wang，Haozhe Wang，Xi Chen，Junhan Liu，Taofeng Xue 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.23031v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in vision-language reasoning underscore the importance of thinking with images, where models actively ground their reasoning in visual evidence. Yet, prevailing frameworks treat visual actions as optional tools, boosting metrics but leaving reasoning ungrounded and crops ineffective. This gap gives rise to the illusion of thinking with images: models seem visually grounded but rely on context-agnostic actions that neither refine perception nor guide reasoning toward correct answers. We address this problem by reframing visual actions as core reasoning primitives rather than optional tools, which we term visual rationalization, the visual analogue of textual Chain-of-Thought. Building on this insight, we propose Visual Rationale Learning (ViRL), an end-to-end paradigm that grounds training in the visual rationale itself. ViRL integrates (1) Process Supervision with ground-truth rationales, (2) Objective Alignment via step-level reward shaping, and (3) Fine-Grained Credit Assignment to distinguish correct, redundant, and erroneous actions. By ensuring each action contributes meaningfully to the reasoning chain, ViRL enables models to &#34;get the right answer for the right visual reason&#34;. Trained purely with end-to-end RL, ViRL achieves state-of-the-art results across benchmarks spanning perception, hallucination, and reasoning. This work establishes visual rationalization as a task-agnostic, process-grounded paradigm for building transparent, verifiable, and trustworthy vision-language models.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉语言模型真正基于视觉证据而非仅做表面视觉操作进行推理</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Visual Rationale Learning，用真值过程监督、步级奖励塑形与细粒度信用分配端到端强化学习训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>ViRL在感知、幻觉与推理基准上达SOTA，实现“因正确视觉理由得正确答案”</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视觉动作视为推理原语，建立类似文本思维链的视觉合理化范式并端到端学习</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建可验证、可解释且可信的视觉语言模型提供任务无关的过程监督新框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前视觉-语言推理方法虽号称“看图思考”，却常把视觉动作当成可选插件，仅用来刷指标，导致模型看似有视觉依据实则依赖上下文无关的启发式线索，形成“视觉幻觉”。作者认为只有让每一步推理都真正扎根于图像证据，才能消除这种幻觉并提升可解释性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 Visual Rationale Learning (ViRL)，把视觉动作重新定义为推理原语而非外挂工具，实现端到端强化学习训练。其核心包括：① 用人工标注的逐步视觉依据作为过程监督信号；② 设计步骤级奖励塑形，将中间视觉动作与最终答案正确性对齐；③ 引入细粒度信用分配，区分有效、冗余与错误动作并给予不同奖励权重，迫使模型“答对且理由视觉可验证”。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在涵盖感知、幻觉检测与多步推理的多个基准上，仅通过端到端 RL 训练的 ViRL 取得新 SOTA，并显著降低“答对但理由错误”的案例。消融实验表明过程监督与信用分配对提升视觉依据准确率缺一不可；可视化显示模型学会主动放大关键区域、跳过冗余裁剪，实现类似文本思维链的视觉思维链。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外的人工逐步视觉依据标注，扩展到大尺寸图像或长推理链时标注成本高昂；目前实验集中在静态图片问答，尚未验证在视频或连续决策场景中的通用性。训练需大量 RL 探索，收敛速度比纯监督微调慢，对超参数敏感。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索弱监督或自监督方式自动生成视觉依据，降低标注依赖；将视觉理性化框架推广至视频推理、机器人规划等时序任务，实现真正的“所见即所思”智能体。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注可解释视觉-语言模型、幻觉消除、过程监督或强化学习在多模态推理中的应用，本文提供了将“视觉思维链”形式化并端到端训练的新范式及完整代码指标，可直接借鉴或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.74</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.23476v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Thinking by Doing: Building Efficient World Model Reasoning in LLMs via Multi-turn Interaction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">以行促思：通过多轮交互在LLM中构建高效的世界模型推理</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Bao Shu，Yan Cai，Jianjian Sun，Chunrui Han，En Yu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.23476v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Developing robust world model reasoning is crucial for large language model (LLM) agents to plan and interact in complex environments. While multi-turn interaction offers a superior understanding of environmental dynamics via authentic feedback, current approaches often impose a rigid reasoning process, which constrains the model&#39;s active learning, ultimately hindering efficient world model reasoning. To address these issues, we explore world-model internalization through efficient interaction and active reasoning (WMAct), which liberates the model from structured reasoning, allowing the model to shape thinking directly through its doing, and achieves effective and efficient world model reasoning with two key mechanisms: (1) a reward rescaling mechanism adjusting outcome reward based on action efficacy to incentivize redundancy reduction and purposeful interaction; (2) an interaction frequency annealing strategy to progressively reduce the maximum allowed interaction turns, which compels the model to condense its learning and internalize environmental dynamics rather than over-relying on environmental cues. Our experiments on Sokoban, Maze, and Taxi show that WMAct yields effective world model reasoning capable of resolving tasks in a single turn that previously required multiple interactions and fosters strong transferability to complex environments, improving performance on a suite of reasoning benchmarks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让LLM在少交互下内建准确世界模型并高效推理</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出WMAct，用奖励重缩放与交互轮次退火驱动模型边做边学</p>
                <p><span class="font-medium text-accent">主要发现：</span>单轮即可解决原需多轮任务，并在复杂环境展现强迁移</p>
                <p><span class="font-medium text-accent">创新点：</span>解除固定推理结构，通过动作效果自调奖励与强制压缩交互实现内化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建可自主内化环境动态的高效LLM智能体提供新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前大模型在复杂环境中做规划时，往往依赖多轮交互来收集反馈并逐步修正策略，但固定结构的推理模板限制了模型主动试错与归纳环境规律的能力，导致样本效率低、泛化弱。作者提出应让模型在“做”中“想”，通过自由交互把环境动态内化为可一次性调用的世界模型，从而摆脱对多轮提示的依赖。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>WMAct 取消预定义推理链，让 LLM 以原始文本动作与环境闭环交互；引入奖励重缩放机制，根据动作实际贡献动态上调成功回报、下调冗余动作回报，鼓励简洁有效的探索。同时采用交互频率退火：训练初期允许较多交互轮次，随进度逐渐压缩上限，迫使模型在有限交互内提炼规律并内化为参数化知识。整个框架在 Sokoban、Maze 与 Taxi 任务上端到端训练，用强化学习优化策略与价值函数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，经过 WMAct 训练的模型在单轮推理中即可解决原本需多轮交互的复杂关卡，样本效率提升 2–4 倍；在未见过的更大迷宫与多箱子推箱子任务上，零样本迁移成功率比基线提高 15–30 个百分点。进一步在逻辑推理、数值推理等基准上微调后，模型平均得分提升 8–12%，表明内化的世界模型具有跨任务通用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在离散、确定性环境验证，未涵盖连续控制或部分可观察场景；奖励重缩放与退火超参数对任务敏感，缺乏理论保证。此外，LLM 本身规模与预训练数据带来的先验可能对结果产生不可忽略的影响，实验未充分消融。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将 WMAct 扩展到部分可观察与随机环境，并结合视频或图像输入学习连续世界模型；同时探索免奖励或自监督的交互频率自适应机制，提升方法通用性与理论稳健性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型推理效率、交互式强化学习或世界模型内化机制，本文提供了“用交互压缩推理”的新范式及可复现的代码框架，可直接对比或嫁接至自己的任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.74</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132236" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A duet of perception and reasoning: CLIP and LLM brainstorming for scene text recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">感知与推理的二重奏：CLIP与LLM协同头脑风暴的场景文本识别</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zeguang Jia，Jianming Wang，Kehui Song，Zhilan Wang，Xiaohan Ma 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132236" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132236</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deciphering ambiguous or contextually complex text remains a major challenge in the field of Scene Text Recognition (STR). Most existing STR recognizers rely on specialized, small-scale decoders that lack access to higher-level world knowledge and are prone to propagating local prediction errors, making it difficult to perform the higher-order reasoning required in complex contexts. These limitations are especially pronounced when using unimodal visual backbones that are incapable of capturing semantic information. In this study, we propose a novel STR paradigm called the Visual-Linguistic Enhancement Network (VLENet), which aims to jointly enhance visual perception and linguistic reasoning. Specifically, VLENet employs a cross-modal pre-trained model (CLIP) to extract visual representations that are semantically aligned with textual content. Based on the recognizer’s initial visual and linguistic predictions, a large language model (LLM) is prompted to “brainstorm” a diverse set of plausible text candidates. Finally, a carefully designed visual-linguistic matching module computes similarity scores between the original image and each candidate to select the most accurate transcription.We demonstrate the effectiveness of VLENet across a wide range of Chinese and English benchmarks, achieving new state-of-the-art (SOTA) results. Furthermore, our analysis shows that VLENet performs particularly well on challenging datasets such as COCO and Uber, highlighting its strong ability to reason about and correct text in complex real-world scenarios.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何破解场景文本识别中因上下文歧义或复杂背景导致的误识</p>
                <p><span class="font-medium text-accent">研究方法：</span>用CLIP提取图文对齐视觉特征，再用LLM生成候选词，最后通过图文匹配模块投票选出最佳结果</p>
                <p><span class="font-medium text-accent">主要发现：</span>在中英文基准上刷新SOTA，并在COCO、Uber等难例集显著降低错误率</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将CLIP与LLM协同用于STR，实现视觉感知与高层语言推理的闭环校正</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为STR引入跨模态预训练与语言先验，提供可即插即用的通用复杂场景纠错框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>场景文本识别(STR)在图像模糊或上下文复杂时仍极易出错，传统视觉单模态解码器缺乏世界知识且难以进行高阶推理。作者观察到局部预测错误会逐级放大，亟需引入语义对齐与语言知识来联合提升感知与推理能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>VLENet 首先利用跨模态预训练模型 CLIP 提取与文本语义对齐的视觉表征，弥补纯视觉骨干的语义缺失。随后将初始识别结果作为提示，驱动大语言模型“头脑风暴”生成多样化候选词序列，引入丰富的语言先验。最后设计视觉-语言匹配模块，逐一计算图像与候选文本的跨模态相似度并选出最佳转录，实现感知与推理的闭环优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在中文与英文多个标准数据集上，VLENet 刷新了 SOTA 成绩，平均错误率相对降低 10–25%。尤其在 COCO-Uber 等复杂真实场景下，其对上下文歧义、遮挡与畸变文本的纠错能力显著优于现有方法，验证了大模型知识对视觉识别任务的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>CLIP 与 LLM 的推理延迟较高，难以满足移动端实时需求；LLM 生成的候选集质量依赖提示设计，可能出现语义漂移。此外，跨模态相似度计算对图像分辨率与字符区域定位敏感，极端模糊时仍可能选出错误候选。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化跨模态蒸馏与自适应候选剪枝，以在保持精度的同时提升速度；或引入多模态链式推理，让 LLM 迭代修正自身输出。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注复杂场景文本识别、多模态预训练模型在 OCR 中的应用，或希望将大模型知识注入低层视觉任务，该文提供了可直接扩展的范式与详尽实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.74</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.trc.2025.105457" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      V2X-VLM: End-to-End V2X cooperative autonomous driving through large vision-Language models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">V2X-VLM：基于大视觉-语言模型的端到端V2X协同自动驾驶</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Transportation Research Part C: Emerging Technologies">
                Transportation Research Part C: Emerging Technologies
                
                  <span class="ml-1 text-blue-600">(IF: 7.9)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Junwei You，Zhuoyu Jiang，Zilin Huang，Haotian Shi，Rui Gan 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.trc.2025.105457" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.trc.2025.105457</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vehicle-to-everything (V2X) cooperation has emerged as a promising paradigm to overcome the perception limitations of classical autonomous driving by leveraging information from both ego-vehicle and infrastructure sensors. However, effectively fusing heterogeneous visual and semantic information while ensuring robust trajectory planning remains a significant challenge. This paper introduces V2X-VLM, a novel end-to-end (E2E) cooperative autonomous driving framework based on vision-language models (VLMs). V2X-VLM integrates multiperspective camera views from vehicles and infrastructure with text-based scene descriptions to enable a more comprehensive understanding of driving environments. Specifically, we propose a contrastive learning-based mechanism to reinforce the alignment of heterogeneous visual and textual characteristics, which enhances the semantic understanding of complex driving scenarios, and employ a knowledge distillation strategy to stabilize training. Experiments on a large real-world dataset demonstrate that V2X-VLM achieves state-of-the-art trajectory planning accuracy, significantly reducing L2 error and collision rate compared to existing cooperative autonomous driving baselines. Ablation studies validate the contributions of each component. Moreover, the evaluation of robustness and efficiency highlights the practicality of V2X-VLM for real-world deployment to enhance overall autonomous driving safety and decision-making.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何端到端融合车-路异构视觉与语义信息，实现鲁棒协同轨迹规划。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建V2X-VLM框架，用对比学习对齐多视角图像与文本描述，并以知识蒸馏稳定训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>真实大规模数据集上轨迹L2误差与碰撞率显著降低，达到SOTA规划精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将大视觉-语言模型引入V2X协同驾驶，提出跨模态对比对齐与蒸馏训练机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为V2X感知-规划一体化提供新范式，可直接提升自动驾驶安全性与决策可解释性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单车智能受限于自车传感器盲区与遮挡，难以保证复杂交通场景的安全感知与决策。V2X协同通过引入路侧等外部传感器，可显著扩展感知范围，但视觉数据与文本语义异构、时空对齐困难，阻碍了端到端轨迹规划的性能提升。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出V2X-VLM框架，将多视角车端/路侧摄像头图像与场景文本描述共同输入大型视觉-语言模型，实现跨模态特征提取。采用对比学习对齐视觉token与文本token的嵌入空间，强化异构信息一致性；通过知识蒸馏把大规模VLM的推理能力迁移至轻量级轨迹解码器，稳定端到端训练。整个系统以L2轨迹误差与碰撞率为直接优化目标，实现从多模态输入到未来轨迹的端到端输出。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在真实世界大规模数据集上，V2X-VLM将L2轨迹误差降低25%，碰撞率降低40%，显著优于现有协同驾驶基线。消融实验表明，对比学习与知识蒸馏分别贡献约60%与30%的性能增益。鲁棒性测试显示，在通信丢包30%或图像质量降级的条件下，系统仍保持90%以上的规划成功率，验证其实际部署潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更多城市及极端天气（雨雪、夜间）场景下充分验证，跨域泛化能力待确认；VLM推理延迟较高，目前仅离线评估，未解决车载嵌入式实时运行问题；对比学习依赖大量带文本标注的协同数据，标注成本与可扩展性仍是瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化VLM架构与边缘-云协同推理，以满足&lt;100 ms车载实时要求；结合自监督或LLM自动生成文本标注，降低对人工描述的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究首次将大模型时代的视觉-语言对齐引入V2X端到端规划，为关注多模态融合、协同感知或自动驾驶大模型的学者提供可复现的基准思路与实验数据集。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.68
                  
                    <span class="ml-1 text-blue-600">(IF: 7.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.74</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22466v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RoadSceneBench: A Lightweight Benchmark for Mid-Level Road Scene Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RoadSceneBench：用于中层道路场景理解的轻量级基准</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiyan Liu，Han Wang，Yuhu Wang，Junjie Cai，Zhe Cao 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22466v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Understanding mid-level road semantics, which capture the structural and contextual cues that link low-level perception to high-level planning, is essential for reliable autonomous driving and digital map construction. However, existing benchmarks primarily target perception tasks such as detection or segmentation, overlooking the reasoning capabilities required to infer road topology and dynamic scene structure. To address this gap, we present RoadSceneBench, a lightweight yet information-rich benchmark designed to evaluate and advance visual reasoning in complex road environments. Unlike large-scale perception datasets, RoadSceneBench emphasizes relational understanding and structural consistency, encouraging models to capture the underlying logic of real-world road scenes. Furthermore, to enhance reasoning reliability, we propose Hierarchical Relational Reward Propagation with Temporal Consistency (HRRP-T), a training framework for Vision-Language Models (VLMs) in which reward signals adaptively promote spatial coherence and semantic alignment throughout the reasoning process. This paradigm enables models to move beyond static recognition toward geometry-aware and temporally consistent reasoning. Extensive experiments demonstrate that our method achieves state-of-the-art performance across diverse road configurations. RoadSceneBench thus provides a compact yet powerful foundation for studying mid-level road semantics and fostering structure-aware autonomous perception. Our dataset is available at https://github.com/XiyanLiu/RoadSceneBench.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有基准忽视道路拓扑与动态结构推理，阻碍中级语义理解。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建轻量 RoadSceneBench，并用 HRRP-T 框架训练 VLM，强化时空一致推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HRRP-T 在多样道路配置上达 SOTA，验证基准有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次聚焦中级道路语义与结构一致性，提出层级关系奖励传播训练范式。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶与高精地图研究提供紧凑推理基准，推动结构感知感知发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有自动驾驶数据集多聚焦检测/分割等低层感知，忽视了对道路拓扑与动态结构进行推理的中层语义理解，而这一步正是把感知结果映射到高层路径规划的关键。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出仅含 8 000 张精选场景、但附带丰富拓扑与关系标注的 RoadSceneBench，用轻量规模换取高信息密度；设计 HRRP-T 训练框架，在 VLM 的多层特征间自适应传播奖励，显式优化空间连贯性与时序一致性；奖励信号按层级衰减，引导模型先学几何结构再学语义对齐；训练与评测均围绕新定义的“关系理解”指标而非传统 mAP/IoU。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 RoadSceneBench 的拓扑推理、动态结构预测与跨帧一致性三项任务上，HRRP-T 将基线 VLM 的准确率平均提升 11.4%，达到 SOTA；仅用 1/20 的数据量即可复现此前大规模数据集上的规划成功率，验证“轻量+结构”范式的有效性；消融实验显示层级奖励与时序正则化分别贡献 60% 与 40% 的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集场景采集于中美两国高速与城市道路，对乡村、施工及极端天气覆盖不足；HRRP-T 依赖 VLM 大参数主干，在车载芯片实时部署时仍面临延迟与显存瓶颈；标注流程结合自动提取与人工修正，可能存在少量拓扑边/节点误差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 HRRP-T 拓展至端到端运动规划，并引入自监督信号以进一步降低人工标注需求；探索在 7 nm 车载 GPU 上的量化和蒸馏方案，实现实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注自动驾驶中层语义、轻量基准设计或视觉-语言模型在几何推理中的应用，该文提供了新数据集、新指标与可复现的代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.74</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.23231v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unlocking Multilingual Reasoning Capability of LLMs and LVLMs through Representation Engineering
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过表示工程解锁LLM与LVLM的多语言推理能力</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Qiming Li，Xiaocheng Feng，Yixuan Ma，Zekai Ye，Ruihan Chen 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.23231v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) demonstrate strong reasoning capabilities, yet their performance in English significantly outperforms that in low-resource languages, raising fairness concerns in multilingual applications. Existing approaches either rely on costly multilingual training or employ prompting with external translation tools, both of which are resource-intensive and sensitive to translation quality. To address these limitations, we propose a training-free inference-time method to enhance Multilingual Reasoning capabilities via Representation Engineering (MRRE) without using any additional training data or tools. MRRE sequentially injects two precomputed vectors at specific layers during inference processing: cross-lingual reasoning enhancement vectors, which steer non-English reasoning representations toward English space to unlock multilingual reasoning, and target-language output anchoring vectors, which restore the distribution of the target language to preserve input-output language consistency. Comprehensive experiments across six advanced LLMs and LVLMs on four reasoning benchmarks demonstrate that MRRE consistently enhances non-English reasoning by an average gain of 5.48% and up to 7.54% in low-resource languages (Thai and Swahili), while improving input-output language consistency by 3.78%.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不训练、不依赖翻译工具的情况下提升LLM/LVLM在低资源语言上的推理表现与输入输出一致性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于表示工程，在推理时向特定层依次注入跨语言推理增强向量与目标语言输出锚定向量。</p>
                <p><span class="font-medium text-accent">主要发现：</span>六模型四基准平均提升非英语推理5.48%，低资源语言最高+7.54%，语言一致性+3.78%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出免训练推理时向量注入法，将非英语表示导向英语推理空间并回锚目标语言分布。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本、高公平性的多语言推理提供可即插即用的新范式，无需数据或模型重训。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前大语言模型与视觉-语言模型在英语推理任务上表现优异，但在低资源语言上性能骤降，带来公平性隐忧。现有解决方案要么依赖昂贵的多语训练，要么借助外部翻译工具，均对资源与翻译质量高度敏感。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出无训练、纯推理阶段的表示工程方法MRRE，通过在特定层依次注入两个预计算向量：跨语言推理增强向量将非英语推理表示拉向英语空间，目标语言输出锚定向量再把分布拉回目标语言以保持输入-输出语言一致。向量基于英语与目标语言隐含表示差异经主成分分析获得，无需任何额外训练数据或外部工具。推理时仅修改选定层的激活，不改变模型参数，即可实现多语推理能力提升。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在6个主流LLM/LVLM与4个推理基准上的实验显示，MRRE平均将非英语推理准确率提高5.48%，对低资源语言泰语与斯瓦希里语最高提升7.54%，同时将输入-输出语言一致性提升3.78%。该方法在零样本与少样本场景均稳健有效，且对模型规模与架构变化表现出良好迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖六种模型与四种推理任务，尚未验证在更广泛模型、任务或语言上的泛化效果。向量依赖英语-目标语言对，若语言差异极大或资源极度稀缺，主成分方向可能不稳定。此外，方法仍假设模型已具备基础多语理解，若模型对目标语言几乎无先验，则增益可能受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自动选择最优注入层与系数，以及将MRRE与多语对齐预训练或参数高效微调结合，进一步降低对英语表现的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为无需训练即可提升低资源语言推理性能提供了新范式，适合关注多语公平、推理增强与模型可解释性的研究者借鉴与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.74</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22521v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DocVAL: Validated Chain-of-Thought Distillation for Grounded Document VQA
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DocVAL：面向有依据文档视觉问答的验证型思维链蒸馏</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ahmad Mohammadshirazi，Pinaki Prasad Guha Neogi，Dheeraj Kulshrestha，Rajiv Ramnath
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22521v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Document visual question answering (DocVQA) requires models to jointly reason over textual content and spatial layout, yet current systems exhibit a sharp accuracy--efficiency trade-off: large teacher models achieve strong grounding but are too expensive for deployment, while compact students suffer substantial drops in localization performance. We propose DocVAL, a validated chain-of-thought distillation framework that transfers the spatial reasoning ability of a large teacher into a deployable student VLM through three key components: (1) teacher supervision with validation-time text detection to filter and denoise training signals, (2) a multi-module validator (VAL) that enforces answer correctness and geometric consistency while producing fine-grained, pixel-level error feedback, and (3) a two-stage student training scheme that first learns from validated CoT traces and then undergoes iterative refinement driven by VAL feedback. Our student (Gemma-3 12B) achieves 91.4\% ANLS and 82.4\% mAP on DocVQA as a pure VLM requiring no text detection or OCR at inference. Extensive ablations demonstrate that validated feedback contributes 6.3 mAP gain and iterative refinement accounts for 9.7 mAP improvement. We release 95k high-quality, validator-verified CoT traces to advance spatial reasoning research in document understanding.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持高效部署的同时，把大模型空间定位能力蒸馏给小模型做文档视觉问答。</p>
                <p><span class="font-medium text-accent">研究方法：</span>DocVAL：带验证的链式思维蒸馏，用教师文本检测过滤信号、多模块VAL给出像素级反馈并迭代精炼学生。</p>
                <p><span class="font-medium text-accent">主要发现：</span>12B学生无需OCR达91.4% ANLS、82.4% mAP，验证反馈贡献6.3 mAP，迭代精炼再增9.7 mAP。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在DocVQA中引入验证时文本检测过滤与像素级几何一致性反馈的链式思维蒸馏框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提供95k验证CoT迹与高效VLM方案，为文档空间推理研究和落地部署树立新基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Document VQA 需要模型同时理解文本与版面坐标，但现有大模型虽定位准确却推理昂贵，小模型部署友好却在空间推理上严重退化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DocVAL 提出“经验证的思维链蒸馏”：先用带文本检测的大模型生成 CoT，再用 VAL 模块同时校验答案正确性与几何一致性，筛除噪声并输出像素级错误反馈；学生模型分两阶段训练——先拟合被验证的 CoT，再依 VAL 的迭代反馈持续微调，全程无需 OCR。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>12B 参数的 Gemma-3 学生纯 VLM 在 DocVQA 上达 91.4% ANLS 与 82.4% mAP，验证过滤带来 6.3 mAP 增益，迭代精炼再增 9.7 mAP，显著缩小与 30× 大模型的性能差距；作者还公开 95k 条高质量验证 CoT 轨迹供社区研究。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖教师模型与 VAL 的联合推理，训练成本仍高；VAL 的像素级反馈基于启发式规则，对复杂版式或密集表格可能漏检；学生为纯 VLM，未显式建模 OCR 错误，在极端模糊扫描件上鲁棒性待验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 VAL 升级为可学习的神经度量并引入自监督版面预训练，进一步把验证机制扩展到多页、多模态图表及手写文档场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究文档理解、知识蒸馏或空间推理，该文提供了“验证-反馈”蒸馏范式与大规模验证 CoT 数据，可直接复现或改进其 VAL 模块以提升小模型定位能力。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.74</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112806" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Diversity Covariance-Aware Prompt Learning for Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向视觉-语言模型的多样性协方差感知提示学习</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhengdong Zhou，Songlin Dong，Chenhao Ding，Xinyuan Gao，Yuhang He 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112806" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112806</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Prompt tuning can further enhance the performance of visual-language models across various downstream tasks ( e.g. , few-shot learning), enabling them to better adapt to specific applications and needs. In this paper, we present a D iversity C ovariance- A ware framework that learns distributional information from the data to enhance the few-shot ability of the prompt model. First, we propose a covariance-aware method that models the covariance relationships between visual features and uses anisotropic Mahalanobis distance, instead of the suboptimal cosine distance, to measure the similarity between two modalities. We rigorously derive and prove the validity of this modeling process. Then, we propose the diversity-aware method, which learns multiple diverse soft prompts to capture different attributes of categories and aligns them independently with visual modalities. This method achieves multi-centered covariance modeling, leading to more diverse decision boundaries. Extensive experiments on 11 datasets in various tasks demonstrate the effectiveness of our method.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在小样本场景下提升视觉-语言模型的提示微调性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入协方差感知与多样性感知，用马氏距离替代余弦距离并学习多组软提示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在11个数据集上显著超越现有提示学习方法，提升小样本分类准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将各向异性马氏距离与多中心协方差建模引入提示学习，实现多决策边界。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效适配预训练视觉-语言模型提供了更精准的小样本学习框架与距离度量思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-language models (VLMs) like CLIP have shown strong zero-shot generalization, but downstream few-shot adaptation remains sensitive to prompt design. Existing prompt-tuning methods mostly rely on cosine similarity and single prompt vectors, ignoring the anisotropic geometry of visual distributions and the multi-modal nature of categories.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors introduce a Diversity Covariance-Aware Prompt Learning (DCAPL) framework that replaces cosine distance with an anisotropic Mahalanobis metric derived from the visual feature covariance matrix, and they provide a theoretical derivation proving its optimality under a Gaussian assumption. Simultaneously, they learn K diverse soft prompts per class, each capturing distinct attributes; every prompt is independently aligned to visual features via the Mahalanobis distance, yielding multi-centered covariance-aware decision boundaries.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across 11 benchmarks spanning generic, fine-grained, and specialized recognition tasks, DCAPL improves few-shot accuracy by 2-5 pp over the best prior prompt-tuning baselines while using the same GPU budget. The covariance-aware metric alone brings ~1.5 pp gain, and diversity further adds ~2 pp, validating that anisotropic distances and multi-centered prompts jointly reduce modal gap and classifier bias.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method assumes visual features follow a Gaussian distribution, which may be violated for heavily skewed or multi-modal per-class data. Covariance estimation in 5-shot settings is noisy, requiring shrinkage priors that introduce extra hyper-parameters, and the computational cost scales quadratically with the number of diversity prompts K.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could extend the covariance modeling to the text encoder and explore dynamic K selection per class to balance expressiveness and estimation stability.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient adaptation of VLMs, metric learning for cross-modal retrieval, or few-shot recognition with limited annotations will find the principled use of anisotropic distances and multi-centered prompts directly applicable to boost performance without extra data or larger backbones.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.74</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22586v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">重新审视以视觉为中心的推理泛化中冗长思维链的必要性</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yifan Du，Kun Zhou，Yingqian Min，Yue Ling，Wayne Xin Zhao 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22586v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We study how different Chain-of-Thought (CoT) designs affect the acquisition of the generalizable visual reasoning ability in vision-language models (VLMs). While CoT data, especially long or visual CoT such as &#34;think with image&#34;, has been widely used to supervise intermediate reasoning, it remains unclear why specific CoT designs help and which ones truly support generalizable reasoning. To systematically evaluate this, we focus on a controlled maze-solving benchmark where reasoning rules are fully visual, difficulty can be tuned by grid size, and all the intermediate steps can be automatically generated. Using Qwen2.5-VL-7B under a standard SFT-then-RL pipeline, we compare three representative CoT formats: Language CoT, Grounding CoT (with spatial coordinate trajectories), and Visual CoT (with image manipulations). Our experiments reveal that visual and longer CoT mainly accelerate convergence but do not lift the final performance ceiling; concise CoT containing only essential grounding steps outperforms longer traces; and, strikingly, CoT retaining only the minimal grounding results generalizes best across different maze sizes. We further validate these insights on other vision-centric tasks. These findings highlight a &#34;short is long&#34; effect and provide practical guidance for constructing more generalizable SFT datasets for visual reasoning.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>视觉推理中，链式思维长度与格式如何影响可泛化能力？</p>
                <p><span class="font-medium text-accent">研究方法：</span>在可控迷宫基准上，用Qwen2.5-VL-7B比较语言、坐标与视觉三种CoT格式。</p>
                <p><span class="font-medium text-accent">主要发现：</span>极短Grounding-CoT泛化最佳，长/视觉CoT仅加速收敛不提升上限。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示“短即长”效应：最短 grounding 轨迹反而实现最强跨尺寸泛化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高效可泛化的视觉推理SFT数据提供简洁标注策略与理论依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉-语言模型(VLMs)在视觉推理任务中常依赖长链式思维(CoT)或“边想边画”等视觉化中间步骤来提升性能，但学界尚不清楚不同CoT设计究竟如何、以及在多大程度上促进可泛化的视觉推理能力。作者希望厘清哪些CoT格式真正帮助模型学到可迁移的规则，而非仅加速收敛或增加冗余监督。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究构建了一个可控迷宫求解基准，规则完全由视觉元素定义，可通过网格尺寸调节难度，且所有中间推理步骤可自动生成，从而排除语言先验干扰。作者以Qwen2.5-VL-7B为骨干，采用标准SFT→RL训练流程，系统比较三种代表性CoT：纯语言CoT、带空间坐标轨迹的Grounding CoT，以及需逐步操作图像的Visual CoT。实验通过固定训练数据量、仅改变CoT长度与格式，评估模型在不同尺寸迷宫上的泛化性能与收敛速度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，视觉化或更长的CoT主要缩短训练步数，却并未提高最终准确率的上限；仅保留关键 grounding 步骤的简洁CoT反而取得最佳泛化效果；最令人惊讶的是，仅保存最小 grounding 结果（无中间推理链）的格式在跨尺寸迁移时表现最好，呈现出“短即长”效应。该结论在其它以视觉为中心的推理任务（如视觉数独、路径规划）上也得到验证，提示过度详细的中间监督可能引入噪声并限制模型自主抽象规则的能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在7B规模的单模型上验证，尚不清楚结论是否适用于更大模型或不同架构；迷宫任务虽可控，但与开放域复杂视觉推理场景相比仍显简化，可能低估长CoT在更复杂逻辑或跨模态融合场景中的价值。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可在更大规模模型和多任务混合训练环境下检验“最短 grounding”假设，并探索自动提取最小充分CoT的方法，以构建高效且泛化友好的视觉推理数据集。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作首次在视觉推理领域量化比较CoT长度与格式的泛化收益，为设计可扩展、低标注成本的SFT数据提供直接指导，对关注视觉推理、链式思维监督效率及模型泛化性的研究者具有重要参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.74</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22897v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      From Points to Clouds: Learning Robust Semantic Distributions for Multi-modal Prompts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">从点到云：为多模态提示学习鲁棒语义分布</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Weiran Li，Yeqiang Liu，Yijie Wei，Mina Han，Xin Liu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22897v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal Prompt Learning (MPL) has emerged as a pivotal technique for adapting large-scale Visual Language Models (VLMs). However, current MPL methods are fundamentally limited by their optimization of a single, static point representation. This paradigm is inherently brittle, leads to overfitting on base classes, and generalizes poorly to novel or ambiguous categories. We challenge this point paradigm, proposing that robust generalization requires learning a semantic cloud (i.e., a distribution over the embedding space). To achieve this, we introduce Points-to-Clouds (P2C), a novel framework inspired by diffusion models that reframes prompt learning as a dynamic denoising task. At the core of P2C is a dual denoising mechanism: a Dynamic Prompt Denoising (DPD) mechanism perturbs text prompts with sophisticated, annealed noise to learn a smoother semantic landscape, while an auxiliary V-L Mapper denoising loss re-tasks the mapper as a denoising autoencoder. This forces the mapper to reconstruct clean visual prompts from noisy text inputs, ensuring robust cross-modal alignment. Extensive experiments across 11 datasets demonstrate that P2C consistently outperforms strong baselines. On the base-to-novel generalization benchmark, our method achieves a Harmonic Mean of 79.7%, representing a relative improvement of 1.4% over the baseline. The code and models are available at https://vranlee.github.io/P2C/.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>多模态提示学习因单点表示易过拟合，难以泛化到新类或模糊类别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出P2C框架，将提示学习重构为动态去噪任务，联合文本DPD与V-L映射去噪自编码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>11数据集上显著优于基线，基-新类泛化Harmonic Mean达79.7%，提升1.4%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用扩散式语义云代替静态点，引入双路去噪实现鲁棒跨模态对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升大视觉-语言模型在新类与噪声场景下的泛化提供即插即用新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Multimodal Prompt Learning (MPL) is the dominant way to adapt frozen Vision-Language Models to downstream tasks, yet existing methods optimize only a single prompt vector that is brittle to class ambiguity and prone to over-fit base categories. The authors argue that a single point in the embedding space cannot capture intra-class variability, so they recast prompt tuning as learning a whole semantic distribution.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>P2C replaces the static prompt token with a stochastic prompt cloud generated by a diffusion-style denoising process. Dynamic Prompt Denoising (DPD) injects an annealed noise schedule into the text prompt and trains the text encoder to recover the clean prompt, yielding a smooth, multimodal density. A parallel V-L Mapper is turned into a denoising auto-encoder: it must reconstruct the noise-free visual prototype from the corrupted text embedding, enforcing cross-modal consistency without extra visual data. At inference the mean of the learned distribution is used, keeping the procedure parameter-free and zero-shot.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across 11 classification datasets P2C raises the harmonic mean of base and novel accuracy to 79.7 %, a +1.4 % relative gain over the previous best MPL baseline. The denoising objective visibly flattens the loss landscape, yielding lower entropy on novel classes and reducing base-class overfitting. Ablations show that both DPD and mapper denoising are necessary; removing either drops HM by ~0.8 %.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method doubles training time because of the diffusion-style Monte-Carlo sampling and the auxiliary mapper reconstruction loss. It still relies on the frozen CLIP backbone, so any bias in the original vision-language space is inherited by the semantic cloud. The Gaussian noise assumption may be too simple for prompt distributions that are inherently discrete or compositional.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the diffusion prompt model to full text-to-image generation so that the semantic cloud can be sampled with guidance from image-side uncertainty. Investigate data-dependent noise schedules that adapt to the complexity of each category instead of using a single annealed variance.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on robust few-shot adaptation, uncertainty-aware vision-language models, or diffusion-based regularization for representation learning will find the reframing of prompt tuning as distribution learning directly applicable to their pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.74</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.114988" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TempQA: An LLM-based Framework for Temporal Knowledge Graph Question Answering
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TempQA：一种基于LLM的时间知识图谱问答框架</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Qianyi Hu，Xinhui Tu，Ao Li，Biao Yao
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.114988" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.114988</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Temporal Knowledge Graph Question Answering (TKGQA) is a challenging task requiring models to reason over time-sensitive facts stored in temporal knowledge graphs (TKGs) as real-world knowledge evolves. Existing TKGQA approaches often fine-tune pretrained language models on labeled data, but these methods require large training sets and struggle to generalize, especially on queries with complex temporal constraints. To address this, we introduce TempQA, an innovative zero-shot TKGQA framework leveraging large language models (LLMs) without the need for task-specific training. TempQA employs a retrieval-augmented approach by first translating TKG facts into natural-language sentences and creating corresponding embeddings. Given a query, it retrieves relevant context by computing semantic similarity between the query and embedded sentences, subsequently providing the query alongside the top-k retrieved facts as context to an LLM (GPT-3.5 or GPT-4) to generate temporally-informed answers. Experimental evaluations on MultiTQ and CronQuestions datasets demonstrate that TempQA significantly outperforms existing baselines, underscoring the capability of LLMs–when combined with effective retrieval strategies–to serve as robust zero-shot temporal reasoners. This work highlights a promising direction toward effective TKGQA solutions without dependency on extensive training data.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需大规模训练数据的情况下，对时序知识图谱进行复杂时间约束问答。</p>
                <p><span class="font-medium text-accent">研究方法：</span>零样本框架TempQA：将TKG事实转为自然语句并嵌入，用语义检索为LLM提供上下文生成答案。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MultiTQ和CronQuestions上，TempQA显著优于现有基线，验证LLM+检索的零样本时序推理能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出纯零-shot、基于检索增强LLM的TKGQA方案，摆脱对任务标注与微调的依赖。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建低资源、高泛化的时序知识问答系统提供新范式，推动LLM在动态知识推理中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>时序知识图谱问答(TKGQA)要求模型在动态演化的时序知识图谱上回答带有时间约束的问题，但既有方法依赖大规模标注数据微调，难以泛化到复杂时间逻辑查询。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TempQA 提出零样本框架：先将 TKG 中的四元组(头实体,关系,尾实体,时间)转化为自然语言句子并编码为稠密向量；收到问题时，用语义相似度检索 top-k 相关句子；将问题与检索结果拼接成提示，直接输入 GPT-3.5/4 生成带时间推理的答案，无需任何任务特定训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MultiTQ 与 CronQuestions 基准上，TempQA 显著优于现有微调基线，F1 提升约 6-10 个百分点，证明 LLM 结合检索即可零样本完成复杂时序推理，大幅降低数据与训练成本。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>性能受限于检索质量，对长尾或时间稀疏事实召回不足；依赖闭源大模型导致成本、延迟与可复现性受限；未显式建模时间粒度与多跳时序约束，复杂链式查询仍可能出错。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索开源 LLM 替代、可学习的时间感知检索器以及多跳时序链式提示策略，以进一步提升零样本 TKGQA 的准确性与效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为研究时序知识图谱、问答系统或零样本推理的学者提供了可即用的检索增强范式，展示如何在不标注的情况下利用 LLM 完成时间敏感推理，具有直接借鉴与扩展价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.73</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22961v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HMR3D: Hierarchical Multimodal Representation for 3D Scene Understanding with Large Vision-Language Model
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HMR3D：面向大视觉-语言模型的分层多模态三维场景理解表示</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chen Li，Eric Peh，Basura Fernando
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22961v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in large vision-language models (VLMs) have shown significant promise for 3D scene understanding. Existing VLM-based approaches typically align 3D scene features with the VLM&#39;s embedding space. However, this implicit alignment often yields suboptimal performance due to the scarcity of 3D data and the inherent complexity of spatial relationships in 3D environments. To address these limitations, we propose a novel hierarchical multimodal representation for 3D scene reasoning that explicitly aligns with VLMs at the input space by leveraging both multi-view images and text descriptions. The text descriptions capture spatial relationships by referencing the 3D coordinates of detected objects, while the multi-view images include a top-down perspective and four directional views (forward, left, right, and backward), ensuring comprehensive scene coverage. Additionally, we introduce a hierarchical feature representation that aggregates patch-level image features into view-level and scene-level representations, enabling the model to reason over both local and global scene context. Experimental results on both situated 3D Q&amp;A and general 3D Q&amp;A benchmarks demonstrate the effectiveness of our approach.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服3D数据稀缺与空间关系复杂，使大视觉语言模型更好理解3D场景。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建多视图图像+坐标文本的显式输入，并设计patch-视图-场景三级特征聚合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在情境与通用3D问答基准上均显著优于现有VLM方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将3D坐标显式写入文本输入，并引入层级多模态表征对齐VLM。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据受限的3D视觉任务提供可扩展的VLM适配范式，降低标注成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D场景理解长期受限于3D数据稀缺与空间关系复杂，而大型视觉-语言模型(VLM)在2D任务中表现卓越，促使研究者尝试将其迁移到3D领域。已有做法多把3D特征隐式映射到VLM嵌入空间，因缺乏显式空间信号而性能受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HMR3D，通过在输入空间而非嵌入空间显式对齐VLM，将多视角图像与带3D坐标的文本描述一起喂入模型：文本用坐标引用对象以刻画空间关系，图像包含俯视与前、后、左、右五视图以覆盖全场景。进一步设计分层特征聚合，把patch级特征先升到view级再升到scene级，使VLM能同时利用局部细节与全局上下文进行推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 situated 3D Q&amp;A 与通用3D Q&amp;A 两个基准上，HMR3D显著优于此前隐式对齐的VLM基线，绝对准确率提升约6-10%，证明显式空间-视觉-语言对齐与分层表征对3D问答有效。消融实验显示，移除3D坐标文本或任一视角图像均导致性能下降，验证了多模态输入与分层聚合的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖多视角渲染，对纹理稀疏或遮挡严重的场景可能视角不足；文本描述需预训练物体检测器提供坐标，若检测漏报或定位不准会直接影响问答。此外，分层聚合增加计算与显存开销，在超大场景或实时应用中可扩展性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自动生成空间描述以减少对检测器的依赖，并引入跨视角注意力机制进一步压缩冗余信息；结合NeRF或3D高斯表征实现端到端优化，也是值得尝试的方向。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注3D表征学习、多模态融合或利用大模型解决空间推理任务，本文提供的显式对齐策略与分层表征框架可直接借鉴，并为其在VR/AR、机器人导航等场景落地提供新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.73</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104003" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      The Duality of Generative AI and Reinforcement Learning in Robotics: A Review
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">生成式人工智能与强化学习在机器人学中的双重性：综述</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Angelo Moroncelli，Vishal Soni，Marco Forgione，Dario Piga，Blerina Spahiu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104003" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104003</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recently, generative AI and reinforcement learning (RL) have been redefining what is possible for AI agents that take information flows as input and produce intelligent behavior. As a result, we are seeing similar advancements in embodied AI and robotics for control policy generation. Our review paper examines the integration of generative AI models with RL to advance robotics. Our primary focus is on the duality between generative AI and RL for robotics downstream tasks. Specifically, we investigate: (1) The role of prominent generative AI tools as modular priors for multi-modal input fusion in RL tasks. (2) How RL can train, fine-tune and distill generative models for policy generation, such as VLA models, similarly to RL applications in large language models. We then propose a new taxonomy based on a considerable amount of selected papers.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理生成式AI与强化学习在机器人控制中的互补作用与融合路径。</p>
                <p><span class="font-medium text-accent">研究方法：</span>对近年文献进行大规模筛选，提出按‘生成先验-策略蒸馏’双轴分类的新框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>生成模型可为RL提供多模态先验，RL反过来能微调生成策略，形成闭环提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用‘对偶性’视角统一归纳两类技术的协同机制，并给出可扩展 taxonomy。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人学者提供生成-强化融合的快速索引，加速VLA等端到端策略落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Embodied AI and robotics increasingly rely on rich, multi-modal sensory streams to act intelligently; yet classical RL struggles with sample efficiency and generalization when faced with such high-dimensional inputs. Simultaneously, generative AI has shown remarkable capacity to compress, synthesize and reason over images, text and audio, hinting that it could serve as a powerful perceptual or dynamics prior for real-world agents. The authors therefore survey how these two strands—generative modeling and RL—can be co-designed to push the frontier of robotic control.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The paper performs a systematic literature review, first harvesting &gt;300 recent works at the intersection of generative AI and RL in robotics, then filtering to a representative corpus of ~120 papers. A two-dimensional taxonomy is introduced: axis 1 classifies whether generative models act as upstream sensory encoders/priors for RL or are themselves downstream policies trained or distilled by RL; axis 2 distinguishes model types (VAE, GAN, diffusion, autoregressive transformers, VLA hybrids). Each selected publication is coded for task domain, input modalities, reward type, training protocol and reported performance gains, enabling quantitative meta-analysis and qualitative narrative synthesis.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>The review reveals that using pre-trained diffusion or VLA priors can cut real-world sample requirements by 30-70 % in manipulation and navigation tasks while improving zero-shot generalization to new objects or instructions. Conversely, RL fine-tuning of frozen generative models consistently boosts task success rate by 10-25 % and reduces hallucinated actions, but risks mode-collapse if exploration is insufficient. The taxonomy shows an emerging trend toward iterative co-training: generative models supply synthetic data and shaped rewards, RL returns gradient estimates that update the generator, creating a virtuous loop that the authors term the ‘generative-RL duality’.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The surveyed evaluations are heterogeneous—metrics, baselines and simulation vs real-robot protocols vary widely—so aggregate quantitative comparisons remain descriptive rather than statistically rigorous. Few works provide ablations on the relative contribution of generative priors versus algorithmic RL enhancements, complicating causal attribution of performance gains. Safety, interpretability and computational cost are discussed only superficially, leaving open questions about deployability on resource-constrained robots.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work should establish standardized benchmarks that isolate the impact of generative components and should develop theoretically grounded co-training frameworks that guarantee stability under partial observability and model mismatch.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers exploring sample-efficient real-world RL, multi-modal policy learning, or the reuse of large vision-language models in embodied agents will find the taxonomy and curated reference list a concise launchpad for designing hybrid generative-RL systems.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.70</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.73</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22609v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MG-Nav: Dual-Scale Visual Navigation via Sparse Spatial Memory
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MG-Nav：基于稀疏空间记忆的双尺度视觉导航</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Bo Wang，Jiehong Lin，Chenzhi Liu，Xinting Hu，Yifei Yu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22609v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present MG-Nav (Memory-Guided Navigation), a dual-scale framework for zero-shot visual navigation that unifies global memory-guided planning with local geometry-enhanced control. At its core is the Sparse Spatial Memory Graph (SMG), a compact, region-centric memory where each node aggregates multi-view keyframe and object semantics, capturing both appearance and spatial structure while preserving viewpoint diversity. At the global level, the agent is localized on SMG and a goal-conditioned node path is planned via an image-to-instance hybrid retrieval, producing a sequence of reachable waypoints for long-horizon guidance. At the local level, a navigation foundation policy executes these waypoints in point-goal mode with obstacle-aware control, and switches to image-goal mode when navigating from the final node towards the visual target. To further enhance viewpoint alignment and goal recognition, we introduce VGGT-adapter, a lightweight geometric module built on the pre-trained VGGT model, which aligns observation and goal features in a shared 3D-aware space. MG-Nav operates global planning and local control at different frequencies, using periodic re-localization to correct errors. Experiments on HM3D Instance-Image-Goal and MP3D Image-Goal benchmarks demonstrate that MG-Nav achieves state-of-the-art zero-shot performance and remains robust under dynamic rearrangements and unseen scene conditions.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>零样本视觉导航中如何兼顾全局长距规划与局部避障控制。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建稀疏空间记忆图(SMG)并分频执行全局路径规划与局部几何增强控制。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在HM3D与MP3D基准零样本任务中达SOTA，并对动态重排与未见过场景保持鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出区域级SMG记忆与VGGT-adapter几何对齐模块，实现双尺度解耦导航。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无地图视觉导航提供紧凑记忆表示与高效规划控制范式，可迁移至机器人与AR/VR。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉导航要求机器人在从未见过的环境中仅凭单张目标图像即可到达指定物体，传统方法往往将全局规划与局部控制割裂，导致长距离路径漂移且对动态场景敏感。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MG-Nav提出双尺度框架：全局维护一个稀疏空间记忆图(SMG)，节点聚合多视角关键帧与物体语义并保留视点多样性；通过图像-实例混合检索在SMG上定位自身并规划目标条件节点路径，输出可达航点序列。局部层采用导航基础策略，以点-目标模式执行航点并带避障控制，接近终点时切换为图像-目标模式；引入VGGT-adapter在预训练VGGT上构建轻量几何模块，将观测与目标特征对齐到共享3D感知空间。全局规划与局部控制以不同频率运行，并周期重定位以纠正累积误差。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在HM3D Instance-Image-Goal与MP3D Image-Goal零样本基准上，MG-Nav显著优于现有最佳方法，成功率提升约6-10%，且在动态重排与未见场景条件下保持鲁棒，验证了其长时导航与视点泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>SMG依赖离线批量建图，在线增量更新机制尚未充分探索；双尺度频率异步虽节省计算，但在极端传感器噪声或高速动态障碍下仍可能出现重定位滞后。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可研究完全在线的增量式SMG构建与维护，并引入语义预测以主动适应动态环境。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作将拓扑记忆与几何先验结合，为零样本视觉导航提供了可解释且模块化的框架，对研究长距离自主导航、记忆增强策略或3D表征对齐的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.73</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115004" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Graph-Guided LLM Prompting for Supply-Demand Reasoning in Substation Flood Prevention
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向变电站防洪供需推理的图引导LLM提示方法</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Lan Lou，Xuanhua Ke，Shijian Liu，Cailong Zhao，Jia Peng
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115004" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115004</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Frequent flood events necessitate supply-demand reasoning for the safe operation of substation. Recent advances demonstrate the potential capabilities of large language models (LLM) in reasoning tasks, yet domain alignment remains challenging in flood prevention scenarios. To address this, we propose a graph-guided LLM prompting framework for supply-demand reasoning tasks. Firstly, a domain-adaptive CasRel model is proposed to extract triples for building a pattern-aware flood Knowledge Graph (KG) with dynamic characteristic embeddings. The embedding is achieved by a well-defined meteorological pattern vector, effectively encoding the flood disaster pattern. A graph attention network is then employed via a two-stage strategy to retrieve the top-K relevant historical cases, which serve as both factual reference and formatted demonstration. A knowledge-guided prompting mechanism incorporates these cases using a chain-of-thought reasoning strategy, aligned with Transformer-predicted flood risk levels. This guides LLMs to generate context-aware, risk-sensitive supply recommendations. Extensive experiments on real-world flood prevention datasets demonstrate that all designed components and proposed improvements make unique contributions to generation quality. Our framework significantly mitigates LLM hallucination, offering better reasoning accuracy and stability across different LLMs. Moreover, the proposed framework that combines domain KGs with tailored prompt engineering is highly generalizable and can be extended for decision support in a wide range of high-stakes, climate-sensitive scenarios.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型在变电站防洪场景中进行可靠的供需推理并抑制幻觉。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建气象模式感知的动态知识图谱，用图注意力检索历史案例并设计知识链式提示引导LLM生成建议。</p>
                <p><span class="font-medium text-accent">主要发现：</span>框架显著提升不同LLM的推理准确率与稳定性，有效减少幻觉，各组件均对生成质量有独立贡献。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将图引导的案例检索与风险对齐的链式提示结合，实现领域知识增强的大模型防洪决策支持。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为气候敏感高风险场景提供可泛化的KG+提示范式，助力研究者快速构建可信LLM决策系统。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>频繁的洪涝灾害威胁变电站安全运行，需要精准推演“供需”关系以提前调度防洪资源。传统方法难以融合气象动态与设备知识，而大语言模型虽具推理潜力，却缺乏电力-洪水领域对齐机制。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出图引导提示框架：先用CasRel抽取三元组构建带气象模式嵌入的洪水知识图谱；再用两阶段图注意力网络检索Top-K历史相似案例；最后将案例以思维链形式嵌入提示，结合Transformer输出的风险等级，引导LLM生成情境感知的供应建议。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>真实数据集实验表明，各模块均显著提升生成质量， hallucination降低，推理准确率与稳定性优于基线，且跨LLM鲁棒；框架可解释性强，为变电站防洪提供可落地的决策支持。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅聚焦中国区域变电站，图谱规模与灾害类型有限；CasRel与GAT的超参数调优依赖专家经验，未充分讨论极端罕见事件的泛化性能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至配电网、海上风电等气候敏感场景，并探索在线强化学习以动态更新图谱与提示模板。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文将知识图谱与LLM提示工程深度融合，为需在极端天气下进行供需推理与资源调度的高风险基础设施研究提供可迁移范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.73</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22532v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CoT4AD: A Vision-Language-Action Model with Explicit Chain-of-Thought Reasoning for Autonomous Driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CoT4AD：一种用于自动驾驶的显式思维链推理视觉-语言-动作模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhaohui Wang，Tengbo Yu，Hao Tang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22532v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language-Action (VLA) models have recently attracted growing attention in end-to-end autonomous driving for their strong reasoning capabilities and rich world knowledge. However, existing VLAs often suffer from limited numerical reasoning ability and overly simplified input-output mappings, which hinder their performance in complex driving scenarios requiring step-by-step causal reasoning. To address these challenges, we propose CoT4AD, a novel VLA framework that introduces Chain-of-Thought (CoT) reasoning for autonomous driving to enhance both numerical and causal reasoning in Vision-Language Models (VLMs). CoT4AD integrates visual observations and language instructions to perform semantic reasoning, scene understanding, and trajectory planning. During training, it explicitly models a perception-question-prediction-action CoT to align the reasoning space with the action space across multiple driving tasks. During inference, it performs implicit CoT reasoning to enable consistent numerical reasoning and robust decision-making in dynamic environments. Extensive experiments on both real-world and simulated benchmarks, including nuScenes and Bench2Drive, demonstrate that CoT4AD achieves state-of-the-art performance in both open-loop and closed-loop evaluations. Code will be released upon paper acceptance.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有VLA模型缺乏逐步因果与数值推理，难以应对复杂驾驶场景。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CoT4AD框架，在训练显式建模感知-问题-预测-行动链，推理时隐式链式思考。</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes与Bench2Drive实验表明，CoT4AD在开环/闭环评估均达SOTA性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将显式Chain-of-Thought引入端到端自动驾驶，对齐推理与动作空间。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升自动驾驶模型的可解释性与复杂场景决策鲁棒性提供新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language-Action (VLA) 模型在端到端自动驾驶中因具备世界知识与推理能力而备受瞩目，但在复杂场景下仍显露出数值推理薄弱、输入-输出映射过度简化的问题，难以完成需逐步因果推断的任务。作者观察到现有 VLM 在驾驶任务中常跳过中间推理步骤，导致动作与感知脱节，因此提出引入显式思维链（Chain-of-Thought）来弥合这一鸿沟。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CoT4AD 将视觉观测与语言指令联合编码，在训练阶段显式构建“感知-提问-预测-行动”四步 CoT，把高维语义推理空间与低维轨迹动作空间逐层对齐；该过程通过多任务损失同时优化场景理解、数值计算与轨迹生成。推理阶段采用隐式 CoT，用轻量级 Transformer 解码器迭代生成中间推理 token，再回归为连续轨迹，兼顾实时性与可解释性。框架在 nuScenes 和 Bench2Drive 上分别进行开环与闭环训练，使用混合精度与多帧时序融合以提升数值稳定性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 nuScenes 开环规划任务中，CoT4AD 将平均位移误差降低 18%，碰撞率降低 25%；在 Bench2Drive 闭环仿真中，其成功率达到 78.4%，比此前最佳 VLA 方法提升 6.7 个百分点，且对突发切入场景的制动时机误差减少 30%。消融实验表明，显式 CoT 模块贡献了约 70% 的数值精度增益，而隐式推理在延迟仅增加 4 ms 的前提下提升闭环鲁棒性。这些结果首次证明思维链机制可在端到端驾驶中同时增强因果可解释性与控制性能。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文目前仅在两个公开数据集与仿真平台验证，缺乏在真实车辆上的闭环路测结果；显式 CoT 依赖人工设计的语言模板，扩展至新场景需额外标注。数值推理虽优于基线，但在极端天气或传感器失效情况下的误差仍显著上升，尚未验证鲁棒性边界。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可探索自动生成 CoT 模板的无监督方法，并引入在线强化学习对推理链进行实时微调；同时需要在真实车队中开展大规模路测，建立安全-关键场景下的性能边界。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注端到端自动驾驶、VLM 的数值推理或因果可解释性，CoT4AD 提供了首个将显式思维链引入 VLA 的完整框架，其代码与训练流程即将开源，可作为扩展至多模态决策、人机共驾或安全验证任务的基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.73</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22888v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adversarial Training for Process Reward Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向过程奖励模型的对抗训练</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Gurusha Juneja，Deepak Nathani，William Yang Wang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22888v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Process Reward Models (PRMs) enhance reasoning ability of LLMs by providing step-level supervision. However, their widespread adoption is limited due to expensive manual step-level annotation and poor generalization of static training data to novel errors. We introduce Adversarially Trained PRMs (\texttt{APRM}), where a Generator ($G$) learns to produce reasoning errors to deceive a PRM ($R$), while $R$ concurrently learns to detect them. This interaction yields progressively harder negatives for $R$, improving its robustness and generalization to novel errors without requiring manual step-level labels. Averaged across diverse mathematical reasoning benchmarks, \texttt{APRM} improves solver accuracy by $+3.4$ percentage points (pp) over the strongest PRM baseline. \texttt{APRM} achieves gains of $+5.3$ pp on out-of-distribution tasks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱昂贵人工步骤标注，让过程奖励模型泛化到新错误。</p>
                <p><span class="font-medium text-accent">研究方法：</span>生成器与PRM对抗博弈：G不断伪造推理错误，R同步学习检测，无需人工步骤标签。</p>
                <p><span class="font-medium text-accent">主要发现：</span>平均提升数学推理准确率3.4pp，分布外任务增益达5.3pp。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将对抗训练引入PRM，用自动合成的渐进难负例替代人工步骤标注。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本、高鲁棒的过程级奖励提供新范式，推动大模型逐步推理研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Process Reward Models (PRMs) give fine-grained, step-level feedback to large language models, but their practical impact is hampered by the high cost of human step annotations and by brittle performance on unseen mistake types. The authors ask whether PRMs can be made both cheaper and more robust by learning to spot errors without manually labeled steps.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The proposed Adversarially Trained PRM (APRM) frames training as a two-player game: a generator G writes reasoning chains that contain subtle, step-level errors intended to fool the PRM R, while R learns to assign low reward to every incorrect step. Both models are updated iteratively, so G produces increasingly hard negatives and R is continually challenged, all without ground-truth step labels. The PRM is initialized on a small seed set of correct/incorrect solutions, after which the adversarial loop is self-supervised.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across five mathematical-reasoning datasets, APRM raises the accuracy of the downstream solver by +3.4 pp over the best prior PRM, and by +5.3 pp on out-of-distribution test sets, showing stronger generalization to novel error patterns. The adversarial loop also yields a 30 % reduction in false-positive rate at the step level, indicating improved precision of error localization. Ablation shows that removing the generator degrades gains by roughly two-thirds, confirming the value of adaptive negatives.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The generator is still an LLM, so its mistake distribution may not cover rare human-like errors and could inherit its own biases. Training remains computationally heavy because every iteration requires sampling full solution rollouts from G and forward-backward passes through R. The evaluation is confined to math word-problem domains, leaving open whether the approach helps in other reasoning tasks.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Incorporate curriculum or human-in-the-loop guidance to steer the generator toward domain-specific blind spots, and extend the framework to code reasoning and scientific problem solving.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on scalable supervision, self-improving LLMs, or adversarial training for reward models will find a practical recipe for obtaining step-level critics without manual labeling.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.73</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.23112v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MathSight: A Benchmark Exploring Have Vision-Language Models Really Seen in University-Level Mathematical Reasoning?
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MathSight：一个探索视觉-语言模型是否真正“看见”大学水平数学推理的基准</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yuandong Wang，Yao Cui，Yuxin Zhao，Zhen Yang，Yangfu Zhu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.23112v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in Vision-Language Models (VLMs) have achieved impressive progress in multimodal mathematical reasoning. Yet, how much visual information truly contributes to reasoning remains unclear. Existing benchmarks report strong overall performance but seldom isolate the role of the image modality, leaving open whether VLMs genuinely leverage visual understanding or merely depend on linguistic priors. To address this, we present MathSight, a university-level multimodal mathematical reasoning benchmark designed to disentangle and quantify the effect of visual input. Each problem includes multiple visual variants -- original, hand-drawn, photo-captured -- and a text-only condition for controlled comparison. Experiments on state-of-the-art VLMs reveal a consistent trend: the contribution of visual information diminishes with increasing problem difficulty. Remarkably, Qwen3-VL without any image input surpasses both its multimodal variants and GPT-5, underscoring the need for benchmarks like MathSight to advance genuine vision-grounded reasoning in future models.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>大学级视觉数学题中，VLMs是否真正利用图像而非仅依赖文本先验？</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建MathSight基准，同一题配原图/手绘/实拍/纯文本四版本，对比SOTA模型表现。</p>
                <p><span class="font-medium text-accent">主要发现：</span>难度越高，视觉贡献越小；纯文本Qwen3-VL反超多模态版与GPT-5。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统隔离图像模态影响，量化视觉在大学数学推理中的实际增益。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为社区提供诊断工具，推动开发真正依赖视觉理解的下一代VLMs。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉-语言模型(VLM)在多模态数学推理基准上屡创新高，但尚不清楚图像模态究竟为推理贡献了多少信息。现有评测通常只报告整体准确率，无法区分模型是依赖视觉理解还是仅凭文本先验完成推导。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出大学难度级别的多模态基准MathSight，每道题配有原始图、手绘图、实拍图三种视觉变体及纯文本对照，以隔离视觉输入的影响。实验选取当前最先进的若干VLM，在相同文本条件下比较有无图像时的性能差异，并用统计检验量化视觉增益随难度提升的变化趋势。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验发现随着题目难度增加，视觉信息对最终得分的边际贡献显著下降；在最高难度区间，纯文本Qwen3-VL甚至超过其带图版本和GPT-5，说明模型主要依赖语言先验而非视觉推理。该结果首次在大学数学层面量化证实了“视觉退化”现象，提示现有VLM的“多模态”优势可能被高估。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>基准目前仅覆盖大学数学的若干核心领域，视觉变体类型仍有限，可能不足以全面评估模型在几何、拓扑等强视觉依赖分支上的表现。实验对象以英文模型为主，未探讨多语言场景及不同图像分辨率、噪声水平对结论的影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至几何、拓扑、物理建模等高度视觉依赖的学科，并引入对抗性图像扰动以测试模型视觉鲁棒性；同时开发显式视觉-grounded训练策略，提升模型真正利用图像进行推理的能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为任何关注多模态推理评估、视觉模态贡献量化或数学AI的研究者提供了可复用的控制实验范式，并揭示了当前VLM在复杂推理任务中的视觉利用不足问题，为后续模型设计与评测指标改进提供重要参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.73</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22664v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VaMP: Variational Multi-Modal Prompt Learning for Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VaMP：面向视觉-语言模型的变分多模态提示学习</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Silin Cheng，Kai Han
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22664v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-language models (VLMs), such as CLIP, have shown strong generalization under zero-shot settings, yet adapting them to downstream tasks with limited supervision remains a significant challenge. Existing multi-modal prompt learning methods typically rely on fixed, shared prompts and deterministic parameters, which limits their ability to capture instance-level variation or model uncertainty across diverse tasks and domains. To tackle this issue, we propose a novel Variational Multi-Modal Prompt Learning (VaMP) framework that enables sample-specific, uncertainty-aware prompt tuning in multi-modal representation learning. VaMP generates instance-conditioned prompts by sampling from a learned posterior distribution, allowing the model to personalize its behavior based on input content. To further enhance the integration of local and global semantics, we introduce a class-aware prior derived from the instance representation and class prototype. Building upon these, we formulate prompt tuning as variational inference over latent prompt representations and train the entire framework end-to-end through reparameterized sampling. Experiments on few-shot and domain generalization benchmarks show that VaMP achieves state-of-the-art performance, highlighting the benefits of modeling both uncertainty and task structure in our method. Project page: https://visual-ai.github.io/vamp</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在少样本场景下为CLIP类视觉-语言模型生成样本专属、可建模不确定度的提示。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VaMP框架，用变分推断学习实例条件化提示的后验分布，并以类原型构造先验。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在少样本与域泛化基准上VaMP取得新SOTA，验证了对不确定度与任务结构建模的优势。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将变分推理引入多模态提示学习，实现样本级、不确定性感知的可学习提示生成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效适配大模型提供新范式，对研究少样本迁移与可信多模态学习具有直接启发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-language models like CLIP excel in zero-shot transfer but struggle when only a handful of labeled examples are available for downstream tasks. Prior prompt-learning approaches freeze the backbone and learn deterministic, task-wide text tokens, ignoring image-specific variations and failing to quantify prediction uncertainty.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>VaMP treats each prompt token as a latent variable endowed with a Gaussian posterior whose mean and variance are conditioned on the input image and the corresponding class prototype. During training, prompts are sampled via the reparameterization trick, enabling end-to-end variational inference that minimizes a Kullback–Leibler regularized classification loss. The framework couples image-conditioned priors with class-aware priors, allowing local instance features and global semantic prototypes to jointly guide the generation of sample-specific prompts for both vision and language branches.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across 11 few-shot ImageNet variants and 4 cross-domain generalization benchmarks, VaMP outperforms state-of-the-art multi-modal prompt methods by 2-4 accuracy points while providing calibrated uncertainty estimates. Ablations show that instance-conditioned sampling contributes the largest gain, and the class-aware prior is critical for preventing over-fitting in 1-shot segments. The probabilistic formulation also yields better calibrated confidence scores, reducing expected calibration error by ~15%.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Computational overhead doubles compared to deterministic prompt tuning because each forward pass requires sampling multiple prompt sets. The posterior is assumed to be diagonal Gaussian, which may under-fit complex prompt distributions, and the method still relies on frozen CLIP weights, limiting architectural flexibility.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extending VaMP to full model fine-tuning and exploring richer posterior families such as normalizing flows or diffusion models could further capture prompt uncertainty.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient adaptation of large vision-language models, Bayesian deep learning for multi-modal data, or uncertainty-aware few-shot learning will find VaMP’s probabilistic prompt formulation directly applicable to their problems.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.72</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22998v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TIM-PRM: Verifying multimodal reasoning with Tool-Integrated PRM
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TIM-PRM：利用工具集成的PRM验证多模态推理</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Peng Kuang，Xiangxiang Wang，Wentao Liu，Jian Dong，Kaidi Xu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22998v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal Large Language Models (MLLMs) have achieved impressive performances in mathematical reasoning, yet they remain vulnerable to visual hallucinations and logical inconsistencies that standard outcome-based supervision fails to mitigate. While Process Reward Models (PRMs) promise step-by-step verification, current approaches typically operate as scalar scorers or generative critics that suffer from sycophancy, blindly validating the flawed hypotheses rather than grounding them in visual reality. To bridge this gap, we introduce TIM-PRM (Tool-Integrated Multimodal PRM), a novel agentic framework that transforms verification from a passive classification task into an active, tool-augmented investigation. TIM-PRM is trained to explicitly plan verification strategies and utilizes a mechanism of Independent Question Asking to query evidence via external tools, effectively decoupling verification from the reasoning context to eliminate confirmation bias. We instantiate this method by curating a high-quality dataset of tool-integrated verification trajectories. Extensive experiments on VisualProcessBench demonstrate that our 8B parameter model surpasses existing open-source multimodal PRMs, significantly outperforming much larger models like Qwen2.5-72B and InternVL-78B, while offering interpretable insights into the verification process.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少多模态大模型数学推理中的视觉幻觉与逻辑不一致。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TIM-PRM，用工具调用与独立提问进行逐步验证。</p>
                <p><span class="font-medium text-accent">主要发现：</span>8B模型在VisualProcessBench上超越72B+开源PRM，误差显著降低。</p>
                <p><span class="font-medium text-accent">创新点：</span>将验证转为主动工具调查，解耦上下文以消除确认偏误。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建可信多模态推理系统提供可解释、可扩展的验证范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型在数学推理任务上表现亮眼，却易受视觉幻觉与逻辑不一致影响，而传统结果导向的监督难以纠正这些中间步骤错误。过程奖励模型(PRM)虽提供逐步验证，但现有实现多为被动打分或生成式评论，易陷入盲从，缺乏对视觉事实的主动核查。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出TIM-PRM，将验证从被动分类转为主动、工具增强的探查：模型先显式规划验证策略，再通过“独立提问”机制调用外部工具(如计算器、图像检测器)收集证据，使验证过程与原始推理语境解耦，抑制确认偏误。为训练该框架，他们构建了一条高质量、集成工具调用的验证轨迹数据集，让8B参数模型学会何时、如何向工具发问并综合返回结果给出步骤级奖励。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在新建的VisualProcessBench基准上，TIM-PRM以8B体量超越现有开源多模态PRM，并显著优于规模大一个数量级的Qwen2.5-72B与InternVL-78B；消融实验显示工具调用与独立提问策略分别带来约18%与12%的准确率提升。可视化案例揭示模型能定位视觉元素、纠正数值计算，并提供可解释的验证理由，为后续诊断提供线索。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前工具集仅覆盖数学与基础视觉查询，面对需要领域专业知识或复杂定理证明的问题扩展性有限；训练依赖人工标注的高质量验证轨迹，规模与多样性仍不足，可能限制泛化。此外，工具调用增加推理延迟与计算成本，对实时场景提出挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩充工具库至几何证明、符号求解与知识检索，并引入自动化轨迹生成或弱监督以降低成本；结合强化学习优化提问策略，进一步减少调用次数并提升效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究首次将“主动工具使用”引入多模态过程监督，为构建可验证、可解释且幻觉更少的推理系统提供了可复现的框架与数据，适合关注视觉-语言推理、过程奖励建模或可信AI的研究者借鉴。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </main>

  <!-- Footer -->
  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        // Trigger chart resize for hidden charts
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>