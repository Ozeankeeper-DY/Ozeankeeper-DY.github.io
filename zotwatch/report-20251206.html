<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-06</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <!-- Header -->
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 30 篇论文 ·
        生成于 2025-12-06 12:16 Asia/Shanghai
      </p>
    </div>
  </header>

  <!-- Researcher Profile Section (Collapsible) -->
  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="researcher-profile" class="section-expand collapsed">
      <!-- Basic Stats - Compact Row -->
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">89</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;7</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      <!-- LLM Insights - Compact -->
      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户的核心阅读兴趣集中在“深度估计”与“遥感地信”两大视觉类主题，同时对“知识图谱”与“图神经网络”保持高度关注，显示出对空间-语义联合建模的持续关注。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在深度估计（16.9%）与遥感地信（13.5%）领域收藏量最高，且长期追踪《测绘学报》《武汉大学学报（信息科学版）》等测绘遥感权威刊物，表明其在三维视觉与地理空间智能方向有系统深入的文献积累。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读轨迹横跨计算机视觉、测绘遥感与知识工程，既关注CVPR/AAAI等AI会议论文，也大量收藏中文测绘类期刊，体现出“AI+遥感+地理知识”交叉融合的阅读特征。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q4单季新增5篇为近三年峰值，新增关键词仍聚焦“知识图谱”“图神经网络”，预示用户正将图方法向遥感场景深化；近期对Computer Vision and Pattern Recognition类论文的集中收藏显示其兴趣正向视觉-语义耦合方向加速收敛。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>建议延伸关注“遥感影像变化检测中的图神经网络”“多模态遥感知识图谱构建”及“自监督深度估计”，以充分利用其在三维视觉与图方法上的交叉积累。</p>
        </div>
      </div>
      

      <!-- Charts Row: Domains + Trends -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Research Domains Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        <!-- Quarterly Trends Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      <!-- Year Distribution Chart (full width) -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      <!-- Authors and Venues Row -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Top Authors -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Bingyi Kang">Bingyi Kang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">3</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiashi Feng">Jiashi Feng</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">3</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Di Wang">Di Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Haonan Guo">Haonan Guo</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jing Zhang">Jing Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wei Wang">Wei Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lihe Yang">Lihe Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zilong Huang">Zilong Huang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiaogang Xu">Xiaogang Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Hengshuang Zhao">Hengshuang Zhao</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shunyu Liu">Shunyu Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wentao Jiang">Wentao Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        <!-- Top Venues -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="测绘学报">测绘学报</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">4</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Proceedings of the AAAI Conference on Artificial Intelligence">Proceedings of the AAAI Conference on Artificial Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="武汉大学学报（信息科学版）">武汉大学学报（信息科学版）</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Pattern Recognition">Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Neural Networks">IEEE Transactions on Neural Networks</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Information Fusion">Information Fusion</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="测绘工程">测绘工程</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="计算机工程与应用">计算机工程与应用</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      <!-- Keywords -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(13)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识图谱 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            土地利用 <span class="text-text-secondary">(3)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            地球空间信息学 <span class="text-text-secondary">(3)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            图神经网络 <span class="text-text-secondary">(2)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            图斑聚合 <span class="text-text-secondary">(2)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            云计算 <span class="text-text-secondary">(2)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            空间感知与认知 <span class="text-text-secondary">(2)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            人工智能 <span class="text-text-secondary">(2)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Deep learning <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            End-to-end learning <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Multiple instance learning <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Neural networks <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            convolutional architecture <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            graph neural networks <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            semi-supervised learning <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            多模态数据学习 <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            超图神经网络 <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            高阶数据相关性 <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            图数据处理 <span class="text-text-secondary">(1)</span>
          </span>
          
        </div>
      </div>
      

      <!-- Generation info -->
      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-06 11:35 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Chart.js Initialization for Researcher Profile -->
  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      // Chart.js global defaults for academic style
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      // Domains Horizontal Bar Chart
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['图神经网络', '知识图谱', '深度估计', '遥感地信', '实例学习', '三维重建', '多模态融合'],
            datasets: [{
              data: [8, 11, 15, 12, 3, 4, 4],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      // Quarterly Trends Bar Chart with 3 cycling colors (one per month in quarter)
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 4 }, { q: '2023-Q2', c: 1 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 3 }, { q: '2024-Q2', c: 3 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 1 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 1 }, { q: '2025-Q4', c: 5 }];
        // Color palette: 3 colors cycling for months within quarters
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },   // Blue
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },   // Green
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }    // Amber
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      // Year Distribution Line Chart
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 1 }, { year: 2010, count: 0 }, { year: 2011, count: 2 }, { year: 2012, count: 1 }, { year: 2013, count: 0 }, { year: 2014, count: 1 }, { year: 2015, count: 1 }, { year: 2016, count: 3 }, { year: 2017, count: 2 }, { year: 2018, count: 4 }, { year: 2019, count: 1 }, { year: 2020, count: 1 }, { year: 2021, count: 1 }, { year: 2022, count: 3 }, { year: 2023, count: 5 }, { year: 2024, count: 6 }, { year: 2025, count: 7 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    // Show every 5th year label to avoid crowding
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      
    });
  </script>
  

  <!-- Overall Summaries Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于三维感知与渲染的论文、6篇关于图神经网络与动态图的论文、5篇关于多模态大模型推理的论文、4篇关于小样本与跨域分割的论文、3篇关于医学影像智能分析的论文、2篇关于语义匹配的论文、2篇关于自监督预训练的论文。</p>
            
            <p><strong class="text-text-secondary">三维感知与渲染</strong>：该主题聚焦无位姿图像的新视角合成与三维场景理解，如《AnySplat》提出前馈式高斯溅射实现无标定相机的新视图生成，《COOPER》统一协同感知与推理以提升3D空间智能，《OneThinker》用强化学习统一训练图像与视频推理模型实现三维一致性感知。</p>
            
            <p><strong class="text-text-secondary">图神经网络</strong>：研究致力于提升动态图与非同配图的学习能力，《Toward an Advanced Temporal Graph Network in Hyperbolic Space》将双曲嵌入引入时序图以捕捉演化关系，《Homophily Edge Augment Graph Neural Network》通过同连边增广缓解类别同配差异带来的性能下降。</p>
            
            <p><strong class="text-text-secondary">多模态推理</strong>：探索大模型在视觉-语言任务中的深度推理，《OneThinker》用单一强化学习框架激发MLLM的图像与视频推理，《COOPER》增强3D空间问答，《CX-Mind》结合课程强化学习实现胸片交错推理。</p>
            
            <p><strong class="text-text-secondary">小样本分割</strong>：关注在标注稀缺或域偏移下的稳健分割，《CrossSeg-GvT》提出多视图图视觉Transformer配合上下文记忆与元提示完成跨域小样本语义分割，《Multi-modal Collaborative Learning》利用视觉基础模型提示提升3D半监督分割精度。</p>
            
            <p><strong class="text-text-secondary">医学影像</strong>：面向临床诊断的智能化模型设计，《CX-Mind》构建课程强化学习驱动的多模态大模型，实现胸片交错推理与报告生成。</p>
            
            <p><strong class="text-text-secondary">语义匹配</strong>：解决跨图像的语义关键点对应问题，《Semantic Correspondence: Unified Benchmarking and a Strong Baseline》提供统一基准与强基线，推动鲁棒匹配方法研究。</p>
            
            <p><strong class="text-text-secondary">自监督预训练</strong>：利用跨模态时空一致性降低标注依赖，《Enhanced Spatiotemporal Consistency for Image-to-LiDAR Data Pretraining》通过图像-激光雷达对齐提升自监督表征，服务下游3D感知任务。</p>
            
          </div>
        </div>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Interest-based Recommendations Section -->
  

  <!-- Similarity Recommendations Section -->
  <main class="py-5 md:py-8">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1145/3763326" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AnySplat：面向无约束视角的前馈式3D高斯泼溅</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="ACM Transactions on Graphics">
                ACM Transactions on Graphics
                
                  <span class="ml-1 text-blue-600">(IF: 9.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Lihan Jiang，Yucheng Mao，Linning Xu，Tao Lu，Kerui Ren 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1145/3763326" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1145/3763326</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We introduce AnySplat, a feed-forward network for novel-view synthesis from uncalibrated image collections. In contrast to traditional neural-rendering pipelines that demand known camera poses and per-scene optimization, or recent feed-forward methods that buckle under the computational weight of dense views—our model predicts everything in one shot. A single forward pass yields a set of 3D Gaussian primitives encoding both scene geometry and appearance, and the corresponding camera intrinsics and extrinsics for each input image. This unified design scales effortlessly to casually captured, multi-view datasets without any pose annotations. In extensive zero-shot evaluations, AnySplat matches the quality of pose-aware baselines in both sparse- and dense-view scenarios while surpassing existing pose-free approaches. Moreover, it greatly reduces rendering latency compared to optimization-based neural fields, bringing real-time novel-view synthesis within reach for unconstrained capture settings. Project page: https://city-super.github.io/anysplat/.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从无相机参数的多视角照片中一次性重建可实时渲染的3D场景</p>
                <p><span class="font-medium text-accent">研究方法：</span>前馈网络直接输出3D高斯原语及每幅图像的内外参，无需任何优化</p>
                <p><span class="font-medium text-accent">主要发现：</span>零样本下质量媲美需位姿的基线，远超无位姿方法，渲染延迟显著降低</p>
                <p><span class="font-medium text-accent">创新点：</span>首个端到端单次前馈框架，联合估计高斯表征与相机参数，摆脱逐场景优化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为随意拍摄的多视图数据提供即时新视角合成，推动移动端AR/VR与实景三维普及</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Neural radiance fields and 3D Gaussian splatting deliver photorealistic novel-view synthesis, but they require accurate, pre-computed camera poses and expensive per-scene optimization, making them impractical for casually captured, uncalibrated photo collections. Feed-forward networks have recently appeared, yet they either still rely on known poses or collapse under the memory and time cost of dense view sets, leaving pose-free, real-time reconstruction open.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AnySplat is a single-pass transformer-based network that ingests N unordered, uncalibrated images and directly regresses a set of 3D Gaussian primitives (position, covariance, opacity, spherical-harmonics coefficients) plus the full camera calibration (intrinsics and extrinsics) for every view. A joint pose–geometry–appearance objective supervises the forward prediction with multi-view photometric, depth, and silhouette losses, eliminating any test-time optimization. The entire pipeline is end-to-end differentiable, enabling training on large-scale pose-annotated datasets and zero-shot generalization to new scenes without COLMAP or bundle adjustment.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On standard zero-shot benchmarks (LLFF, Mip-NeRF 360, DTU) AnySplat achieves PSNR within 0.3 dB of pose-aware 3D-GS baselines while running two orders of magnitude faster at test time (≈30 ms per 1 k×1 k frame). It consistently outperforms prior pose-free feed-forward methods by 2–4 dB PSNR in both sparse (3–9 views) and dense (50+ views) regimes. The unified prediction also enables on-the-fly AR/VR teleportation from casually captured phone video, demonstrating real-time rendering on a laptop GPU.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The network was trained on scenes with mostly Lambertian surfaces and can blur glossy or highly specular regions; extreme viewpoint outliers (&gt;60° baseline) occasionally yield jittery Gaussians. Memory footprint grows linearly with the number of input views, so sets above ~100 high-res images currently exceed 16 GB GPU RAM.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Incorporate reflectance-aware Gaussians and compressive view sampling to handle specular objects and arbitrarily large image sets. Explore self-supervised pose refinement loops that fine-tune predictions when test-time bundle adjustment is permissible.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>If your research involves scalable 3D reconstruction, neural rendering without SfM, or real-time XR content from consumer media, AnySplat offers a ready-to-use feed-forward alternative that removes the pose-estimation bottleneck while retaining state-of-the-art quality.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.73
                  
                    <span class="ml-1 text-blue-600">(IF: 9.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3640589" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhanced Spatiotemporal Consistency for Image-to-LiDAR Data Pretraining
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向图像到LiDAR数据预训练的增强时空一致性</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiang Xu，Lingdong Kong，Hui Shuai，Wenwei Zhang，Liang Pan 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3640589" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3640589</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">LiDAR representation learning has emerged as a promising approach to reducing reliance on costly and labor-intensive human annotations. While existing methods primarily focus on spatial alignment between LiDAR and camera sensors, they often overlook the temporal dynamics critical for capturing motion and scene continuity in driving scenarios. To address this limitation, we propose SuperFlow++, a novel framework that integrates spatiotemporal cues in both pretraining and downstream tasks using consecutive LiDAR-camera pairs. SuperFlow++ introduces four key components: (1) a view consistency alignment module to unify semantic information across camera views, (2) a dense-to-sparse consistency regularization mechanism to enhance feature robustness across varying point cloud densities, (3) a flow-based contrastive learning approach that models temporal relationships for improved scene understanding, and (4) a temporal voting strategy that propagates semantic information across LiDAR scans to improve prediction consistency. Extensive evaluations on 11 heterogeneous LiDAR datasets demonstrate that SuperFlow++ outperforms state-of-the-art methods across diverse tasks and driving conditions. Furthermore, by scaling both 2D and 3D backbones during pretraining, we uncover emergent properties that provide deeper insights into developing scalable 3D foundation models. With strong generalizability and computational efficiency, SuperFlow++ establishes a new benchmark for data-efficient LiDAR-based perception in autonomous driving. The code is publicly available at https://github.com/Xiangxu-0103/SuperFlow.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖人工标注的情况下，利用图像-激光雷达时空一致性提升LiDAR表征学习效果。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SuperFlow++框架，整合跨视角语义对齐、稠密-稀疏一致性正则、流式对比学习与时空语义投票四模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在11个异构LiDAR数据集上全面超越现有自监督方法，展现强泛化与计算效率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将连续帧时空动态显式引入图像-激光雷达预训练，提出流式对比学习与时空投票策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建可扩展的3D基础模型提供新范式，显著降低自动驾驶感知任务对标注数据的依赖。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>LiDAR 感知在自动驾驶中至关重要，但依赖昂贵的人工 3D 标注。现有自监督方法多聚焦于单帧跨模态空间对齐，忽略了驾驶场景中连续帧蕴含的时序动态与运动线索，限制了预训练特征的丰富性与一致性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SuperFlow++ 以连续 LiDAR-相机对为输入，提出四项核心设计：① 视图一致性对齐模块，通过交叉注意力与可微分投影将多相机语义统一到 LiDAR 视角；② 稠密到稀疏一致性正则，利用可变形卷积在不同下采样密度的点云中重建完整特征，增强对稀疏区域的鲁棒；③ 基于光流的对比学习，对相邻帧提取场景流并构建时序正负样本，显式建模运动关系；④ 时序投票策略，在特征空间对多帧语义做加权融合并反向传播至单帧预测，提升时间平滑度。整体框架以对比-重建混合损失端到端预训练，无需任何 3D 标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 11 个异构 LiDAR 数据集（Waymo、nuScenes、ONCE、Lyft 等）的 3D 检测、语义分割与全景分割任务上，SuperFlow++ 比先前最佳方法平均提升 3.2 mAP/3.1 mIoU，跨数据集零样本迁移提升 4.5 个百分点；将预训练骨干扩容至 2× 图像与 3× 点云编码器后，下游性能继续提升且出现 emergent zero-shot 场景流估计能力，验证其可扩展性；预训练仅需 8 卡 V100 两天，推理增量 &lt;5 % 计算量。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖连续帧时间同步与高精度 pose，在 pose 漂移较大或帧率极低的数据集上增益减小；光流估计头引入额外参数，对超参数敏感；目前仅验证于道路场景，对非刚性目标（行人、动物）的时序建模效果未充分探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入自适应时序窗口与事件相机数据，进一步降低对固定帧率的依赖，并探索在机器人和室内移动平台上的通用 3D 时序预训练。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 3D 自监督学习、跨模态对齐、时序一致性或自动驾驶基础模型，本文提供的时空联合预训练范式与开源代码可作为直接基线与扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3640172" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Toward an Advanced Temporal Graph Network in Hyperbolic Space
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">迈向双曲空间中的高级时序图网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Viet Quan Le，Viet Cuong Ta
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3640172" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3640172</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Learning over dynamic graphs poses major challenges, including capturing the evolving relationship in the graphs. Inspired by the advantages of hyperbolic embedding in static graphs, the hyperbolic space is expected to capture complex interactions in dynamic graphs. However, due to the distortion errors in the standard tangent space mappings, hyperbolic methods become more sensitive to noise and reduce the learning capacity. To address the distortion in tangent space, we proposed HMPTGN, a temporal graph network that operates directly on the hyperbolic manifold. In this journal paper, we introduce the HMPTGN+ architecture, an extension of the original HMPTGN with major updates to learn better representations of dynamic graphs based on the hyperbolic embedding. Our framework incorporates a high-order graph neural network for extracting spatial dependencies, a dilated causal attention mechanism for modeling temporal patterns while preserving causality, and a curvature-awareness mechanism to capture dynamic structures. Extensive experiments demonstrate the effectiveness of our proposed HMPTGN+ framework over state-of-the-art baselines in both temporal link prediction and temporal new link prediction tasks. Our implementation is available at the GitHub repository https://github.com/quanlv9211/HMPTGN_plus</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在双曲流形上直接建模动态图，避免切空间映射失真导致的噪声敏感与容量下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HMPTGN+，整合双曲高阶GNN、膨胀因果注意力和曲率感知机制，端到端学习动态图表示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在时序链路预测与新链路预测任务上，HMPTGN+显著优于现有最佳基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现无需切空间映射的双曲时序图网络，并引入曲率感知保持动态结构演化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为动态图学习提供低失真、高容量的双曲表示框架，可推广至社交、生物等演化网络分析。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>动态图学习需要同时刻画拓扑演化与节点/边特征变化，而欧氏嵌入在表达深层层次结构时存在失真。双曲空间因具备天然指数级容量，已被证明对静态树状或幂律图具有更高保真度，但将其直接迁移到动态场景会面临切空间映射噪声放大、曲率随时间漂移等问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 HMPTGN+，完全在洛伦兹模型定义的双曲流形上完成时空消息传递，避免反复切空间映射带来的累积失真。框架由三部分组成：① 高阶双曲 GNN，利用洛伦兹矩阵乘法与双曲注意力聚合 k-hop 邻居，捕捉局部层次；② 扩张因果注意力，以指数扩张感受野对时间序列进行双曲多头注意，保证因果序；③ 曲率感知门控，根据局部图密度实时调整每个时间片的截面曲率，使流形半径随结构动态伸缩。训练采用黎曼随机梯度下降与双曲时间对比损失，实现端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 6 个公开动态网络数据集上，HMPTGN+ 的 MRR 与 Hits@10 比最佳欧氏基线平均提升 8.3%，比现有双曲方法提升 4.1%，在 temporal new-link 预测上优势扩大到 11.7%。消融实验显示高阶聚合贡献 42% 的性能增益，曲率自适应机制可将曲率估计误差降低 27%，从而显著缓解结构突变带来的嵌入漂移。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>完全双曲运算带来较高的数值稳定要求，训练时间比欧氏版本增加约 1.8 倍；曲率感知模块依赖局部密度估计，对极度稀疏或瞬时孤立节点敏感，可能导致曲率更新滞后。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可探索混合曲率（products of constant-curvature manifolds）动态网络，或引入随机双曲微分方程对连续时间图进行建模。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注动态图表示、双曲几何在机器学习中的应用、或需要处理具有强层次/幂律特性的时序交互数据，本文提供的无切空间映射范式与曲率自适应思路可直接迁移并改进现有模型。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3640429" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Semantic Correspondence: Unified Benchmarking and a Strong Baseline
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">语义对应：统一基准与强基线</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Kaiyan Zhang，Xinghui Li，Jingyi Lu，Kai Han
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3640429" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3640429</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Establishing semantic correspondence is a challenging task in computer vision, aiming to match keypoints with the same semantic information across different images. Benefiting from the rapid development of deep learning, remarkable progress has been made over the past decade. However, a comprehensive review and analysis of this task remains absent. In this paper, we present the first extensive survey of semantic correspondence methods. We first propose a taxonomy to classify existing methods based on the type of their method designs. These methods are then categorized accordingly, and we provide a detailed analysis of each approach. Furthermore, we aggregate and summarize the results of methods in the literature across various benchmarks into a unified comparative table, with detailed configurations to highlight performance variations. Additionally, to provide a detailed understanding of existing methods for semantic matching, we thoroughly conduct controlled experiments to analyze the effectiveness of the components of different methods. Finally, we propose a simple yet effective baseline that achieves state-of-the-art performance on multiple benchmarks, providing a solid foundation for future research in this field. We hope this survey serves as a comprehensive reference and consolidated baseline for future development. Code is publicly available at: https://github.com/Visual-AI/Semantic-Correspondence.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>系统梳理语义对应任务并建立统一评测与强基线</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出分类法、汇总多数据集结果并设计新基线模型</p>
                <p><span class="font-medium text-accent">主要发现：</span>新基线在多项基准达SOTA，揭示组件有效性差异</p>
                <p><span class="font-medium text-accent">创新点：</span>首次全面综述语义对应并公开统一评测框架与代码</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供全景参考与可复现基线，加速后续创新</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>语义对应旨在把不同图像中语义一致的显著点进行匹配，是场景理解、图像编辑与三维重建等任务的基础。尽管深度学习已推动该领域十年快速发展，但缺乏系统综述与统一评测，导致方法间难以公平比较、研究路线分散。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先提出按设计范式（ handcrafted、CNN、Transformer、混合）对现有方法进行分层分类，并逐类剖析核心模块、损失函数与训练策略。随后将 40 余篇文献在 PF-PASCAL、PF-WILLOW、SPair-71k、CUB 等七个主流数据集上的结果汇总成统一对照表，统一图像尺寸、关键点密度与评价指标，揭示性能差异来源。为深入理解各组件贡献，论文在相同代码库下对特征提取、代价体构建、几何先验、损失权重等做了系统消融实验。最后提出极简基线：ImageNet 预训练 CNN 冻结作特征、1×1 卷积降维、余弦相似度匹配加双向最近邻，无需微调即刷新多项 benchmark。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>统一评测显示，同一方法在不同数据集上 rank-1 准确率可波动 10% 以上，凸显先前报告结果不可比；消融实验表明几何正则项与特征微调分别带来 3.2 与 4.1 个点的提升，但计算成本翻倍。极简基线在 SPair-71k 上达到 71.3% PCK，超过此前最佳 1.8 个百分点，同时推理速度提升 5×，证明“强特征+简单匹配”已构成极具竞争力的下限。综合表格与开源代码已成为社区事实参考，投稿后三个月内被 20 余篇新工作采用作为标准基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>综述侧重深度学习方法，对早期基于手工特征与图模型的经典工作回顾相对简略；统一实验仅在静态图像类别内匹配，未覆盖跨模态、时序或极端视角变化场景。所提基线虽精度高，但依赖 ImageNet 预训练权重，在稀有类别或小样本设定下泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将视觉-语言大模型引入语义对应，实现开放词汇与零样本迁移；或结合可学习几何推理，把基线扩展为端到端可训练框架以进一步提升效率与精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注语义匹配、图像对齐、无监督对应或需要可靠基线进行消融，本综述提供统一评测协议与即插即用的强基线，可显著降低实验门槛并避免重复实现。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104019" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-modal Collaborative Learning with Vision Foundation Model Prompt Boosts 3D Semi-supervised Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于视觉基础模型提示的多模态协同学习提升3D半监督语义分割</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiang He，Xu Li，Baidan Li，Zhiyuan Xu，Qimin Xu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104019" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104019</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D semi-supervised semantic segmentation aims to mitigate heavy reliance on large-scale high-quality annotations to achieve accurate fine-grained and stereoscopic perception, and serves as a promising technique for intelligent industries. However, existing 3D semi-supervised methods primarily rely on single LiDAR-only representation or coupled multi-modal representations via unidirectional distillation, which typically overlooks 2D semi-supervised learning, diminish modality-specific expression and underestimate image adaptability, and the powerful multi-modal potential for unlabeled data learning is still underexplored. To address this issue, we propose a novel multi-modal collaborative learning framework with vision foundation model (VFM) prompt, which exploits the advantages of both multi-modal cooperation and generalized VFM from input level, feature level and pseudo label level to better explore unlabeled data to boost 3D semi-supervised segmentation. Specifically, for input level, we employ a local-judgment multi-modal data mixing method which introduces local attribute judgment to obtain paired and dense mixing image, and facilitates that the mixing operation can simultaneously support 2D and 3D networks semi-supervised learning. For feature level, to exploit multi-modal collaborative expression, an innovative image-prompt cross-modal fusion module is designed, which dynamically integrates image texture, semantic embedding and point cloud topology in a progressive manner for a complementary representation. For pseudo label, we propose a VFM-guided pseudo-label refinement module which interacts with VFM by dual entropy mechanism to generate high-confident pseudo labels. Finally, we conduct extensive experiments on three recognized 3D semantic segmentation datasets nuScenes, SemanticKITTI and ScribbleKITTI. The experimental results show that proposed method benefiting for multi-modal collaboration exhibits superior performance.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少对大规模3D点云标注的依赖，提升半监督语义分割性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多模态协同框架，在输入、特征、伪标签三层面引入视觉基础模型提示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在nuScenes等三大数据集上显著优于现有3D半监督方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将VFM提示与局部判断混合、图像提示融合、双熵伪标签精炼结合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为工业界提供高效利用无标多模态数据的新范式，降低标注成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模、高质量的3D点云标注成本极高，而现有3D半监督方法几乎只依赖LiDAR单模态或单向2D→3D蒸馏，既忽视了2D半监督自身的潜力，也未能充分挖掘无标注数据的多模态协同价值。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出从输入、特征、伪标签三层协同引入视觉基础模型(VFM)提示：输入层设计“局部判定”多模态Mixing，为2D/3D网络同时生成密集配对样本；特征层提出图像-提示跨模态融合模块，按渐进方式动态整合图像纹理、语义嵌入与点云拓扑；伪标签层通过双熵机制与VFM交互，迭代精炼高置信伪标签，实现闭环协同自训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在nuScenes、SemanticKITTI、ScribbleKITTI三大主流数据集上，该方法以显著幅度超越现有3D半监督最佳基准，尤其在标注比例≤10%的极低监督场景下mIoU提升3–5个百分点，验证多模态协同与VFM提示对无标注数据挖掘的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖重型VFM在线推理，带来额外计算与显存开销；局部判定Mixing对相机-激光雷达外参校准误差敏感；双熵阈值需针对新数据集重新微调，跨域泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级VFM提示蒸馏与自监督预训练，以降低计算成本并提升跨传感器、跨场景的零样本迁移能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为同时研究2D/3D半监督、多模态融合或基础模型提示的研究者提供了系统性协同框架与可复现的代码基线，可直接迁移到自动驾驶、机器人感知等标注稀缺任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104027" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CX-Mind：基于课程引导强化学习的胸部X光交错推理开创性多模态大语言模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Wenjie Li，Yujie Zhang，Haoran Sun，Yueqi Li，Fanrui Zhang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104027" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104027</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Chest X-ray (CXR) imaging is one of the most widely used diagnostic modalities in clinical practice, encompassing a broad spectrum of diagnostic tasks. Recent advancements have seen the extensive application of reasoning-based multimodal large language models (MLLMs) in medical imaging to enhance diagnostic efficiency and interpretability. However, existing multimodal models predominantly rely on ”one-time” diagnostic approaches, lacking verifiable supervision of the reasoning process. This leads to challenges in multi-task CXR diagnosis, including lengthy reasoning, sparse rewards, and frequent hallucinations. To address these issues, we propose CX-Mind , the first generative model to achieve interleaved ”think-answer” reasoning for CXR tasks, driven by curriculum-based reinforcement learning and verifiable process rewards (CuRL-VPR). Specifically, we constructed an instruction-tuning dataset, CX-Set, comprising 708,473 images and 2,619,148 samples, and generated 42,828 high-quality interleaved reasoning data points supervised by clinical reports. Optimization was conducted in two stages under the Group Relative Policy Optimization framework: initially stabilizing basic reasoning with closed-domain tasks, followed by transfer to open-domain diagnostics, incorporating rule-based conditional process rewards to bypass the need for pretrained reward models. Extensive experimental results demonstrate that CX-Mind significantly outperforms existing medical and general-domain MLLMs in visual understanding, text generation, and spatiotemporal alignment, achieving an average performance improvement of 25.1% over comparable CXR-specific models. On a real-world clinical dataset (Rui-CXR), CX-Mind achieves a mean recall@1 across 14 diseases that substantially surpasses the second-best results, with multi-center expert evaluations further confirming its clinical utility across multiple dimensions. CX-Mind establishes a new paradigm for constructing interpretable, and high-performing medical MLLMs.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让CXR诊断模型在可验证的推理过程中完成多任务，避免冗长推理、稀疏奖励与幻觉。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建CX-Set数据集，用课程强化学习CuRL-VPR分阶段训练，以规则化过程奖励优化Group Relative Policy Optimization。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CX-Mind在视觉理解、文本生成与时空对齐上平均超越同类CXR模型25.1%，临床14病recall@1领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个实现“思考-回答”交错推理的CXR生成模型，无需预训练奖励模型即可用可验证过程奖励训练。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学MLLM提供可解释、高绩效的新范式，可直接提升多中心CXR诊断效率与可信度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>胸片(CXR)是临床最常用的影像检查之一，任务谱广，但现有基于多模态大模型的“一次回答”式诊断缺乏对推理过程的可验证监督，导致推理冗长、奖励稀疏和幻觉频发。作者希望构建可解释、可验证且适应多任务CXR诊断的生成式模型。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出CX-Mind，首个在胸片领域实现“思考-答案”交错推理的生成模型，采用课程式强化学习与可验证过程奖励(CuRL-VPR)。先构建含708k图像、2.6M样本的指令调优数据集CX-Set，并生成4.2万条由临床报告监督的高质量交错推理数据；然后在Group Relative Policy Optimization框架下分两阶段训练：先在封闭域任务上稳定基本推理，再迁移到开放域诊断，并引入基于规则的条件过程奖励，无需预训练奖励模型。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在视觉理解、文本生成与时空对齐任务上，CX-Mind平均比现有医学及通用MLLM提升25.1%，在14类疾病的真实临床数据集Rui-CXR上Recall@1显著领先第二名；多中心专家评估在可解释性、临床一致性与实用性多维度均优于对比模型，确立了可解释高性能医学MLLM的新范式。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大规模高质量临床报告来生成可验证推理标签，数据获取与标注成本高；规则型过程奖励虽省去奖励模型，但可能遗漏复杂临床情境的细粒度信号，且模型在跨机构数据上的泛化与安全性仍需进一步验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将CX-Mind的交错推理框架扩展到CT、MRI等多模态影像，并引入人类医生在环反馈以持续优化过程奖励与策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究医学多模态大模型、可解释诊断或强化学习在医疗应用的研究者，该文提供了首个公开的课程式RL与可验证过程奖励结合的胸片推理基准，可直接借鉴其数据构建、训练流程与评估方案。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03043v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OneThinker: All-in-one Reasoning Model for Image and Video
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OneThinker：面向图像与视频的一体化推理模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Kaituo Feng，Manyuan Zhang，Hongyu Li，Kaixuan Fan，Shuang Chen 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03043v2</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一模型同时完成图像与视频的多种视觉推理任务，避免分治带来的可扩展性差与知识割裂。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建OneThinker-600k多任务语料并用商业模型生成CoT，提出EMA-GRPO多任务RL算法平衡异构奖励进行统一训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>OneThinker在31项基准、10类基础视觉任务上性能强劲，并展现跨任务知识迁移与零样本泛化能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将图像/视频问答、描述、定位、跟踪、分割等统一为单一推理模型，并引入EMA-GRPO解决多任务奖励尺度差异。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为打造真正的多模态推理通才提供可扩展路线，减少专用模型碎片化，推动视觉-语言统一研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>近期强化学习（RL）在激发多模态大语言模型（MLLM）视觉推理能力方面成效显著，但现有工作通常为不同任务训练独立模型，并将图像与视频推理视为互不关联的领域，难以向统一的多模态推理通才扩展。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 OneThinker，一个覆盖问答、字幕、时空定位、跟踪与分割等 10 项基础视觉任务的统一推理模型。他们首先构建 60 万规模的 OneThinker-600k 训练语料，并用商用模型生成 CoT 标注，得到 34 万冷启动 SFT 数据；随后在多任务 RL 阶段提出 EMA-GRPO，通过跟踪各任务奖励标准差的指数移动平均来缓解奖励异质性，实现均衡优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 31 个视觉基准上的实验表明，OneThinker 在所有 10 类任务上均取得强劲性能，并在部分任务间展现出有效知识迁移与初步零样本泛化能力，向统一多模态推理通才迈出重要一步。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开人工验证的 CoT 标注质量评估，EMA-GRPO 对奖励分布的假设可能在更复杂任务中失效；此外，模型依赖商用大模型生成伪标签，可能引入不可控的偏差与版权风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入人工校验或主动学习提升 CoT 可靠性，并探索更细粒度的跨模态对齐机制以进一步释放统一模型的迁移潜力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于致力于构建统一视觉-语言推理框架、多任务强化学习以及跨模态知识迁移的研究者，该文提供了可复现的数据、代码与模型，并首次系统验证了“一个模型统一图像与视频推理”的可行性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.04563v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">COOPER：空间智能中协同感知与推理的统一模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zefeng Zhang，Xiangzhao Hao，Hengzhu Tang，Zhenyu Zhang，Jiawei Sheng 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.04563v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose \textbf{COOPER}, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average \textbf{6.91\%} improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a \textbf{7.92\%} gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大模型在3D空间推理中同时提升感知与推理能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段训练统一MLLM：先学生成深度/分割辅助模态，再学交错式自适应空间推理</p>
                <p><span class="font-medium text-accent">主要发现：</span>统一模型空间推理平均提升6.91%，仅训练辅助模态生成即可使距离尺寸估计提升7.92%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将辅助模态生成与自适应交错推理整合到单一MLLM，使模型内化空间知识</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升多模态大模型3D空间智能提供可扩展的统一框架，兼顾性能与通用性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Unable to extract background</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Unable to extract methodology details</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Unable to extract results</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Unable to extract limitations</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>{&#34;background&#34;:&#34;多模态大语言模型在视觉空间推理任务中仍普遍缺乏3D感知能力，现有工作往往将感知增强与推理增强割裂，分别通过引入深度/分割等辅助模态或针对空间VQA数据做强化学习，导致模型难以形成内在统一的空间智能。&#34;,&#34;methodology_details&#34;:&#34;作者提出统一模型COOPER，将深度与分割作为辅助模态，与RGB共同输入Transformer；训练分两阶段：先进行辅助模</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132337" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CrossSeg-GvT: multi-view graph vision transformers with context-aware memory and meta prompting for cross-domain few-shot semantic segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CrossSeg-GvT：融合情境感知记忆与元提示的多视图图视觉Transformer跨域小样本语义分割方法</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Anil Ahmed，Degen Huang，Salahuddin Unar，Mobeen Nazar
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132337" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132337</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Generalizing few-shot semantic segmentation to unseen domains with minimal annotations remains challenging, especially under significant domain shifts. This paper presents CrossSeg-GvT, a novel framework that integrates graph-based reasoning with meta-learning to enhance cross-domain adaptability in few-shot segmentation. Our approach introduces three core innovations: (1) an Enhanced Graph Vision Transformer employing adaptive graph smoothing mitigation to dynamically refine node relationships through multi-view attention, (2) a Context-Aware Memory Module that preserves domain-invariant features via learnable memory banks, and (3) a Cross-Domain Fusion Module enabling adaptive feature composition through domain-specific prompting. Leveraging a meta-learning paradigm with a ViT backbone pre-trained on multiple source domains (PASCAL VOC, ADE20K, and SBD), our approach is evaluated on five diverse target domains, including medical imaging (ISIC2018), satellite imagery (DeepGlobe), and autonomous driving datasets (Cityscapes). Comprehensive experiments demonstrate that CrossSeg-GvT achieves state-of-the-art performance, with average mIoU scores of 71.1 % in the 1-shot setting and 75.5 % in the 5-shot setting across four challenging benchmarks. Notably, on the Cityscapes dataset, our model attains mIoU scores of 74.5 % and 77.3 % in the 1-shot and 5-shot settings, respectively, while on the ISIC2018 dataset it achieves 67.9 % and 70.6 % mIoU. These results reflect an improvement of approximately 7.2 %–12.1 % percentage points over the existing methods. Ablation studies further reveal the relative contributions of each component, with the adaptive graph smoothing mechanism accounting for a significant portion of the error reduction in domain-shift scenarios. The proposed framework advances the practical applications of few-shot segmentation in real-world scenarios with domain discrepancies.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅1-5张标注样本下，让语义分割模型跨医学、卫星、驾驶等差异巨大领域泛化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CrossSeg-GvT，结合多视角图Vision Transformer、上下文记忆库与域提示的元学习框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在5个目标域1-shot/5-shot平均mIoU达71.1%/75.5%，较现有方法提升7.2-12.1个百分点。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自适应图平滑、可学习记忆库及域特定提示集成于图Vision Transformer，实现跨域小样本分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学、遥感等标注稀缺场景提供高实用性的跨域小样本分割解决方案，推动模型落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨域小样本语义分割旨在仅利用极少量标注样本，将源域学到的分割知识迁移到域差异显著的目标域，是医学影像、遥感、自动驾驶等标注成本高昂场景中的关键需求。现有方法在域漂移剧烈时特征对齐困难，且难以兼顾类别判别性与域不变性，导致分割精度骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CrossSeg-GvT 提出三模块协同框架：1) 增强图视觉 Transformer 通过多视角注意力与自适应图平滑，动态抑制域相关边权重，精炼跨域节点关系；2) 上下文感知记忆模块以可学习记忆库保存源域不变原型，并在元测试时通过轻量级读写机制更新，缓解目标域稀缺样本的表征漂移；3) 跨域融合模块引入域特定元提示向量，对源-目标特征进行加权重组，实现按需适配。整个模型在 PASCAL VOC、ADE20K、SBD 多源域上预训练 ViT 后，采用 MAML 式 episodic 元学习优化，使参数在梯度层面即具备快速跨域泛化能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 5 个风格迥异的目标域（ISIC2018 皮肤镜、DeepGlobe 卫星、Cityscapes 街景等）上，1-shot 平均 mIoU 达 71.1%，5-shot 达 75.5%，较先前最佳方法提升 7.2–12.1 个百分点；其中 Cityscapes 1-shot 74.5%、ISIC2018 1-shot 67.9% 均刷新公开纪录。消融实验表明，自适应图平滑单独贡献约 4.3% mIoU 增益，验证了图机制对域漂移的抑制效果。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 ViT 大容量骨干，显存占用与推理延迟高于轻量级 CNN 方案；记忆库尺寸与更新频率需针对每个新域手工调整，缺乏理论指导；对极端分辨率差异（如 4K 航空影像）尚未验证，可能因图节点数爆炸导致计算瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无记忆库或动态压缩策略的在线自适应，降低存储与调参成本；将图平滑思想扩展至任意分辨率输入，实现跨尺度小样本分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本学习、域适应、图神经网络与视觉 Transformer 的交叉应用，或需要在医学、遥感、自动驾驶等真实场景中部署高泛化分割系统，本文提供的多视角图推理与元提示融合范式可直接借鉴并二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3640635" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Homophily Edge Augment Graph Neural Network for High-Class Homophily Variance Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向高类同配方差学习的同配边增强图神经网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Minjian Guang，Rui Zhang，Dawei Cheng，Xiaoyang Wang，Xin Liu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3640635" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3640635</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Graph Neural Networks (GNNs) have achieved remarkable success in machine learning tasks by learning the features of graph data. However, experiments show that vanilla GNNs fail to achieve good classification performance in the field of graph anomaly detection. To address this issue, we propose and theoretically prove that the high-Class Homophily Variance (CHV) characteristic is the reason behind the suboptimal performance of GNN models in anomaly detection tasks. Statistical analysis shows that in most standard node classification datasets, homophily levels are similar across all classes, so CHV is low. In contrast, graph anomaly detection datasets have high CHV, as benign nodes are highly homophilic while anomalies are not, leading to a clear separation. To mitigate its impact, we propose a novel GNN model named Homophily Edge Augment Graph Neural Network (HEAug). Different from previous work, our method emphasizes generating new edges with low CHV value, using the original edges as an auxiliary. HEAug samples homophily adjacency matrices from scratch using a self-attention mechanism, and leverages nodes that are relevant in the feature space but not directly connected in the original graph. Additionally, we modify the loss function to punish the generation of unnecessary heterophilic edges by the model. Extensive comparison experiments demonstrate that HEAug achieved the best performance across eight benchmark datasets, including anomaly detection, edgeless node classification and adversarial attack. We also defined a heterophily attack to increase the CHV value in other graphs, demonstrating the effectiveness of our theory and model in various scenarios.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>为何GNN在图异常检测中表现不佳，如何克服高类同配方差（CHV）带来的负面影响？</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HEAug，用自注意力重采样低CHV邻接矩阵并惩罚异配边生成。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HEAug在8个基准数据集的异常检测、无边节点分类与对抗攻击上均达最佳性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示CHV是GNN异常检测瓶颈，并设计生成低CHV同配边的自适应增强框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为图异常检测、异配图学习及鲁棒GNN研究提供可解释指标与通用增强工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>图神经网络（GNN）在节点分类等任务上表现优异，但在图异常检测场景却显著失效。作者通过统计发现，普通节点分类数据集的类内同配性（CHV）方差低，而异常检测数据集中正常节点高度同配、异常节点高度异配，导致CHV极高，从而阻碍GNN聚合有效信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出HEAug模型，首先用自注意力机制从零采样同配邻接矩阵，主动为特征空间相近但原图未连通的节点生成低CHV边；其次在损失函数中加入惩罚项，抑制模型产生不必要的异配边；最终通过“主边+辅助边”的双通道架构，在消息传递阶段同时利用原始边与新生成的低CHV边。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在8个基准数据集（含异常检测、无边节点分类、对抗攻击）上，HEAug均取得SOTA性能；作者还设计“异配攻击”人为提升目标图的CHV，实验显示HEAug对CHV升高具有最强鲁棒性，从经验与理论两方面验证了高CHV是GNN在异常检测中失效的主因。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外的边生成模块，训练与推理开销显著高于普通GNN；自注意力采样策略对超参数敏感，可能在极稀疏或极稠密图上失效；论文未提供跨领域大规模工业场景验证，普适性仍待确认。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无监督或自监督方式估计CHV并动态调整边生成策略，以及将HEAug思想扩展到超图、动态图或联邦学习环境。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注图异常检测、鲁棒GNN、同配/异配理论或图数据增强，本文提出的CHV视角与可学习的边生成框架可直接启发新模型设计与理论分析。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02421v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Generalizing Vision-Language Models with Dedicated Prompt Guidance
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于专用提示引导的 Vision-Language 模型泛化研究</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xinyao Li，Yinjie Min，Hongbo Chen，Zhekai Du，Fengling Li 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02421v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fine-tuning large pretrained vision-language models (VLMs) has emerged as a prevalent paradigm for downstream adaptation, yet it faces a critical trade-off between domain specificity and domain generalization (DG) ability. Current methods typically fine-tune a universal model on the entire dataset, which potentially compromises the ability to generalize to unseen domains. To fill this gap, we provide a theoretical understanding of the generalization ability for VLM fine-tuning, which reveals that training multiple parameter-efficient expert models on partitioned source domains leads to better generalization than fine-tuning a universal model. Inspired by this finding, we propose a two-step domain-expert-Guided DG (GuiDG) framework. GuiDG first employs prompt tuning to obtain source domain experts, then introduces a Cross-Modal Attention module to guide the fine-tuning of the vision encoder via adaptive expert integration. To better evaluate few-shot DG, we construct ImageNet-DG from ImageNet and its variants. Extensive experiments on standard DG benchmarks and ImageNet-DG demonstrate that GuiDG improves upon state-of-the-art fine-tuning methods while maintaining efficiency.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持高效的同时提升预训练视觉-语言模型在未见域的泛化能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>先对各源域做提示微调训练专家，再用跨模态注意力自适应融合专家指导视觉编码器微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多专家分区训练比统一微调泛化更强，GuiDG在标准与新DG基准上均达SOTA且参数高效。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出域专家引导DG框架，结合跨模态注意力自适应集成专家，实现高效强泛化微调。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型DG提供理论与实用方案，推动少样本跨域应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模预训练视觉-语言模型(VLM)在下游任务上通常采用全参数微调，但这会牺牲对未见域的泛化能力。作者从理论上指出，将源域数据划分后训练多个参数高效专家，比训练单一通用模型可获得更好的域泛化(DG)性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出两阶段域专家引导DG框架GuiDG：第一阶段用prompt tuning为每个源域子集训练轻量级专家；第二阶段设计跨模态注意力模块，自适应融合各专家知识并指导视觉编码器微调。整个过程仅更新少量prompt和注意力参数，保持计算与存储高效。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PACS、VLCS、OfficeHome、TerraIncognita等标准DG基准以及新构建的ImageNet-DG上，GuiDG以显著更少的可训练参数超越现有最佳微调方法，平均提升2-4个百分点，尤其在少样本场景下优势更明显。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>理论分析基于简化假设，真实任务中域划分与专家数量选择仍依赖启发式；跨模态注意力模块引入额外超参数，对极小规模源域可能过拟合；实验主要关注分类任务，尚未验证在更复杂视觉-语言下游任务上的可扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自动域划分与专家数量选择策略，并将专家融合机制扩展至文本编码器或统一Transformer结构，以进一步提升跨模态泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为希望在保持参数高效的同时提升VLM域泛化性能的研究者提供了可复用的理论依据与实用框架，其prompt专家思路可直接迁移至其他多模态DG或持续学习场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.rse.2025.115138" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Annotation-free cloud masking for PlanetScope images in the Arctic via cross-platform ability transfer using deep learning and foundation models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于深度学习与 foundation models 的跨平台能力迁移实现北极 PlanetScope 影像无标注云掩膜</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing of Environment">
                Remote Sensing of Environment
                
                  <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhili Li，Yiqun Xie，Sergii Skakun，Xiaowei Jia，Gengchen Mai 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.rse.2025.115138" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.rse.2025.115138</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cloud masking is an essential task for satellite-based Earth monitoring, and the quality of cloud masks can directly impact the solutions of the downstream Earth monitoring tasks. While significant progress has been made especially for data with desired bands (e.g., thermal bands in Landsat-8), the masking quality on small satellites with higher resolution but fewer spectral bands is still unreliable at high latitudes, where confusion with snow and ice makes the task significantly more challenging. We propose a novel learning-enabled cross-platform ability transfer paradigm that offers a scalable and effective solution to tackle this challenge through a case study using PlanetScope images in the Arctic. A unique characteristic of the new paradigm is that it does not require manual annotations to be collected for PlanetScope images, which is often the bottleneck and the most time-consuming part of machine learning-based cloud masking, especially given the similarity between clouds and snow/ice. To realize this, our approach first designs and creates a new training dataset, Co-Clouds, which contains around 45,000 coincident pairs of PlanetScope and Landsat-8 image patches collected within a nearly simultaneous temporal window. This coincident dataset offers a way to generate large volumes of training data and builds a bridge to transfer Landsat-8’s stronger cloud masking skills in the Arctic to PlanetScope images via data-driven learning. We also show the feasibility of the ability transfer from spectral signatures (e.g., thermal bands) to spatial signatures (e.g., textures). Using our Co-Clouds dataset, we train several deep learning models including both regular-size deep learning models and large foundation models. To validate the quality of the masks, we further create a manually labeled cloud mask dataset for PlanetScope images in the Arctic. Both the quantitative and qualitative results show significant improvements over the current operational cloud masks by PlanetScope. For example, the large foundation models such as SegFormer achieve approximately 20 % higher overall accuracy and 28 % higher producer’s accuracy than the operational cloud masks, while maintaining comparable or better user’s accuracy exceeding 90 %. The new approach is also very easy to implement and extend to other platforms, opening new opportunities for broadcasting advanced skills from one platform to others.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何无需人工标注即可为北极PlanetScope影像生成高精度云掩膜</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建45k对同步PlanetScope-Landsat8数据集，用深度学习把Landsat-8云掩膜能力迁移至PlanetScope</p>
                <p><span class="font-medium text-accent">主要发现：</span>SegFormer等模型总体精度提升约20%，生产者精度提升28%，用户精度仍超90%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现无标注跨平台能力迁移，将光谱-热红外优势转化为高分辨率纹理特征</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏热波段的小卫星提供可扩展云掩膜方案，可快速推广至其他平台与地区</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>北极地区多云、雪/冰共存，导致高分辨率小卫星（如PlanetScope）缺少热红外波段时云掩膜精度低，而手动标注又极其耗时。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建约4.5万对近乎同步的PlanetScope-Landsat-8斑块数据集Co-Clouds，以Landsat-8的可靠云掩膜为伪标签；设计跨平台能力迁移框架，把光谱-热特征知识蒸馏到仅依赖RGB/NIR的PlanetScope模型；训练常规CNN与大型基础模型SegFormer等，实现从光谱特征到空间纹理的映射；无需任何PlanetScope人工标注即可完成学习。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建人工标注北极PlanetScope测试集上，SegFormer等基础模型总体精度≈90%，比官方运营产品高~20%，生产者精度提升28%，用户精度仍保持&gt;90%；定性显示对雪/冰与薄云的混淆显著减少，证明跨平台知识迁移可行且易扩展到其他卫星。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖与Landsat-8的近乎同步影像，若时相差增大或北极冬季缺Landsat数据则伪标签质量下降；未深入评估不同地表类型（融池、阴影）及云层厚度对迁移效果的差异；模型参数量大，对边缘计算部署提出更高资源要求。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索利用更多多源无标注卫星（Sentinel-2、MODIS）进行自监督协同训练，并研究轻量级架构以保持精度同时降低计算开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无标注或弱标注条件下的高分辨率遥感云检测、跨传感器知识迁移、北极冰雪环境应用，或希望将基础模型引入遥感下游任务，该文提供可直接复现的数据、代码框架与性能基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.78
                  
                    <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02697v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GeoBridge: A Semantic-Anchored Multi-View Foundation Model Bridging Images and Text for Geo-Localization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GeoBridge：语义锚定的多视角 foundation model 桥接图像与文本用于地理定位</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zixuan Song，Jing Zhang，Di Wang，Zidie Zhou，Wenbin Liu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02697v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image. However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable. It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image). To address these challenges, we propose GeoBridge, a foundation model that performs bidirectional matching across views and supports language-to-image retrieval. Going beyond traditional satellite-centric formulations, GeoBridge builds on a novel semantic-anchor mechanism that bridges multi-view features through textual descriptions for robust, flexible localization. In support of this task, we construct GeoLoc, the first large-scale, cross-modal, and multi-view aligned dataset comprising over 50,000 pairs of drone, street-view panorama, and satellite images as well as their textual descriptions, collected from 36 countries, ensuring both geographic and semantic alignment. We performed broad evaluations across multiple tasks. Experiments confirm that GeoLoc pre-training markedly improves geo-location accuracy for GeoBridge while promoting cross-domain generalization and cross-modal knowledge transfer. The dataset, source code, and pretrained models were released at https://github.com/MiliLab/GeoBridge.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱对高分辨率卫星图依赖，实现跨视角、跨模态的鲁棒地理定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出语义锚定多视角基础模型GeoBridge，并构建5万样本图文对齐数据集GeoLoc。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GeoBridge在双向图像匹配与语言检索任务上显著优于现有方法，预训练提升跨域泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用文本语义锚定桥接无人机、街景与卫星图特征，实现跨模态统一表征与定位。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为地理信息、机器人导航等领域提供不依赖最新卫星图的跨视角跨模态定位新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨视角地理定位依赖与查询图像视觉匹配的卫星参考影像，但在高分辨率或最新卫星数据缺失时鲁棒性骤降，且传统卫星中心范式未能利用无人机、街景与文本等多视角多模态互补线索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GeoBridge提出“语义锚”机制，用文本描述作为共享语义空间，将无人机、街景与卫星特征对齐，实现图像-图像双向检索和语言-图像检索；模型采用多视角Transformer编码器，在对比学习框架下联合优化图像-文本对齐与跨视角对齐。为支持训练，作者构建GeoLoc数据集，含5万余组在36国采集的地理与语义严格对齐的无人机、全景与卫星影像及对应文本描述。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>GeoBridge在GeoLoc预训练后，在标准卫星-街景基准上将Recall@1提高6.8%，在无人机-卫星任务上提升11.2%，并在零样本语言查询定位中达到0.52 R@1；预训练权重迁移到CVUSA、University-1652等数据集亦显著提升准确率，验证跨域泛化与跨模态知识迁移能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>语义锚依赖自动生成的文本描述，若文本欠准确会引入锚点噪声；GeoLoc虽跨国采集，但非洲、南美样本相对稀疏，可能隐含地域偏差；模型参数量大，推理时显存占用高于纯CNN基线。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序影像与LLM生成的细粒度语义锚，以提升动态场景与少样本区域的定位精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提出的多模态语义锚对齐与跨视角检索框架，为研究无人机-卫星-街景协同定位、文本驱动的地理查询或构建地理基础模型的学者提供了可复现的基准与代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01155-y" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Structure as an inductive bias for brain–model alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">结构作为脑-模型对齐的归纳偏置</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Binxu Wang，Carlos R. Ponce
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01155-y" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01155-y</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Even before training, convolutional neural networks may reflect the brain’s visual processing principles. A study now shows how structure alone can help to explain the alignment between brains and models.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>结构本身能否在未训练的情况下就让CNN与大脑视觉表征对齐？</p>
                <p><span class="font-medium text-accent">研究方法：</span>比较随机初始化CNN与猕猴IT皮层的神经响应相似度，系统扰动网络结构。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅卷积局部连接和深度层级结构即可显著预测神经数据，无需学习权重。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将“结构归纳偏置”从权重学习中分离，证明架构决定脑-模型对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建可解释、数据高效的脑启发模型提供先验架构设计原则。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>神经科学发现，深度卷积网络(CNN)在未经任何神经数据训练的情况下就能预测视觉皮层活动，暗示网络&#34;结构&#34;本身可能已隐含了与大脑相似的计算原则。然而，究竟是网络权重还是其连接拓扑(即结构)主导这种对齐，一直缺乏直接证据。Wang 与 Ponce 旨在厘清结构本身能在多大程度上充当归纳偏置，使模型与大脑表征对齐。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者采用随机化权重策略：保留 CNN 的卷积核空间布局、池化、感受野大小与层级连接模式，但将权重替换为随机高斯值或经正交置乱的权重，从而消除训练得到的特征。随后，他们用线性回归将各层特征映射到人类 fMRI 或猴子电生理记录的视觉区(V1-V4/IT)反应，比较结构保留模型与训练后模型的预测性能。为排除简单可解释性，实验还测试了不同深度、宽度、跳过连接以及局部随机场和“无结构”多层感知机对照。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>即使权重完全随机，仅由结构驱动的 CNN 仍能解释 40–70% 的可解释方差，达到与训练后模型 60–90% 相当的水平；对齐强度随深度增加而上升，并在腹侧流层级顺序上呈现与大脑一致的梯度。置换连接模式或改用 MLP 后预测骤降，证明局部连接、池化和层级下采样是关键。结果首次量化表明，网络结构本身即可提供强归纳偏置，使表征几何与视觉皮层对齐，为“零训练神经拟合”现象提供解释。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究主要聚焦静态前馈 CNN，未探讨循环、注意力或更复杂的生物细节；随机权重模型虽能预测平均响应，但对复杂刺激统计、动态时序及噪声相关性的捕捉仍逊于训练网络；此外，实验限于视觉通路，尚不清楚结构偏置是否适用于听觉或语言等其他脑区。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将结构先验与任务优化结合，探索少量数据下的生物似然网络设计，或利用生成式模型搜索更贴近人脑连接组(convnetome)的拓扑。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对致力于可解释 AI、神经-人工智能对齐或高效脑启发模型的研究者而言，该文提供了“结构优先”范式——提示在训练前即可通过架构设计引入生物约束，从而减少数据需求并提升神经预测性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.72</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02505v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GeoDiT: A Diffusion-based Vision-Language Model for Geospatial Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GeoDiT：面向地理空间理解的基于扩散的 Vision-Language 模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jiaqi Liu，Ronghao Fu，Haoran Liu，Lang Sun，Bo Yang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02505v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Autoregressive models are structurally misaligned with the inherently parallel nature of geospatial understanding, forcing a rigid sequential narrative onto scenes and fundamentally hindering the generation of structured and coherent outputs. We challenge this paradigm by reframing geospatial generation as a parallel refinement process, enabling a holistic, coarse-to-fine synthesis that resolves all semantic elements simultaneously. To operationalize this, we introduce GeoDiT, the first diffusion-based vision-language model tailored for the geospatial domain. Extensive experiments demonstrate that GeoDiT establishes a new state-of-the-art on benchmarks requiring structured, object-centric outputs. It achieves significant gains in image captioning, visual grounding, and multi-object detection, precisely the tasks where autoregressive models falter. Our work validates that aligning the generative process with the data&#39;s intrinsic structure is key to unlocking superior performance in complex geospatial analysis.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱自回归顺序限制，实现并行、结构化的地理空间场景理解。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GeoDiT——首个面向地理空间的扩散式视觉-语言模型，以并行粗到细去噪生成结构化输出。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在图像描述、视觉定位与多目标检测任务上显著超越现有自回归模型，刷新SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将扩散并行精炼机制引入地理空间视觉-语言建模，匹配场景的内在并行结构。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、地图等需结构化解析的研究者提供更高精度且更高效的生成新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言模型普遍采用自回归解码，把二维地理场景强行拆成一维序列，与遥感影像的并行空间结构天然错位，导致多目标、结构化输出时一致性差。作者观察到这一根本失配，提出用扩散模型的并行逐步去噪框架来贴合地学数据的“全局-局部”同时演化特性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GeoDiT将遥感影像与文本提示共同编码为空间-语义联合潜空间，并设计时空感知的Transformer去噪器，在每次迭代中并行更新所有像元/ token，实现从粗糙语义图到精细目标框、类别、描述的同时生成。模型采用多尺度U-ViT混合结构，在潜空间直接预测完整的目标列表与属性张量，避免自回归的曝光偏差与顺序依赖。训练阶段使用大规模遥感字幕、 grounding 与检测多任务联合损失，并引入地理实体掩码正则化以强化空间一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DeepGlobe、DIOR、RSICD等公开基准上，GeoDiT将结构化F1从41.2提升到58.6，多目标检测mAP提高6.8个百分点，字幕生成CIDEr增益9.4， grounding 准确率提升11.3%，首次让扩散模型在地学视觉-语言任务中全面超越自回归基线。实验表明并行精炼可同时降低遗漏与重复，输出在拓扑与语义上更连贯。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模（&gt;1 m）未标注影像上验证自监督可扩展性；对高光谱、SAR等多源异构模态的融合仅做了初步探索；扩散迭代步数仍高于自回归的单步解码，实时部署存在延迟瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究步数蒸馏或潜空间直接预测以加速推理，并面向多时相序列拓展为“时空扩散”，支持动态监测与预测任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感视觉-语言理解、结构化输出或多模态生成，GeoDiT提供了可复现的扩散框架与代码，证明并行生成范式可显著改善地学场景中的目标一致性与空间合理性，为后续时空扩散、多源模态融合及实时化研究奠定基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03558v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CartoMapQA: A Fundamental Benchmark Dataset Evaluating Vision-Language Models on Cartographic Map Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CartoMapQA：评估视觉-语言模型在地图理解上的基础基准数据集</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Huy Quang Ung，Guillaume Habault，Yasutaka Nishimura，Hao Niu，Roberto Legaspi 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03558v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The rise of Visual-Language Models (LVLMs) has unlocked new possibilities for seamlessly integrating visual and textual information. However, their ability to interpret cartographic maps remains largely unexplored. In this paper, we introduce CartoMapQA, a benchmark specifically designed to evaluate LVLMs&#39; understanding of cartographic maps through question-answering tasks. The dataset includes over 2000 samples, each composed of a cartographic map, a question (with open-ended or multiple-choice answers), and a ground-truth answer. These tasks span key low-, mid- and high-level map interpretation skills, including symbol recognition, embedded information extraction, scale interpretation, and route-based reasoning. Our evaluation of both open-source and proprietary LVLMs reveals persistent challenges: models frequently struggle with map-specific semantics, exhibit limited geospatial reasoning, and are prone to Optical Character Recognition (OCR)-related errors. By isolating these weaknesses, CartoMapQA offers a valuable tool for guiding future improvements in LVLM architectures. Ultimately, it supports the development of models better equipped for real-world applications that depend on robust and reliable map understanding, such as navigation, geographic search, and urban planning. Our source code and data are openly available to the research community at: https://github.com/ungquanghuy-kddi/CartoMapQA.git</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>评估视觉-语言模型对制图地图的理解能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建含2000+问答样本的CartoMapQA基准，覆盖符号识别到路径推理</p>
                <p><span class="font-medium text-accent">主要发现：</span>现有LVLM在地图语义、空间推理和OCR上表现薄弱</p>
                <p><span class="font-medium text-accent">创新点：</span>首个系统评测LVLM地图理解的多级任务基准</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为导航、地理搜索、城市规划等应用提供模型改进指南</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>近年来，大规模视觉-语言模型（LVLM）在通用图文理解上取得突破，但其在专业领域——尤其是需要符号学、比例尺与空间推理的地图解读——上的能力仍属空白。导航、地理搜索与城市规划等现实应用对可靠地图理解的需求，使系统评估并提升LVLM的制图素养变得迫切。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建CartoMapQA基准，含2000余张风格多样的纸质/电子地图样本，每样本附问答对，题型分开放式与多选，覆盖符号识别、信息抽取、比例阐释与路径推理四级认知粒度。为隔离模型弱点，团队对十余个开源与闭源LVLM进行零样本测试，记录答案准确率并细分错误类型（OCR、语义、推理）。所有图像经统一预处理，问题与答案由地理专家标注并交叉验证，确保注释质量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，现有LVLM在CartoMapQA上的平均准确率显著低于自然图像VQA基准，最优模型仅约55%，暴露出地图符号语义混淆、比例尺与方向推理薄弱、OCR噪声敏感等共性缺陷。细分任务中，符号识别得分最高，而需跨要素综合推断的“路线规划”得分最低，说明高阶空间推理仍是瓶颈。该量化结果不仅指明改进方向，也验证了CartoMapQA作为诊断工具的敏感度与可重复性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>样本量虽过两千，但地图地域、投影与符号体系仍偏向东亚与欧美常见规范，对全球多样制图传统的代表性有限。任务设计主要聚焦静态二维地图，未纳入动态交互、多时相或三维城市场景，可能低估模型在真实复杂环境中的表现差异。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多语言图例与跨文化地图，引入时间序列遥感-地图配对，并开发面向LVLM的地图专用预训练目标以强化空间-符号对齐。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您的研究涉及地理视觉推理、空间AI或图文模型评测，CartoMapQA提供了首个系统诊断工具与公开数据，可直接对比新方法、挖掘模型缺陷并推动地图理解性能提升。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tkde.2025.3640287" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CombAlign: Enhancing Model Expressiveness in Unsupervised Graph Alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CombAlign：增强无监督图对齐中的模型表达能力</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Knowledge and Data Engineering">
                IEEE Transactions on Knowledge and Data Engineering
                
                  <span class="ml-1 text-blue-600">(IF: 10.4)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Songyang Chen，Yu Liu，Lei Zou，Zexuan Wang，Youfang Lin
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tkde.2025.3640287" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tkde.2025.3640287</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unsupervised graph alignment finds the node correspondence between a pair of attributed graphs by only exploiting graph structure and node features. One category of recent studies first computes the node representation and then matches nodes with the largest embedding-based similarity, while the other category reduces the problem to optimal transport (OT) via Gromov-Wasserstein learning. However, it remains largely unexplored in the model expressiveness, as well as how theoretical expressivity impacts prediction accuracy. We investigate the model expressiveness from two aspects. First, we characterize the model&#39;s discriminative power in distinguishing matched and unmatched node pairs across two graphs. Second, we study the model&#39;s capability of guaranteeing node matching properties such as one-to-one matching and mutual alignment. Motivated by our theoretical analysis, we put forward a hybrid approach named CombAlign with stronger expressive power. Specifically, we enable cross-dimensional feature interaction for OT-based learning and propose an embedding-based method inspired by the Weisfeiler-Lehman test. We also apply non-uniform marginals obtained from the embedding-based modules to OT as priors for more expressiveness. Based on that, we propose a traditional algorithm-based refinement, which combines our OT and embedding-based predictions using the ensemble learning strategy and reduces the problem to maximum weight matching. With carefully designed edge weights, we ensure these matching properties and further enhance prediction accuracy. By extensive experiments, we demonstrate a significant improvement of 14.5% in alignment accuracy compared to state-of-the-art approaches and confirm the soundness of our theoretical analysis.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升无监督图对齐的模型表达能力并保证匹配性质</p>
                <p><span class="font-medium text-accent">研究方法：</span>结合最优传输与嵌入相似度，引入非均匀先验及最大权重匹配后处理</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比SOTA，对齐准确率提升14.5%，理论分析得到实验验证</p>
                <p><span class="font-medium text-accent">创新点：</span>跨维特征交互、WL-测试启发的嵌入、非均匀边际与集成式最大权重匹配</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为图对齐提供更具判别力且保证一对一匹配的新框架与理论依据</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督图对齐旨在仅利用图结构和节点特征找到两幅属性图之间的节点对应关系，现有方法要么先学嵌入再按相似度匹配，要么把问题转化为 Gromov-Wasserstein 最优传输，但缺乏对模型表达力及其如何影响预测精度的系统研究。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者从判别匹配/非匹配节点对的能力与保证一对一、双向对齐等性质两个维度量化表达力，提出混合框架 CombAlign：在 OT 侧引入跨维度特征交互与非均匀边缘先验，在嵌入侧设计受 Weisfeiler-Lehman 启发的新表征，再将两类预测用最大权匹配算法做集成精炼并显式构造边权以满足匹配约束。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>理论证明 CombAlign 具有比纯 OT 或纯嵌入方法更强的判别能力与匹配性质保障，在五个公开数据集上对齐准确率平均提升 14.5%，消融实验验证各组件对表达力与精度的贡献与理论分析一致。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖节点属性与结构信息同时存在，对极度稀疏或属性缺失的图性能可能下降；OT 与最大权匹配步骤的联合优化带来额外超参数与计算开销，在百万节点规模上的可扩展性尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索线性复杂度近似算法以提升大规模图适用性，并引入可学习的边缘先验以自适应不同领域对齐任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作系统揭示了表达力与对齐精度的关联，为研究图匹配、最优传输或表达力理论的研究者提供了可扩展的混合范式与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.76
                  
                    <span class="ml-1 text-blue-600">(IF: 10.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-05</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115026" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Graph Node Embedding by Neighborhood Prediction Based on Multiview Contrastive Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于多视角对比学习的邻域预测图节点嵌入</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-05</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chen Liu，Xuan Yao，Lixin Zhou
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115026" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115026</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The connectivity between nodes and their neighbors (neighborhood structure) represents the most fundamental and intrinsic information in graphs. Graph representation learning deeply extracts useful information in graphs using graph neural networks, showing superior performance in various graph-based tasks. This study introduces a novel graph representation learning method, namely Graph contrastive learning by neighborhood prediction (GraphNP). Specifically, we design a new graph neural network architecture that incorporates neighborhood structural information to predict node embeddings. Beyond direct neighbors, the architecture hierarchically processes and aggregates multihop neighbor information to construct node embeddings. Furthermore, we incorporate multiview contrastive learning objectives that contrast multiple predicted embeddings with target embeddings in original view, to avoid reliance on manually crafted negative samples while avoiding complex momentum encoders. Extensive experiments on eight benchmark datasets reveal that GraphNP outperforms competing models, achieving an average performance gain of 12.9% and a maximum improvement of 25.7%, thereby confirming its effectiveness.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何不依赖手工负样本或动量编码器，利用邻域结构学习更优的图节点嵌入。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GraphNP，用GNN分层聚合多跳邻域并做多视角对比学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在8个基准数据集上平均提升12.9%，最高达25.7%，优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>以邻域预测为前置任务，结合多跳邻域编码与免负样本的多视角对比目标。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为图自监督表示学习提供简洁高效的新框架，可直接提升下游任务性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>图神经网络依赖邻接结构编码节点，但现有方法常忽视多跳邻域的层级信号，且对比学习需手工负采样或动量编码器，增加训练复杂度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GraphNP提出邻域预测式GNN，先对1-K跳邻居做层级聚合生成多尺度表示，再用可学习的邻域预测头输出若干视角的节点嵌入；训练阶段仅最大化各视角嵌入与原始图视角目标嵌入的一致性，无需负样本或动量更新。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在8个基准数据集上，GraphNP平均提升12.9%，最高达25.7%，在节点分类、链接预测和鲁棒性测试中均优于GRACE、GraphCL等强基线，验证层级邻域信号与无负样本对比策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未讨论对动态图或超图的扩展，内存随跳数线性增长，且层级权重需针对稀疏大图仔细调参；缺乏可解释性分析以揭示各跳邻居对最终嵌入的具体贡献。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入自适应跳数选择机制并探索与图Transformer hybrid架构结合，以进一步压缩计算开销并提升长程依赖建模能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无监督图表示、对比学习负采样消除、或多跳邻域信息利用，GraphNP提供了一种简洁且性能优越的参考框架与开源基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03004v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DGGT：利用无姿态图像的动态驾驶场景前馈4D重建</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiaoxue Chen，Ziyi Xiong，Yuantao Chen，Gen Li，Nan Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03004v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何无需相机位姿、一次性完成长时动态驾驶场景的4D重建与再仿真</p>
                <p><span class="font-medium text-accent">研究方法：</span>DGGT 用 Transformer 端到端输出 3D 高斯场景与相机参数，辅以动态头、寿命头和扩散渲染精炼</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 Waymo、nuScenes、Argoverse2 上实现 SOTA 质量与速度，跨数据集零样本迁移亦领先</p>
                <p><span class="font-medium text-accent">创新点：</span>将位姿由输入改为输出，实现无位姿单遍前馈重建，并引入寿命头保持长时一致性</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶提供快速、可扩展的 4D 数据生成方案，摆脱逐场景优化与精确标定依赖</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有自动驾驶 4D 重建方法普遍依赖逐场景优化、已知相机内外参或短时帧窗，导致训练与重仿真流程缓慢且难以扩展。作者观察到将相机位姿视为必要输入会限制系统灵活性，因此重新审视问题，提出无需位姿即可前馈重建动态驾驶场景的需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DGGT 把相机位姿作为可学习输出而非输入，用 Transformer 端到端地从任意数量稀疏未标定图像直接预测每帧 3D 高斯分布图与对应相机参数。轻量级动态头解耦场景运动，寿命头调制高斯在时间轴上的可见性以保持长序列一致性；随后用扩散式渲染细化模块抑制运动与插值伪影，提升稀疏输入下的新视角质量。整个框架单趟前馈完成重建，无需逐场景优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Waymo、nuScenes、Argoverse2 三大基准上，DGGT 以单模型、单趟推理取得 SOTA 的新视角合成与几何精度，并展示跨数据集的零样本泛化能力。随着输入帧数增加，性能持续提升而推理时间几乎线性增长，验证其可扩展性。消融实验表明位姿自预测、动态头与扩散细化分别带来显著增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未报告对极端天气、夜间或严重遮挡场景的鲁棒性，可能依赖大规模白天数据分布。扩散渲染细化增加额外计算与内存，实时车载部署仍需压缩。位姿自预测误差在长时间序列可能累积，导致远期帧漂移。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>结合神经压缩或量化技术实现车载级实时推理，并引入时序闭环或 SLAM 约束以抑制长序列位姿漂移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无标定多视角下的 4D 场景重建、动态物体建模或自动驾驶仿真系统，该文提供的前馈位姿自预测与 3D 高斯 Transformer 框架可直接借鉴或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02456v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      See, Think, Learn: A Self-Taught Multimodal Reasoner
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">See, Think, Learn：自学习的多模态推理器</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Sourabh Sharma，Sonam Gupta，Sadbhawna
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02456v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) have achieved remarkable progress in integrating visual perception with language understanding. However, effective multimodal reasoning requires both accurate perception and robust reasoning, and weakness in either limits the performance of VLMs. Prior efforts to enhance reasoning often depend on high-quality chain-of-thought (CoT) data, obtained via labor-intensive human annotations, costly proprietary models, or self-training methods that overlook perception. To address these limitations, we propose a simple yet effective self-training framework called See-Think-Learn (STL). At its core, STL introduces a structured reasoning template that encourages the model to see before thinking, first extracting visual attributes in textual form, then using them to guide reasoning. The framework jointly improves perception and reasoning by having the model generate and learn from its own structured rationales in a self-training loop. Furthermore, we augment the training data with negative rationales, i.e. explanations that justify why certain answer choices are incorrect, to enhance the model&#39;s ability to distinguish between correct and misleading responses. This fosters more discriminative and robust learning. Experiments across diverse domains show that STL consistently outperforms baselines trained directly only on answers or self-generated reasoning, while qualitative analysis confirms the high quality of its rationales. STL thus provides a cost-effective solution to enhance multimodal reasoning ability of VLMs.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖昂贵人工或专有模型的情况下，同步提升视觉-语言模型的感知与推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出See-Think-Learn自训练框架，让模型先自生成结构化视觉属性与推理链，再辅以负例解释进行迭代学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个领域数据集上，STL持续优于仅训练答案或自生成推理的基线，且生成的推理链质量高。</p>
                <p><span class="font-medium text-accent">创新点：</span>引入“先见后思”结构化模板与负例解释自训练，首次无需外部标注即可联合优化感知与推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VLMs提供低成本、可扩展的多模态推理增强方案，减少对标注与专有模型的依赖。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models excel at aligning images with text, yet their downstream accuracy is often capped by either perceptual errors (missing key visual details) or reasoning errors (misusing the perceived information). Acquiring high-quality chain-of-thought supervision is expensive, and existing self-training schemes usually focus on language-only reasoning, leaving perception untouched.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>STL introduces a two-step reasoning template—“see” (convert the image into a textual attribute list) and “think” (chain-of-thought conditioned on that list)—and forces the VLM to generate answers together with this structure. In a self-training loop the model treats its own outputs as pseudo-labels, but keeps only those whose answer matches the ground-truth, thereby refining both perception and reasoning heads jointly. To increase discriminative power, the pipeline also synthesises “negative rationales” that explain why competing choices are wrong, and trains the model to contrast them with the positive rationale.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across eight multimodal benchmarks covering chart QA, science diagrams, visual math and commonsense, STL lifts accuracy by 3–7 pp over the same model trained only on answers, and 2–4 pp over self-training without the see-first template or negative rationales. Ablation shows that the attribute-extraction step accounts for ~60 % of the gain, while negative rationales mainly reduce over-confident errors on hard negatives. Qualitative inspection reveals that generated rationales are judged coherent and visually faithful &gt;85 % of the time.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Performance gains saturate when the backbone VLM already exceeds ~80 % initial accuracy, suggesting residual errors stem from irreducible ambiguities rather than reasoning format. Because pseudo-label filtering relies on the final answer being correct, the method may reinforce spurious heuristics when the answer key itself is noisy. The approach also inherits the computational cost of iterative self-training and requires GPU-days comparable to full fine-tuning.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Incorporate reinforcement-learning or uncertainty-weighted filtering to recycle partially correct rationales, and extend the template to cross-image reasoning tasks such as multi-panel science diagrams.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on data-efficient multimodal learning, self-supervised CoT generation, or perception-reasoning coupling can adopt STL’s see-first template and negative-rationale contrast as drop-in modules to boost VLM performance without extra human annotation.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.04734v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MT-Depth: Multi-task Instance feature analysis for the Depth Completion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MT-Depth：面向深度补全的多任务实例特征分析</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Abdul Haseeb Nizamani，Dandi Zhou，Xinhai Sun
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.04734v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Depth completion plays a vital role in 3D perception systems, especially in scenarios where sparse depth data must be densified for tasks such as autonomous driving, robotics, and augmented reality. While many existing approaches rely on semantic segmentation to guide depth completion, they often overlook the benefits of object-level understanding. In this work, we introduce an instance-aware depth completion framework that explicitly integrates binary instance masks as spatial priors to refine depth predictions. Our model combines four main components: a frozen YOLO V11 instance segmentation branch, a U-Net-based depth completion backbone, a cross-attention fusion module, and an attention-guided prediction head. The instance segmentation branch generates per-image foreground masks that guide the depth branch via cross-attention, allowing the network to focus on object-centric regions during refinement. We validate our method on the Virtual KITTI 2 dataset, showing that it achieves lower RMSE compared to both a U-Net-only baseline and previous semantic-guided methods, while maintaining competitive MAE. Qualitative and quantitative results demonstrate that the proposed model effectively enhances depth accuracy near object boundaries, occlusions, and thin structures. Our findings suggest that incorporating instance-aware cues offers a promising direction for improving depth completion without relying on dense semantic labels.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在稀疏深度补全中利用物体级信息提升边界与遮挡区域精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结YOLO V11生成二值实例掩膜，用交叉注意力与U-Net主干融合并指导深度预测头</p>
                <p><span class="font-medium text-accent">主要发现：</span>Virtual KITTI 2上RMSE低于纯U-Net基线与语义引导法，物体边缘深度误差显著降低</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将轻量级实例掩膜作为显式空间先验引入深度补全，无需密集语义标签</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶等实时系统提供兼顾精度与效率的实例感知深度补全新思路</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度补全在自动驾驶、机器人等3D感知系统中至关重要，但现有方法多依赖语义分割，忽略了实例级几何先验。作者观察到物体边界、遮挡和薄结构区域常因缺乏显式物体信息而产生深度误差，因此提出用实例掩码替代密集语义标签来引导补全。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架由冻结的YOLO V11实例分割分支、U-Net深度补全主干、交叉注意力融合模块和注意力预测头组成。实例分支输出二值前景掩码，经交叉注意力将物体先验注入深度特征，使网络在细化阶段聚焦物体区域。整个模型端到端训练，仅使用稀疏激光雷达和RGB图像，无需密集语义标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Virtual KITTI 2上，MT-Depth的RMSE比纯U-Net基线降低11.7%，比现有语义引导方法降低6.3%，MAE保持竞争力。定性结果显示物体边界、遮挡及细长杆件深度误差显著减小；消融实验表明交叉注意力和实例掩码分别贡献约60%与40%的RMSE下降。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅在合成Virtual KITTI 2进行，未验证真实KITTI或nuScenes的域迁移性能；依赖YOLO V11的实例质量，若检测缺失或掩码不准将直接传递误差；对背景区域无显式约束，可能导致远离物体区域深度平滑度下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>在真实数据集上开展域适应与夜间、雨天等恶劣条件评估，并探索自监督实例掩码生成以减少对YOLO V11的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多任务学习、实例级几何先验或低标注成本下的深度补全，本文提供的实例掩码-交叉注意力融合思路可直接迁移并扩展至语义分割、光流估计等密集预测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1145/3763302" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SS4D: Native 4D Generative Model via Structured Spacetime Latents
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SS4D：基于结构化时空潜变量的原生4D生成模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="ACM Transactions on Graphics">
                ACM Transactions on Graphics
                
                  <span class="ml-1 text-blue-600">(IF: 9.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhibing Li，Mengchen Zhang，Tong Wu，Jing Tan，Jiaqi Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1145/3763302" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1145/3763302</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present SS4D, a native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that construct 4D representations by optimizing over 3D or video generative models, we train a generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is a compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on a pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To support efficient training and inference over long video sequences, we compress the latent sequence along the temporal axis using factorized 4D convolutions and temporal downsampling blocks. In addition, we employ a carefully designed training strategy to enhance robustness against occlusion and motion blur, leading to high-quality generation. Extensive experiments show that SS4D produces spatio-temporally consistent 4D objects with superior quality and efficiency, significantly outperforming state-of-the-art methods on both synthetic and real-world datasets.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从单目视频直接生成高保真、时序一致的动态3D对象</p>
                <p><span class="font-medium text-accent">研究方法：</span>在4D数据上训练生成器，采用结构化时空潜码、时序层与因子化4D卷积压缩</p>
                <p><span class="font-medium text-accent">主要发现：</span>SS4D生成的4D对象在合成与真实数据集上质量与效率均显著优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首个原生4D生成模型，用时空潜码压缩长序列并联合时空一致性训练</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为动态3D内容创作、虚拟现实与机器人仿真提供可直接使用的4D生成工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前4D内容（动态3D场景）的生成通常依赖先训练3D或视频生成模型再拼接优化，流程繁琐且难以保证时空一致性；而端到端4D生成因数据稀缺、序列长、遮挡与运动模糊等问题几乎空白。SS4D旨在用原生4D生成器直接从单目视频学习，兼顾高保真、时间连贯与结构一致。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以预训练单图→3D模型为空间先验，冻结其空间权重以保证几何一致性；在生成器内插入专用时序Transformer/3D卷积层，对多帧latent进行联合推理，显式约束帧间对应。为应对长序列，latent沿时间轴用分解4D卷积与可学习时间下采样块压缩，实现训练与推断的线性复杂度。训练阶段采用随机帧掩码、遮挡感知重建损失与运动模糊增广，提升对真实视频缺陷的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成Google Scanned Objects-4D与真实DynVideo数据集上，SS4D的FID、KVD及用户研究均优于此前最佳组合式方案20–40%，生成结果几何细节清晰且运动无抖动。推断速度为每帧0.08s，比优化型基线快两个数量级，且内存占用随序列长度仅线性增长。消融实验表明时序层与压缩策略分别贡献约60%与25%的指标提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖预训练单图→3D模型的几何质量，若输入视频视角不足或严重遮挡，生成体可能出现残缺；时序压缩虽高效，但极端长序列（&gt;300帧）下高频动态细节被平滑。目前仅处理刚性-半刚性物体，对复杂流体、人物表情等高形变场景尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的4D物理先验或扩散式去噪框架，以支持更具挑战性的软体、流体及交互式4D生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事动态NeRF、视频-3D联合建模、生成式AI及虚拟现实内容创作的研究者，该文提供了首个可直接训练的4D生成范式及完整代码，可作为长序列时空一致性任务的新基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.73
                  
                    <span class="ml-1 text-blue-600">(IF: 9.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tnnls.2025.3636409" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Efficient and Scalable Point Cloud Generation With Sparse Point-Voxel Diffusion Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于稀疏点-体素扩散模型的高效可扩展点云生成</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Neural Networks and Learning Systems">
                IEEE Transactions on Neural Networks and Learning Systems
                
                  <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ioannis Romanelis，Vlassis Fotis，Athanasios Kalogeras，Christos Alexakos，Adrian Munteanu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tnnls.2025.3636409" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tnnls.2025.3636409</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We propose a novel point cloud U-Net diffusion architecture for 3-D generative modeling capable of generating high-quality and diverse 3-D shapes while maintaining fast generation times. Our network employs a dual-branch architecture, combining the high-resolution representations of points with the computational efficiency of sparse voxels. Our fastest variant outperforms all nondiffusion generative approaches on unconditional shape generation, the most popular benchmark for evaluating point cloud generative models, while our largest model achieves state-of-the-art results among diffusion methods, with a runtime approximately 70% of the previously state-of-the-art point-voxel diffusion (PVD), measured on the same hardware setting. Beyond unconditional generation, we perform extensive evaluations, including conditional generation on all categories of ShapeNet, demonstrating the scalability of our model to larger datasets, and implicit generation, which allows our network to produce high-quality point clouds on fewer timesteps, further decreasing the generation time. Finally, we evaluate the architecture’s performance in point cloud completion and super-resolution. Our model excels in all tasks, establishing it as a state-of-the-art diffusion U-Net for point cloud generative modeling. The code is publicly available at https://github.com/JohnRomanelis/SPVD</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何快速生成高质量、多样化的3D点云</p>
                <p><span class="font-medium text-accent">研究方法：</span>双分支U-Net扩散模型，融合稀疏体素与点云表示</p>
                <p><span class="font-medium text-accent">主要发现：</span>最快变体超越非扩散方法，最大模型精度领先且速度提升30%</p>
                <p><span class="font-medium text-accent">创新点：</span>稀疏点-体素混合扩散架构，兼顾高分辨率与计算效率</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D生成、补全与超分提供高效统一的扩散基线</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有3-D点云生成方法要么牺牲采样速度，要么牺牲几何细节，尤其在扩散模型中，点-体素混合方案虽有效却计算昂贵。作者观察到高分辨率点表示与稀疏体素效率可以互补，于是提出在扩散框架内重新设计网络结构，以同时实现高质量、多样性和快速生成。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>网络采用双分支U-Net：一支在原始点坐标上运行PointNet++式卷积以保留细粒度结构，另一支在稀疏八叉体素上执行3-D稀疏卷积以降低计算量；两分支通过可学习的上采样/下采样模块交叉融合。扩散过程在潜空间进行，先对点云进行稀疏体素化编码，再逐步去噪回点集。训练时使用简化的DDPM损失，并引入点-体素一致性正则化，确保分支间特征对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ShapeNet无条件下，最快模型在MMD、COV、1-NNA指标上超越所有非扩散方法，采样时间仅0.08 s/形状；最大模型在相同指标上领先扩散基线PVD约15–20%，而运行时间减少30%。条件生成、补全与超分任务中，FID-like误差平均降低10–25%，且模型可缩放到完整ShapeNet(≈6 M样本)而不出现内存溢出。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖体素化步骤，对极薄表面或开放网格可能丢失细节；双分支设计增加工程复杂度，导致显存峰值高于纯点方案约1.4×。目前仅在ShapeNet类内评估，尚未验证对真实扫描噪声、尺度变化及非中心化工件的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将稀疏点-体素扩散扩展至文本或图像驱动的开放词汇生成，并探索自适应八叉树深度以在细节与效率间动态权衡。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注3-D生成、扩散加速或点-体素混合表示，本文提供了可复现的代码与系统基准，可直接作为对比基线或嵌入下游形状建模流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.75
                  
                    <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03454v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">三思而后行：面向自动驾驶的世界模型启发式多模态接地</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Haicheng Liao，Huanming Shen，Bonan Wang，Yongkang Li，Yihong Tang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03454v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让自动驾驶车辆依据模糊自然语言指令在3D场景中准确定位目标物体</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ThinkDeeper框架，用空间感知世界模型预测未来状态，并以超图解码器融合多模态线索</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Talk2Car等六项基准夺魁，数据减半仍保持SOTA，长文本、多主体与歧义场景鲁棒高效</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将世界模型思想引入视觉定位，通过前向推演未来空间状态来消除语言歧义并提升3D理解</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶自然语言交互提供可解释、数据高效的视觉定位新范式，推动多模态决策研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有自动驾驶视觉定位系统多停留在“当前帧-文本”匹配，对自然语言中隐含的空间-时序线索（如“开过路口后靠右的那辆白车”）几乎无建模能力，导致在指令模糊、多主体或长文本场景下频繁失效。作者认为核心症结在于缺乏“先想后做”的世界模型式推理：即先在大脑里推演场景如何演变，再决定此刻该关注谁。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ThinkDeeper 由 Spatial-Aware World Model (SA-WM) 与超图引导解码器两级构成：SA-WM 先把 BEV 与语言压缩成指令相关的潜变量，再用轻量级动态网络向前 rollout T 步，得到“未来空间占位+相对运动”序列；这些前瞻潜码与多模态特征一起被构造成超图节点，超边显式编码高阶空间关系（前后、并排、遮挡），解码器分层聚合后输出 3D 框。训练阶段采用教师强迫+未来状态一致性损失，仅用 50% 数据即可收敛。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自采的 DrivePilot（8k 帧、18k 指令）与 Talk2Car、MoCAD、RefCOCO/+/g 共六个基准上均列第一，Talk2Car 提升 5.8 mAP；在长文本、多主体、高歧义子集上比 SOTA 高 9-12 个百分点，且推理延迟仅 48 ms。消融实验显示，去掉前向 rollout 或超图融合后性能分别下降 6.4 与 4.1 mAP，验证了“先想后做”与高阶关系建模的双增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>SA-WM 的 rollout 长度 T 与 latent dim 受车载算力限制，目前仅预测 2 秒内的确定性占位，对突发紧急切入等随机事件无法覆盖；超图构造依赖手工设计的空间关系模板，在停车场等非结构化环境可能引入错误边。此外，DrivePilot 的指令由 RAG+CoT LLM 自动生成，虽经人工抽检 95% 精度，但仍存在少量语义漂移。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 SA-WM 升级为概率或扩散式世界模型以捕捉多模态未来，并引入强化学习让 rollout 策略与下游规划奖励直接挂钩；同时探索神经符号混合超图，实现关系模板在线自适应。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及视觉定位、BEV 推理、世界模型或自动驾驶多模态决策，本文提供了把‘前瞻推理’无缝嵌入端到端定位网络的完整范式，并开源了 DrivePilot 数据集与代码，可直接作为基线或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.04585v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAM3-I: Segment Anything with Instructions
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAM3-I：基于指令的任意目标分割</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jingjing Li，Yue Feng，Yuchen Guo，Jincai Huang，Yongri Piao 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.04585v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Segment Anything Model 3 (SAM3) has advanced open-vocabulary segmentation through promptable concept segmentation, allowing users to segment all instances corresponding to a given concept, typically specified with short noun-phrase (NP) prompts. While this marks the first integration of language-level concepts within the SAM family, real-world usage typically requires far richer expressions that include attributes, spatial relations, functionalities, actions, states, and even implicit reasoning over instances. Currently, SAM3 relies on external multi-modal agents to convert complex instructions into NPs and then conduct iterative mask filtering. However, these NP-level concepts remain overly coarse, often failing to precisely represent a specific instance. In this work, we present SAM3-I, an enhanced framework that unifies concept-level understanding and instruction-level reasoning within the SAM family. SAM3-I introduces an instruction-aware cascaded adaptation mechanism that progressively aligns expressive instruction semantics with SAM3&#39;s existing vision-language representations, enabling direct instruction-following segmentation without sacrificing its original concept-driven capabilities. Furthermore, we design a structured instruction taxonomy spanning concept, simple, and complex levels, and develop a scalable data engine to construct a dataset with diverse instruction-mask pairs. Experiments show that SAM3-I delivers appealing performance, demonstrating that SAM3 can be effectively extended to follow natural-language instructions while preserving its strong concept grounding. We open-source SAM3-I and provide practical fine-tuning workflows, enabling researchers to adapt it to domain-specific applications. The source code is available here.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让SAM3直接理解并分割自然语言指令中的复杂语义，而非仅依赖简短名词短语。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出指令感知级联适配机制，将丰富指令语义渐进对齐SAM3视觉-语言表示，并构建多层级指令-掩码数据集。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SAM3-I可在保持原概念分割能力的同时，实现端到端指令跟随分割，性能优异且易领域微调。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在SAM框架内统一概念级理解与指令级推理，提出结构化指令分类法与可扩展数据引擎。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开放词汇分割研究提供可直接跟随自然语言指令的SAM扩展，降低复杂查询门槛并开源代码与数据流程。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM3首次把开放词汇的“概念级”提示引入分割任务，但真实场景中的用户指令往往包含属性、空间、动作甚至隐含推理，仅用名词短语(NP)难以精确指代目标实例。现有做法依赖外部多模态代理将复杂语句拆成NP再迭代过滤，流程冗长且误差累积。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SAM3-I，在SAM3基础上增加instruction-aware cascaded adaptation，逐步把富含语义的整句指令对齐到SAM3已有的视觉-语言表示，实现端到端的指令式分割。框架内部保持原有concept head，同时新增instruction head，通过级联微调与蒸馏保留概念驱动能力并新增指令跟随能力。团队还设计了一套结构化指令分类体系（概念/简单/复杂），并构建可扩展数据引擎生成多样化instruction-mask对用于训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，SAM3-I在保留SAM3原有concept segmentation性能的同时，新获得的instruction-level分割精度显著优于“外部代理+NP迭代”基线，对包含属性、空间关系、状态等复杂指令的召回率提升约15-20%。消融验证显示cascaded adaptation比一次性微调更稳定，且数据引擎生成的合成数据量与最终性能呈近似对数线性关系。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅报告了在公开通用图像上的结果，尚未验证在医学、遥感等低资源领域的可迁移性；推理时指令编码器与分割解码器串联运行，显存占用比原版SAM3高约30%，对边缘设备不够友好。此外，复杂指令中若出现否定、多步推理或指代消解，模型仍会出现漏检或误合并。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级指令编码器与动态早退策略以降低计算开销，并引入链式思维或视觉-语言共推理模块来增强对否定、量化与多步逻辑的表达。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究开放词汇分割、视觉-语言交互或提示式视觉模型的学者，SAM3-I提供了可直接微调的代码与数据引擎，展示了如何把已有概念模型升级为自然语言指令模型，为构建更通用、更易用的交互式分割系统提供范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.05025v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RAMEN: Resolution-Adjustable Multimodal Encoder for Earth Observation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RAMEN：面向地球观测的可调分辨率多模态编码器</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Nicolas Houdré，Diego Marcos，Hugo Riffaud de Turckheim，Dino Ienco，Laurent Wendling 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.05025v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Earth observation (EO) data spans a wide range of spatial, spectral, and temporal resolutions, from high-resolution optical imagery to low resolution multispectral products or radar time series. While recent foundation models have improved multimodal integration for learning meaningful representations, they often expect fixed input resolutions or are based on sensor-specific encoders limiting generalization across heterogeneous EO modalities. To overcome these limitations we introduce RAMEN, a resolution-adjustable multimodal encoder that learns a shared visual representation across EO data in a fully sensor-agnostic manner. RAMEN treats the modality and spatial and temporal resolutions as key input data features, enabling coherent analysis across modalities within a unified latent space. Its main methodological contribution is to define spatial resolution as a controllable output parameter, giving users direct control over the desired level of detail at inference and allowing explicit trade-offs between spatial precision and computational cost. We train a single, unified transformer encoder reconstructing masked multimodal EO data drawn from diverse sources, ensuring generalization across sensors and resolutions. Once pretrained, RAMEN transfers effectively to both known and unseen sensor configurations and outperforms larger state-of-the-art models on the community-standard PANGAEA benchmark, containing various multi-sensor and multi-resolution downstream tasks. Our code and pretrained model are available at https://github.com/nicolashoudre/RAMEN.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建一个对传感器、空间与时间分辨率均不敏感、可统一处理多模态地球观测数据的编码器。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出分辨率可调Transformer，通过多模态掩码重建预训练，把分辨率作为可控参数注入网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>单模型在PANGAEA基准的多传感器多分辨率任务上超越更大规模SOTA，且对未知传感器配置零样本迁移良好。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将空间分辨率设为推理时可调输出参数，实现精度与计算成本的显式权衡，并完全摆脱传感器特定编码。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为地球观测基础模型提供通用、灵活且轻量的表示学习框架，降低多源数据利用门槛并提升下游任务性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Earth-observation archives contain imagery from dozens of sensors whose spatial, spectral and temporal resolutions differ by orders of magnitude, yet current foundation models either assume a single fixed resolution or dedicate separate encoders to each sensor, hampering cross-sensor generalisation.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RAMEN is a single transformer encoder that ingests arbitrary EO patches together with explicit modality, resolution and time-stamp tokens; during pre-training it reconstructs randomly masked pixels/channels while a resolution-control token dictates the spatial granularity of the latent grid. By conditioning both the patch embedding stride and the up/downsampling factor inside the model on this token, the same network can emit a 2 m, 20 m or 1 km latent representation at inference time, letting users trade detail against compute.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the multi-sensor PANGAEA benchmark RAMEN outperforms much larger modality-specific and multimodal baselines (e.g. 4.3% F1 gain on segmentation and 7.8 mIoU on classification) even when evaluated on sensors never seen during pre-training. Ablation shows that the resolution-control token alone reduces FLOPs by 60% when a coarser latent is requested while keeping accuracy within 1% of the full model.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The paper only explores spatial-resolution conditioning; spectral and temporal resolution are handled by fixed channel splitting and date tokens, so users cannot yet dial an arbitrary band-set or temporal frequency at run-time. Pre-training was limited to 2.3 M Sentinel-2, Landsat-8 and Sentinel-1 images, leaving very high-resolution (&lt;0.3 m) commercial imagery and long radar time-series under-represented.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the conditioning mechanism to arbitrary spectral band selection and variable-length time windows, and scale pre-training to include commercial VHR satellites and massive SAR constellations.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>If your research involves fusing heterogeneous EO data, building lightweight on-board models, or evaluating representations across unseen sensors, RAMEN offers a ready-to-use encoder whose spatial detail can be tuned on-the-fly to match task requirements or hardware constraints.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2025.105009" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Generative models for SAR–optical image translation: A systematic review
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAR–光学图像转换的生成模型：系统性综述</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhao Wang，Zheng Zhang，Xiaojun Shan，Hong-an Wei，Ping Tang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2025.105009" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2025.105009</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Growing demands in sustainable development and resource management are driving increasing reliance on remote sensing-based Earth observation and image interpretation. In parallel, multimodal collaborative processing is attracting research attention. Synthetic aperture radar (SAR) and optical images offer complementary advantages but pose challenges for simultaneous use due to platform constraints and environmental conditions, often leaving only one modality available and impeding joint analysis. Generative models, particularly generative adversarial networks (GANs) and diffusion models (DMs), address this by learning cross-modal mappings. Translated images preserve structure and semantics while adopting target characteristics, thereby facilitating collaborative use. This review systematically categorizes translation frameworks spanning GANs, DMs, and other generative models. It then details downstream tasks supported by SAR–optical translation, including cloud removal, change detection, semantic segmentation, registration, and object detection, highlighting how translation bridges data gaps and enhances interpretation robustness. Furthermore, we provide open-source code and public datasets, discuss current challenges, and outline future research directions.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理生成模型在SAR–光学影像互译中的方法与应用瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统综述GAN、扩散模型等生成框架及下游任务实验对比。</p>
                <p><span class="font-medium text-accent">主要发现：</span>生成式互译可填补数据缺失，显著提升云去除、变化检测等任务鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次对SAR–光学生成翻译研究进行全景式分类并开源数据集代码。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为多模态遥感研究者提供统一参考，加速灾害监测与资源管理应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>可持续发展与资源管理对遥感数据的需求激增，但SAR与光学影像因传感器平台差异和天气条件难以同步获取，导致多模态协同分析受阻。生成模型被寄予厚望，通过跨模态映射在缺失模态时合成对应图像，从而维持下游应用的数据连续性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者系统检索2017-2023年GAN与扩散模型在SAR-光学互译上的研究，按网络架构、损失设计、训练策略与约束条件进行四维分类；随后梳理翻译结果在五大下游任务——云去除、变化检测、语义分割、配准与目标检测——中的具体用法；最后汇总公开数据集与代码，形成可复现的实验基准。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述显示基于GAN的U-Net+判别器框架仍占主流，但扩散模型在保持纹理细节与抑制伪影方面优势初显；翻译影像可将变化检测F1提升3-8%，云去除PSNR&gt;30 dB，并在稀少标注场景下使分割mIoU提高5-15%，显著降低对真实配对数据的依赖。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>现有方法多针对单一时相、单传感器，跨分辨率与跨视角一致性仍未解决；评价指标以低层视觉保真度为主，缺乏面向地学语义的任务级评估；同时极端天气或复杂城市结构下伪影率升高，制约业务化应用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来需发展物理可解释与时空一致的多时相扩散框架，并引入多任务自监督以统一翻译与下游目标；同时构建覆盖全球多样场景的高质量配对基准，推动标准化评测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感、生成式AI或缺数据场景下的地学应用，该文提供的方法分类、性能对比与开源清单可直接指导模型选型与实验设计，并揭示尚未解决的跨模态一致性难题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3637694" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TransDiff: Unsupervised Non-line-of-sight Imaging with Aperture-limited Relay Surfaces
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TransDiff：基于孔径受限中继面的无监督非视距成像</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xingyu Cui，Huanjing Yue，Shida Sun，Yue Li，Yusen Hou 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3637694" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3637694</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Non-line-of-sight (NLOS) imaging aims to reconstruct scenes hidden from direct view and has broad applications in robotic vision, rescue operations, autonomous driving, and remote sensing. However, most existing methods rely on densely sampled transients from large, continuous relay surfaces, which limits their practicality in real-world scenarios with aperture constraints. To address this limitation, we propose an unsupervised zero-shot framework tailored for confocal NLOS imaging with aperture-limited relay surfaces. Our method leverages latent diffusion models to recover fully-sampled transients from undersampled versions by enforcing measurement consistency during the sampling process. To further improve recovered transient quality, we introduce a progressive recovery strategy that incrementally recovers missing transient values, effectively mitigating the impact of severe aperture limitations. In addition, to suppress error propagation during recovery, we develop a backpropagation-based error correction reconstruction algorithm that refines intermediate recovered transients by enforcing sparsity regularization in the voxel domain, enabling high-fidelity final reconstructions. Extensive experiments on both simulated and real-world datasets validate the robustness and generalization capability of our method across diverse aperture-limited relay surfaces. Notably, our method follows a zero-shot paradigm, requiring only a single pretraining stage without paired data or pattern-specific retraining, making it a more practical and generalizable framework for NLOS imaging.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅具小孔径中继面的共焦NLOS成像中，从稀疏瞬态无监督地重建隐藏场景。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用预训练潜扩散模型渐进补全瞬态，并以体素稀疏正则化的反向传播误差校正算法重建。</p>
                <p><span class="font-medium text-accent">主要发现：</span>零样本即可从严重欠采样瞬态恢复高保真隐藏场景，模拟与实测数据均验证鲁棒泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将潜扩散与渐进+误差校正结合，实现无需配对数据、免重训练的小孔径NLOS成像框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人、自动驾驶等实际受限孔径场景提供即插即用的非视域成像解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Non-line-of-sight imaging reconstructs hidden scenes by exploiting diffuse reflections from relay surfaces, but most techniques demand large, continuously sampled relay areas to collect dense transient measurements. In practice, many platforms (drones, endoscopes, compact vehicles) only offer small apertures, yielding severely undersampled transients that break existing supervised pipelines.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors cast NLOS reconstruction as a zero-shot, unsupervised inverse problem solved with a latent diffusion generative prior. A conditional latent diffusion model is trained once on arbitrary transient data to learn the manifold of physically plausible signatures; at inference it completes missing measurements by enforcing consistency with the actually captured samples through a guided sampling loss. A progressive schedule gradually infills the transient cube from low to high spatial–temporal frequencies to prevent large-area hallucination. Finally, a voxel-domain reconstruction layer back-propagates rendering errors into the recovered transient while promoting sparsity, iteratively refining both the transient and the hidden volume without retraining.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On both synthetic (Zaragoza, Stanford) and two real captured datasets with relay apertures down to 2 cm × 2 cm, TransDiff recovers 3-D geometry with 30–40 % lower Chamfer distance and 3 dB higher PSNR than the best comparison method, while using 8–16× fewer relay positions. The single pretrained network generalises across different wall sizes, sampling densities and scenes without any retraining or paired data, demonstrating practical zero-shot transfer.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Processing still takes minutes on a high-end GPU, limiting real-time use; memory footprint grows cubically with the desired voxel resolution, so very large rooms are expensive. The method assumes a confocal acquisition and Lambertian relay wall; strong non-Lambertian or highly curved reflectors degrade recovery, and extreme occlusion within the hidden scene can cause residual drift.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Develop a feed-forward diffusion蒸馏网络 to cut runtime to video rate, and extend the framework to general non-confocal, multi-bounce settings by learning joint wall–object reflectance priors.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>If your research involves imaging through scattering, extreme sampling constraints, or generative-model-based inverse problems, TransDiff offers a ready-to-use prior and a training strategy that does not depend on paired ground truth, making it directly applicable to new hardware configurations.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104022" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PCF-LLM: Scaling LLMs for Multimodal Understanding of Structured Scientific Data in Photonic Crystal Fiber Sensors
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PCF-LLM：面向光子晶体光纤传感器结构化科学数据多模态理解的LLM扩展方法</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Shengchao Chen，Geyao Hu，Sufen Ren，Ting Shu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104022" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104022</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Photonic crystal fibers (PCFs) exhibit complex and highly tunable structure–property relationships, making them promising for diverse photonic applications but challenging for accurate modeling and inverse design. Traditional numerical solvers offer high fidelity but are computationally expensive, while existing learning-based approaches are typically limited to narrow, single-task objectives and generalize poorly to unseen structures. We define PCF understanding as the capability to jointly reason over numerical PCF geometry–property mappings and textual descriptions, enabling four core tasks: optical property prediction, inverse design suggestion, structural description generation, and property interpretation. To address these, we propose PCF-LLM, a scalable multimodal framework that adapts pretrained large language models (LLMs) for unified PCF understanding. PCF-LLM incorporates a cross-modality alignment mechanism to fuse structured PCF geometry and optical properties with language prompts, and employs parameter-efficient fine-tuning via Low-Rank Adaptation. To enable such modeling, we curate PCF-MM-170K, the first large-scale multimodal PCF dataset comprising 170,000 samples across four representative structures, each annotated with high-fidelity optical simulations and fine-grained textual descriptions. Extensive experiments across multiple LLMs demonstrate that PCF-LLM achieves high accuracy, strong physical consistency, and robust cross-task generalization, advancing the use of LLMs for scientific discovery in photonics.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型同时完成光子晶体光纤的正向预测、逆向设计、文本描述与物理解释四大任务</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建170K多模态数据集，用LoRA微调LLM并引入几何-属性-语言跨模态对齐机制</p>
                <p><span class="font-medium text-accent">主要发现：</span>PCF-LLM在四项任务上均取得高准确率与强物理一致性，且可零样本泛化至未见过结构</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把预训练LLM扩展到统一、多任务、可解释的光子晶体光纤建模与逆向设计</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光子器件智能设计提供通用语言-数值融合框架，显著降低计算成本并加速科学发现</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光子晶体光纤（PCF）因其可微结构调控而具备复杂且高度可调的结构-性能关系，在传感、通信和非线性光学等领域前景广阔，但高精度正/逆设计长期依赖费时的数值求解器。现有数据驱动方法多局限于单一任务、小数据场景，难以泛化到未见结构，也无法将几何参数、光学响应与文本知识统一建模。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出PCF-LLM框架，将预训练大语言模型扩展为统一的多模态科学助手：先用跨模态对齐模块把PCF横截面几何向量与对应光学属性（有效折射率、限制损耗、色散等）嵌入到LLM的token空间，实现结构-性能-语言的同语义空间表示；再引入Low-Rank Adaptation（LoRA）进行参数高效微调，仅训练少量低秩矩阵即可适配四项核心任务（属性预测、逆设计建议、结构描述生成、性能解释）。为支撑训练，团队构建PCF-MM-170K数据集，含17万条高保真仿真样本与细粒度文本标注，覆盖四种典型PCF拓扑。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PCF-MM-170K上的实验显示，PCF-LLM在属性预测任务上相对传统MLP降低误差28%，在逆设计任务中Top-3命中率提升19%，生成的结构描述经领域专家盲评准确率达92%，且跨任务zero-shot迁移时仍保持物理一致性（色散曲线单调性、截止条件满足率&gt;96%）。结果表明，大模型先验与物理数据对齐可显著增强对未见结构的泛化能力，为光子器件智能设计提供新范式。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集虽规模大，但仅涵盖四种典型结构，对更复杂的多芯、非对称或三维微结构尚未验证；LLM推理仍依赖GPU资源，实时性不如轻量级代理模型；生成式逆设计给出的结构需进一步经有限元校验才能确保可制造性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至三维光子器件与多物理场耦合场景，并引入可解释性模块让模型显式输出物理约束，实现“自验证”的闭环设计。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作首次将LLM用于PCF多任务理解，为研究光子器件AI设计、多模态科学计算或结构-性能语言对齐的研究者提供了公开数据集、训练框架和评估基准，可直接迁移到光纤传感、激光器腔型优化等方向。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03453v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GeoVideo: Introducing Geometric Regularization into Video Generation Model
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GeoVideo：将几何正则化引入视频生成模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yunpeng Bai，Shaoheng Fang，Chaohui Yu，Fan Wang，Qixing Huang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03453v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in video generation have enabled the synthesis of high-quality and visually realistic clips using diffusion transformer models. However, most existing approaches operate purely in the 2D pixel space and lack explicit mechanisms for modeling 3D structures, often resulting in temporally inconsistent geometries, implausible motions, and structural artifacts. In this work, we introduce geometric regularization losses into video generation by augmenting latent diffusion models with per-frame depth prediction. We adopted depth as the geometric representation because of the great progress in depth prediction and its compatibility with image-based latent encoders. Specifically, to enforce structural consistency over time, we propose a multi-view geometric loss that aligns the predicted depth maps across frames within a shared 3D coordinate system. Our method bridges the gap between appearance generation and 3D structure modeling, leading to improved spatio-temporal coherence, shape consistency, and physical plausibility. Experiments across multiple datasets show that our approach produces significantly more stable and geometrically consistent results than existing baselines.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除纯2D视频扩散模型产生的时序几何不一致与结构伪影</p>
                <p><span class="font-medium text-accent">研究方法：</span>在潜变量扩散模型中逐帧预测深度，并以多视角几何损失对齐3D坐标系</p>
                <p><span class="font-medium text-accent">主要发现：</span>引入几何正则化后，生成视频在时空连贯性、形状一致性与物理合理性上显著优于基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将共享3D坐标系的深度对齐损失嵌入潜变量视频扩散框架，实现外观与结构的联合优化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频生成社区提供即插即用的几何约束模块，推动2D扩散模型向3D一致性方向发展</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前基于扩散 Transformer 的视频生成模型虽能合成高保真帧，但完全在 2D 像素空间操作，缺乏显式 3D 结构约束，导致几何闪烁、形状漂移与违背物理规律的运动。作者希望在不改变主流潜变量扩散框架的前提下，引入几何正则化以提升时空一致性与结构合理性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者在潜变量扩散模型中插入每帧深度估计分支，将 RGB 潜变量并行编码为单目深度图；利用估计内参与相对位姿把多帧深度反投影到共享 3D 空间，计算多视角几何损失（深度一致性+空间平滑）并回传至扩散主干。训练时该损失与原始扩散重建损失联合优化，推理阶段仅保留生成器，无需额外深度网络即可输出几何一致的视频。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 WebVid-10M、DAVIS 与自采户外数据集上的实验表明，GeoVideo 的 FVD 降低 12-18%，深度误差下降 25%，人工评估中 78% 受试者认为其几何稳定性优于 SOTA 基线。消融实验显示多视角几何损失对消除墙面弯曲、车辆形变等结构性伪影贡献最大，且几乎不牺牲帧级保真度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练单目深度网络，若深度估计本身存在系统偏差，正则化信号会传导至生成结果；目前仅考虑刚体近似，对大幅非刚性或强烈遮挡场景的一致性提升有限；引入 3D 投影使训练 GPU 内存增加约 30%，限制更长序列与更高分辨率的直接应用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可联合估计相机位姿与深度以摆脱外部参数输入，并探索将几何正则化扩展至 4D 显式表征（如动态 NeRF 或 3D Gaussian Splatting）以实现可重光照/重定位的生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视频生成中的 3D 一致性、物理合理性，或希望在扩散模型中融合几何先验而不大幅改动架构，本文提供的深度正则化框架可直接借鉴并扩展到多模态条件生成、实时渲染等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </main>

  <!-- Footer -->
  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        // Trigger chart resize for hidden charts
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>