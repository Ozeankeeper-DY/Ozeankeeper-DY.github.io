<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Thu, 04 Dec 2025 04:20:36 +0000</lastBuildDate><item><title>CityVLM: Towards sustainable urban development via multi-view coordinated vision–language model</title><link>https://doi.org/10.1016/j.isprsjprs.2025.11.030</link><guid>10.1016/j.isprsjprs.2025.11.030</guid><pubDate>Wed, 03 Dec 2025 20:50:27 +0000</pubDate><dc:creator>Junjue Wang</dc:creator><dc:creator>Weihao Xuan</dc:creator><dc:creator>Heli Qi</dc:creator><dc:creator>Zihang Chen</dc:creator><dc:creator>Hongruixuan Chen</dc:creator><dc:creator>Zhuo Zheng</dc:creator><dc:creator>Junshi Xia</dc:creator><dc:creator>Yanfei Zhong</dc:creator><dc:creator>Naoto Yokoya</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.11.030</prism:doi><description>Vision–language models (VLMs) have shown remarkable promise in Earth Vision, particularly in providing human-interpretable analysis of remote sensing imagery. While existing VLMs excel at general visual perception tasks, they often fall short in addressing the complex needs of geoscience, which requires comprehensive urban analysis across geographical, social, and economic dimensions. To bridge this gap, we expand VLM capabilities to tackle sustainable urban development challenges by integrating two complementary sources: remote sensing (RS) and street-view (SV) imagery. Specifically, we first design a multi-view vision–language dataset ( CitySet ), comprising 20,589 RS images, 1.1 million SV images, and 0.8 million question–answer pairs. CitySet facilitates geospatial object reasoning, social object analysis, urban economic assessment, and sustainable development report generation. Besides, we develop CityVLM to integrate macro- and micro-level semantics using geospatial and temporal modeling, while its language modeling component generates detailed urban reports. We extensively benchmarked 10 advanced VLMs on our dataset, revealing that state-of-the-art models struggle with urban analysis tasks, primarily due to domain gaps and limited multi-view data alignment capabilities. By addressing these issues, CityVLM achieves superior performance consistently across all tasks and advances automated urban analysis through practical applications like heat island effect monitoring, offering valuable tools for city planners and policymakers in their sustainability efforts.
Published: 2025-12-03T20:50:27+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.830 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junjue Wang; Weihao Xuan; Heli Qi; Zihang Chen; Hongruixuan Chen; Zhuo Zheng; Junshi Xia; Yanfei Zhong; Naoto Yokoya&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.11.030"&gt;10.1016/j.isprsjprs.2025.11.030&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.830 (must_read)&lt;/p&gt;
&lt;p&gt;Vision–language models (VLMs) have shown remarkable promise in Earth Vision, particularly in providing human-interpretable analysis of remote sensing imagery. While existing VLMs excel at general visual perception tasks, they often fall short in addressing the complex needs of geoscience, which requires comprehensive urban analysis across geographical, social, and economic dimensions. To bridge this gap, we expand VLM capabilities to tackle sustainable urban development challenges by integrating two complementary sources: remote sensing (RS) and street-view (SV) imagery. Specifically, we first design a multi-view vision–language dataset ( CitySet ), comprising 20,589 RS images, 1.1 million SV images, and 0.8 million question–answer pairs. CitySet facilitates geospatial object reasoning, social object analysis, urban economic assessment, and sustainable development report generation. Besides, we develop CityVLM to integrate macro- and micro-level semantics using geospatial and temporal modeling, while its language modeling component generates detailed urban reports. We extensively benchmarked 10 advanced VLMs on our dataset, revealing that state-of-the-art models struggle with urban analysis tasks, primarily due to domain gaps and limited multi-view data alignment capabilities. By addressing these issues, CityVLM achieves superior performance consistently across all tasks and advances automated urban analysis through practical applications like heat island effect monitoring, offering valuable tools for city planners and policymakers in their sustainability efforts.&lt;/p&gt;</content:encoded></item><item><title>GeoViS: Geospatially Rewarded Visual Search for Remote Sensing Visual Grounding</title><link>https://arxiv.org/abs/2512.02715v1</link><guid>http://arxiv.org/abs/2512.02715v1</guid><pubDate>Tue, 02 Dec 2025 12:45:52 +0000</pubDate><dc:creator>Peirong Zhang</dc:creator><dc:creator>Yidan Zhang</dc:creator><dc:creator>Luxiao Xu</dc:creator><dc:creator>Jinliang Lin</dc:creator><dc:creator>Zonghao Guo</dc:creator><dc:creator>Fengxiang Wang</dc:creator><dc:creator>Xue Yang</dc:creator><dc:creator>Kaiwen Wei</dc:creator><dc:creator>Lei Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advances in multimodal large language models(MLLMs) have led to remarkable progress in visual grounding, enabling fine-grained cross-modal alignment between textual queries and image regions. However, transferring such capabilities to remote sensing imagery remains challenging, as targets are often extremely small within kilometer-scale scenes, and queries typically involve intricate geospatial relations such as relative positions, spatial hierarchies, or contextual dependencies across distant objects. To address these challenges, we propose GeoViS, a Geospatially Rewarded Visual Search framework that reformulates remote sensing visual grounding as a progressive search-and-reasoning process. Rather than directly predicting the target location in a single step, GeoViS actively explores the global image through a tree-structured sequence of visual cues, integrating multimodal perception, spatial reasoning, and reward-guided exploration to refine geospatial hypotheses iteratively. This design enables the model to detect subtle small-scale targets while maintaining holistic scene awareness. Extensive experiments on five remote sensing grounding benchmarks demonstrate that GeoViS achieves precise geospatial understanding and consistently surpasses existing methods across key visual grounding metrics, highlighting its strong cross-domain generalization and interpretability.
Published: 2025-12-02T12:45:52+00:00
Venue: arXiv
Score: 0.819 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Peirong Zhang; Yidan Zhang; Luxiao Xu; Jinliang Lin; Zonghao Guo; Fengxiang Wang; Xue Yang; Kaiwen Wei; Lei Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.819 (must_read)&lt;/p&gt;
&lt;p&gt;Recent advances in multimodal large language models(MLLMs) have led to remarkable progress in visual grounding, enabling fine-grained cross-modal alignment between textual queries and image regions. However, transferring such capabilities to remote sensing imagery remains challenging, as targets are often extremely small within kilometer-scale scenes, and queries typically involve intricate geospatial relations such as relative positions, spatial hierarchies, or contextual dependencies across distant objects. To address these challenges, we propose GeoViS, a Geospatially Rewarded Visual Search framework that reformulates remote sensing visual grounding as a progressive search-and-reasoning process. Rather than directly predicting the target location in a single step, GeoViS actively explores the global image through a tree-structured sequence of visual cues, integrating multimodal perception, spatial reasoning, and reward-guided exploration to refine geospatial hypotheses iteratively. This design enables the model to detect subtle small-scale targets while maintaining holistic scene awareness. Extensive experiments on five remote sensing grounding benchmarks demonstrate that GeoViS achieves precise geospatial understanding and consistently surpasses existing methods across key visual grounding metrics, highlighting its strong cross-domain generalization and interpretability.&lt;/p&gt;</content:encoded></item><item><title>Cross-Stain Contrastive Learning for Paired Immunohistochemistry and Histopathology Slide Representation Learning</title><link>https://arxiv.org/abs/2512.03577v1</link><guid>http://arxiv.org/abs/2512.03577v1</guid><pubDate>Wed, 03 Dec 2025 09:00:27 +0000</pubDate><dc:creator>Yizhi Zhang</dc:creator><dc:creator>Lei Fan</dc:creator><dc:creator>Zhulin Tao</dc:creator><dc:creator>Donglin Di</dc:creator><dc:creator>Yang Song</dc:creator><dc:creator>Sidong Liu</dc:creator><dc:creator>Cong Cong</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Universal, transferable whole-slide image (WSI) representations are central to computational pathology. Incorporating multiple markers (e.g., immunohistochemistry, IHC) alongside H&amp;E enriches H&amp;E-based features with diverse, biologically meaningful information. However, progress is limited by the scarcity of well-aligned multi-stain datasets. Inter-stain misalignment shifts corresponding tissue across slides, hindering consistent patch-level features and degrading slide-level embeddings. To address this, we curated a slide-level aligned, five-stain dataset (H&amp;E, HER2, KI67, ER, PGR) to enable paired H&amp;E-IHC learning and robust cross-stain representation. Leveraging this dataset, we propose Cross-Stain Contrastive Learning (CSCL), a two-stage pretraining framework with a lightweight adapter trained using patch-wise contrastive alignment to improve the compatibility of H&amp;E features with corresponding IHC-derived contextual cues, and slide-level representation learning with Multiple Instance Learning (MIL), which uses a cross-stain attention fusion module to integrate stain-specific patch features and a cross-stain global alignment module to enforce consistency among slide-level embeddings across different stains. Experiments on cancer subtype classification, IHC biomarker status classification, and survival prediction show consistent gains, yielding high-quality, transferable H&amp;E slide-level representations. The code and data are available at https://github.com/lily-zyz/CSCL.
Published: 2025-12-03T09:00:27+00:00
Venue: arXiv
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yizhi Zhang; Lei Fan; Zhulin Tao; Donglin Di; Yang Song; Sidong Liu; Cong Cong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;Universal, transferable whole-slide image (WSI) representations are central to computational pathology. Incorporating multiple markers (e.g., immunohistochemistry, IHC) alongside H&amp;amp;E enriches H&amp;amp;E-based features with diverse, biologically meaningful information. However, progress is limited by the scarcity of well-aligned multi-stain datasets. Inter-stain misalignment shifts corresponding tissue across slides, hindering consistent patch-level features and degrading slide-level embeddings. To address this, we curated a slide-level aligned, five-stain dataset (H&amp;amp;E, HER2, KI67, ER, PGR) to enable paired H&amp;amp;E-IHC learning and robust cross-stain representation. Leveraging this dataset, we propose Cross-Stain Contrastive Learning (CSCL), a two-stage pretraining framework with a lightweight adapter trained using patch-wise contrastive alignment to improve the compatibility of H&amp;amp;E features with corresponding IHC-derived contextual cues, and slide-level representation learning with Multiple Instance Learning (MIL), which uses a cross-stain attention fusion module to integrate stain-specific patch features and a cross-stain global alignment module to enforce consistency among slide-level embeddings across different stains. Experiments on cancer subtype classification, IHC biomarker status classification, and survival prediction show consistent gains, yielding high-quality, transferable H&amp;amp;E slide-level representations. The code and data are available at https://github.com/lily-zyz/CSCL.&lt;/p&gt;</content:encoded></item><item><title>Alliance: All-in-One Spectral-Spatial-Frequency Awareness Foundation Model</title><link>https://doi.org/10.1109/tpami.2025.3639595</link><guid>10.1109/tpami.2025.3639595</guid><pubDate>Wed, 03 Dec 2025 18:41:57 +0000</pubDate><dc:creator>Boyu Zhao</dc:creator><dc:creator>Wei Li</dc:creator><dc:creator>Junjie Wang</dc:creator><dc:creator>Yuxiang Zhang</dc:creator><dc:creator>Hong Yang</dc:creator><dc:creator>Haitao Zhao</dc:creator><dc:creator>Ran Tao</dc:creator><dc:creator>Qian Du</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3639595</prism:doi><description>Frequency domain analysis reveals fundamental image patterns difficult to observe in raw pixel values, while avoiding redundant information in original image processing. Although recent remote sensing foundation models (FMs) have made progress in leveraging spatial and spectral information, they have limitations in fully utilizing frequency characteristics that capture hidden features. Existing FMs that incorporate frequency properties often struggle to maintain connections with the original image content, creating a semantic gap that affects downstream performance. To address these challenges, we propose the All-in-One Spectral-Spatial-Frequency Awareness Foundation Model (Alliance), a framework that effectively integrates information across all three domains. Alliance introduces several key innovations: (1) a progressive frequency decoding mechanism inspired by human visual cognition that minimizes multi-domain information gaps while preserving connections between general image information and frequency characteristics, progressively reconstructing from low to mid to high frequencies to extract patterns difficult to observe in raw pixel values; (2) a triple-domain fusion attention module that separately processes amplitude, phase, and spectral-spatial relationships for comprehensive feature integration; and (3) frequency embedding with frequency-aware Cls token initialization and frequency-specific mask token initialization that achieves fine-grained modeling of different frequency band information. Additionally, to evaluate FMs generalizability, we construct the Yellow River dataset, a large-scale multi-temporal collection that introduces challenging cross-domain tasks and establishes more rigorous standards for FMs assessment. Extensive experiments across six downstream tasks demonstrate Alliance's superior performance.
Published: 2025-12-03T18:41:57+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Boyu Zhao; Wei Li; Junjie Wang; Yuxiang Zhang; Hong Yang; Haitao Zhao; Ran Tao; Qian Du&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3639595"&gt;10.1109/tpami.2025.3639595&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Frequency domain analysis reveals fundamental image patterns difficult to observe in raw pixel values, while avoiding redundant information in original image processing. Although recent remote sensing foundation models (FMs) have made progress in leveraging spatial and spectral information, they have limitations in fully utilizing frequency characteristics that capture hidden features. Existing FMs that incorporate frequency properties often struggle to maintain connections with the original image content, creating a semantic gap that affects downstream performance. To address these challenges, we propose the All-in-One Spectral-Spatial-Frequency Awareness Foundation Model (Alliance), a framework that effectively integrates information across all three domains. Alliance introduces several key innovations: (1) a progressive frequency decoding mechanism inspired by human visual cognition that minimizes multi-domain information gaps while preserving connections between general image information and frequency characteristics, progressively reconstructing from low to mid to high frequencies to extract patterns difficult to observe in raw pixel values; (2) a triple-domain fusion attention module that separately processes amplitude, phase, and spectral-spatial relationships for comprehensive feature integration; and (3) frequency embedding with frequency-aware Cls token initialization and frequency-specific mask token initialization that achieves fine-grained modeling of different frequency band information. Additionally, to evaluate FMs generalizability, we construct the Yellow River dataset, a large-scale multi-temporal collection that introduces challenging cross-domain tasks and establishes more rigorous standards for FMs assessment. Extensive experiments across six downstream tasks demonstrate Alliance&amp;#x27;s superior performance.&lt;/p&gt;</content:encoded></item><item><title>Constituency-Tree-Induced Vision-Language Alignment for Multimodal Large Language Models</title><link>https://doi.org/10.1109/tcsvt.2025.3639574</link><guid>10.1109/tcsvt.2025.3639574</guid><pubDate>Wed, 03 Dec 2025 18:44:30 +0000</pubDate><dc:creator>Yingchen Zhai</dc:creator><dc:creator>Ning Xu</dc:creator><dc:creator>Hongshuo Tian</dc:creator><dc:creator>Bolun Zheng</dc:creator><dc:creator>Chenggang Yan</dc:creator><dc:creator>Jinbo Cao</dc:creator><dc:creator>Rongbao Kang</dc:creator><dc:creator>An-An Liu</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3639574</prism:doi><description>Multimodal large language models (MLLMs) integrate sophisticated large vision models (LVMs) to empower large language models (LLMs) with vision ability to perceive, reason, and interact in vision-language (V-L) tasks, while the modality bridge between two specialists becomes the bottleneck that translates visual signals into linguistic representations. However, most of the existing methods train the modality bridge with coarse-grained image-text pairs, neglecting the structural mapping between V-L semantics that facilitates modality translation from LVMs to LLMs. To mitigate this, we propose a Constituency-Tree-Induced Multimodal Bridging mechanism (CTIMB) that learns the fine-grained connection from LVMs to LLMs by the structural guidance from multi-modal constituency tree. Our approach consists of: 1) the multi-modal constituency-tree parser that jointly exploits the semantic structure of vision and language; 2) the lightweight connector that translates visual signals into linguistic representation and re-arranges them according to the constituency-tree structure; 3) the dynamic construction loss that aids in aligning the semantic structures derived from the tree parser and the connector. The CTIMB can learn the fine-grained mapping between visual and linguistic semantics, seamlessly bridge the LVMs and LLMs to enhance V-L tasks, and is more cost-efficient compared with current methods. Extensive experiments have demonstrated that our method more accurately interprets the visual features, enabling LLMs to conduct downstream tasks more effectively, and achieve superior performance with less training cost.
Published: 2025-12-03T18:44:30+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yingchen Zhai; Ning Xu; Hongshuo Tian; Bolun Zheng; Chenggang Yan; Jinbo Cao; Rongbao Kang; An-An Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3639574"&gt;10.1109/tcsvt.2025.3639574&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal large language models (MLLMs) integrate sophisticated large vision models (LVMs) to empower large language models (LLMs) with vision ability to perceive, reason, and interact in vision-language (V-L) tasks, while the modality bridge between two specialists becomes the bottleneck that translates visual signals into linguistic representations. However, most of the existing methods train the modality bridge with coarse-grained image-text pairs, neglecting the structural mapping between V-L semantics that facilitates modality translation from LVMs to LLMs. To mitigate this, we propose a Constituency-Tree-Induced Multimodal Bridging mechanism (CTIMB) that learns the fine-grained connection from LVMs to LLMs by the structural guidance from multi-modal constituency tree. Our approach consists of: 1) the multi-modal constituency-tree parser that jointly exploits the semantic structure of vision and language; 2) the lightweight connector that translates visual signals into linguistic representation and re-arranges them according to the constituency-tree structure; 3) the dynamic construction loss that aids in aligning the semantic structures derived from the tree parser and the connector. The CTIMB can learn the fine-grained mapping between visual and linguistic semantics, seamlessly bridge the LVMs and LLMs to enhance V-L tasks, and is more cost-efficient compared with current methods. Extensive experiments have demonstrated that our method more accurately interprets the visual features, enabling LLMs to conduct downstream tasks more effectively, and achieve superior performance with less training cost.&lt;/p&gt;</content:encoded></item><item><title>Look, Recite, Then Answer: Enhancing VLM Performance via Self-Generated Knowledge Hints</title><link>https://arxiv.org/abs/2512.00882v3</link><guid>http://arxiv.org/abs/2512.00882v3</guid><pubDate>Sun, 30 Nov 2025 13:04:43 +0000</pubDate><dc:creator>Xisheng Feng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language Models (VLMs) exhibit significant performance plateaus in specialized domains like precision agriculture, primarily due to "Reasoning-Driven Hallucination" where linguistic priors override visual perception. A key bottleneck is the "Modality Gap": visual embeddings fail to reliably activate the fine-grained expert knowledge already encoded in model parameters. We propose "Look, Recite, Then Answer," a parameter-efficient framework that enhances VLMs via self-generated knowledge hints while keeping backbone models frozen. The framework decouples inference into three stages: (1) Look generates objective visual descriptions and candidate sets; (2) Recite employs a lightweight 1.7B router to transform visual cues into targeted queries that trigger candidate-specific parametric knowledge; (3) Answer performs parallel evidence alignment between descriptions and recited knowledge to select the most consistent label. On AgroBench, our method achieves state-of-the-art results, improving Weed Identification accuracy by 23.52% over Qwen2-VL-72B and surpassing GPT-4o without external search overhead. This modular design mitigates hallucinations by transforming passive perception into active, controllable knowledge retrieval
Published: 2025-11-30T13:04:43+00:00
Venue: arXiv
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xisheng Feng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;Vision-Language Models (VLMs) exhibit significant performance plateaus in specialized domains like precision agriculture, primarily due to &amp;quot;Reasoning-Driven Hallucination&amp;quot; where linguistic priors override visual perception. A key bottleneck is the &amp;quot;Modality Gap&amp;quot;: visual embeddings fail to reliably activate the fine-grained expert knowledge already encoded in model parameters. We propose &amp;quot;Look, Recite, Then Answer,&amp;quot; a parameter-efficient framework that enhances VLMs via self-generated knowledge hints while keeping backbone models frozen. The framework decouples inference into three stages: (1) Look generates objective visual descriptions and candidate sets; (2) Recite employs a lightweight 1.7B router to transform visual cues into targeted queries that trigger candidate-specific parametric knowledge; (3) Answer performs parallel evidence alignment between descriptions and recited knowledge to select the most consistent label. On AgroBench, our method achieves state-of-the-art results, improving Weed Identification accuracy by 23.52% over Qwen2-VL-72B and surpassing GPT-4o without external search overhead. This modular design mitigates hallucinations by transforming passive perception into active, controllable knowledge retrieval&lt;/p&gt;</content:encoded></item><item><title>OneThinker: All-in-one Reasoning Model for Image and Video</title><link>https://arxiv.org/abs/2512.03043v2</link><guid>http://arxiv.org/abs/2512.03043v2</guid><pubDate>Tue, 02 Dec 2025 18:59:52 +0000</pubDate><dc:creator>Kaituo Feng</dc:creator><dc:creator>Manyuan Zhang</dc:creator><dc:creator>Hongyu Li</dc:creator><dc:creator>Kaixuan Fan</dc:creator><dc:creator>Shuang Chen</dc:creator><dc:creator>Yilei Jiang</dc:creator><dc:creator>Dian Zheng</dc:creator><dc:creator>Peiwen Sun</dc:creator><dc:creator>Yiyuan Zhang</dc:creator><dc:creator>Haoze Sun</dc:creator><dc:creator>Yan Feng</dc:creator><dc:creator>Peng Pei</dc:creator><dc:creator>Xunliang Cai</dc:creator><dc:creator>Xiangyu Yue</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.
Published: 2025-12-02T18:59:52+00:00
Venue: arXiv
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kaituo Feng; Manyuan Zhang; Hongyu Li; Kaixuan Fan; Shuang Chen; Yilei Jiang; Dian Zheng; Peiwen Sun; Yiyuan Zhang; Haoze Sun; Yan Feng; Peng Pei; Xunliang Cai; Xiangyu Yue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.&lt;/p&gt;</content:encoded></item><item><title>Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling</title><link>https://arxiv.org/abs/2512.01821v1</link><guid>http://arxiv.org/abs/2512.01821v1</guid><pubDate>Mon, 01 Dec 2025 16:01:41 +0000</pubDate><dc:creator>Meng Cao</dc:creator><dc:creator>Haokun Lin</dc:creator><dc:creator>Haoyuan Li</dc:creator><dc:creator>Haoran Tang</dc:creator><dc:creator>Rongtao Xu</dc:creator><dc:creator>Dong An</dc:creator><dc:creator>Xue Liu</dc:creator><dc:creator>Ian Reid</dc:creator><dc:creator>Xiaodan Liang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Spatial reasoning, the ability to understand and interpret the 3D structure of the world, is a critical yet underdeveloped capability in Multimodal Large Language Models (MLLMs). Current methods predominantly rely on verbal descriptive tuning, which suffers from visual illiteracy, i.e., they learn spatial concepts through textual symbols alone, devoid of connection to their visual manifestations. To bridge this gap, this paper introduces MILO, an Implicit spatIaL wOrld modeling paradigm that simulates human-like spatial imagination. MILO integrates a visual generator to provide geometry-aware feedback, thereby implicitly grounding the MLLM's symbolic reasoning in perceptual experience. Complementing this paradigm, we propose RePE (Relative Positional Encoding), a novel encoding scheme that captures relative camera-pose transformations, offering superior performance over absolute coordinate systems. To support the training, we construct GeoGen, a large-scale Geometry-aware Generative dataset with approximately 2,241 videos and 67,827 observation-action-outcome triplets. Experiments demonstrate that our approach significantly enhances spatial reasoning capabilities across multiple baselines and benchmarks, offering a more holistic understanding of 3D space.
Published: 2025-12-01T16:01:41+00:00
Venue: arXiv
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Meng Cao; Haokun Lin; Haoyuan Li; Haoran Tang; Rongtao Xu; Dong An; Xue Liu; Ian Reid; Xiaodan Liang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Spatial reasoning, the ability to understand and interpret the 3D structure of the world, is a critical yet underdeveloped capability in Multimodal Large Language Models (MLLMs). Current methods predominantly rely on verbal descriptive tuning, which suffers from visual illiteracy, i.e., they learn spatial concepts through textual symbols alone, devoid of connection to their visual manifestations. To bridge this gap, this paper introduces MILO, an Implicit spatIaL wOrld modeling paradigm that simulates human-like spatial imagination. MILO integrates a visual generator to provide geometry-aware feedback, thereby implicitly grounding the MLLM&amp;#x27;s symbolic reasoning in perceptual experience. Complementing this paradigm, we propose RePE (Relative Positional Encoding), a novel encoding scheme that captures relative camera-pose transformations, offering superior performance over absolute coordinate systems. To support the training, we construct GeoGen, a large-scale Geometry-aware Generative dataset with approximately 2,241 videos and 67,827 observation-action-outcome triplets. Experiments demonstrate that our approach significantly enhances spatial reasoning capabilities across multiple baselines and benchmarks, offering a more holistic understanding of 3D space.&lt;/p&gt;</content:encoded></item><item><title>Generalizing Vision-Language Models with Dedicated Prompt Guidance</title><link>https://arxiv.org/abs/2512.02421v1</link><guid>http://arxiv.org/abs/2512.02421v1</guid><pubDate>Tue, 02 Dec 2025 05:06:17 +0000</pubDate><dc:creator>Xinyao Li</dc:creator><dc:creator>Yinjie Min</dc:creator><dc:creator>Hongbo Chen</dc:creator><dc:creator>Zhekai Du</dc:creator><dc:creator>Fengling Li</dc:creator><dc:creator>Jingjing Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Fine-tuning large pretrained vision-language models (VLMs) has emerged as a prevalent paradigm for downstream adaptation, yet it faces a critical trade-off between domain specificity and domain generalization (DG) ability. Current methods typically fine-tune a universal model on the entire dataset, which potentially compromises the ability to generalize to unseen domains. To fill this gap, we provide a theoretical understanding of the generalization ability for VLM fine-tuning, which reveals that training multiple parameter-efficient expert models on partitioned source domains leads to better generalization than fine-tuning a universal model. Inspired by this finding, we propose a two-step domain-expert-Guided DG (GuiDG) framework. GuiDG first employs prompt tuning to obtain source domain experts, then introduces a Cross-Modal Attention module to guide the fine-tuning of the vision encoder via adaptive expert integration. To better evaluate few-shot DG, we construct ImageNet-DG from ImageNet and its variants. Extensive experiments on standard DG benchmarks and ImageNet-DG demonstrate that GuiDG improves upon state-of-the-art fine-tuning methods while maintaining efficiency.
Published: 2025-12-02T05:06:17+00:00
Venue: arXiv
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinyao Li; Yinjie Min; Hongbo Chen; Zhekai Du; Fengling Li; Jingjing Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;Fine-tuning large pretrained vision-language models (VLMs) has emerged as a prevalent paradigm for downstream adaptation, yet it faces a critical trade-off between domain specificity and domain generalization (DG) ability. Current methods typically fine-tune a universal model on the entire dataset, which potentially compromises the ability to generalize to unseen domains. To fill this gap, we provide a theoretical understanding of the generalization ability for VLM fine-tuning, which reveals that training multiple parameter-efficient expert models on partitioned source domains leads to better generalization than fine-tuning a universal model. Inspired by this finding, we propose a two-step domain-expert-Guided DG (GuiDG) framework. GuiDG first employs prompt tuning to obtain source domain experts, then introduces a Cross-Modal Attention module to guide the fine-tuning of the vision encoder via adaptive expert integration. To better evaluate few-shot DG, we construct ImageNet-DG from ImageNet and its variants. Extensive experiments on standard DG benchmarks and ImageNet-DG demonstrate that GuiDG improves upon state-of-the-art fine-tuning methods while maintaining efficiency.&lt;/p&gt;</content:encoded></item><item><title>From Sight to Insight: Enhancing Confusable Structure Segmentation via Vision-Language Mutual Prompting</title><link>https://doi.org/10.1109/tmm.2025.3639888</link><guid>10.1109/tmm.2025.3639888</guid><pubDate>Wed, 03 Dec 2025 18:43:18 +0000</pubDate><dc:creator>Yixiang Huang</dc:creator><dc:creator>Yihao Zuo</dc:creator><dc:creator>Mengqiu Xu</dc:creator><dc:creator>Kaixin Chen</dc:creator><dc:creator>Ming Wu</dc:creator><dc:creator>Chuang Zhang</dc:creator><dc:creator>Zhanyu Ma</dc:creator><dc:creator>Jun Guo</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2025.3639888</prism:doi><description>Confusable structure segmentation (CSS) is a type of semantic segmentation applied in remote sensing sea fog detection, medical image segmentation, camouflaged object detection, etc. Structural similarity and visual ambiguity are two critical issues in CSS that pose difficulties in distinguishing foreground objects from the background. Current methods focus primarily on enhancing visual representations and do not often incorporate multimodal information, which leads to performance bottlenecks. Inspired by recent achievements in vision-language models, we propose Vision-Language Mutual Prompting (VLMP), a novel and unified language-guided framework that leverages text prompts to enhance CSS. Specifically, VLMP consists of vision-to-language prompting and language-to-vision prompting, which bidirectionally model the interactions between visual and linguistic features, thereby facilitating cross-modal complementary information flow. To prevent the predominance of one modality over another, we design a feature integration modulator that modulates and balances feature weights for adaptive multimodal fusion. Our framework is designed to be modular and flexible, allowing for integration with any backbone, including CNNs and transformers. We evaluate VLMP with three diverse datasets: SFDD-H8, QaTa-COV19, and CAMO-COD10K. Extensive experiments demonstrate the effectiveness and superiority of the proposed framework over those of state-of-the-art methods across these datasets. This shift from basic sight to deeper insight in CSS through vision-language integration represents a significant advancement in the field.
Published: 2025-12-03T18:43:18+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yixiang Huang; Yihao Zuo; Mengqiu Xu; Kaixin Chen; Ming Wu; Chuang Zhang; Zhanyu Ma; Jun Guo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2025.3639888"&gt;10.1109/tmm.2025.3639888&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;Confusable structure segmentation (CSS) is a type of semantic segmentation applied in remote sensing sea fog detection, medical image segmentation, camouflaged object detection, etc. Structural similarity and visual ambiguity are two critical issues in CSS that pose difficulties in distinguishing foreground objects from the background. Current methods focus primarily on enhancing visual representations and do not often incorporate multimodal information, which leads to performance bottlenecks. Inspired by recent achievements in vision-language models, we propose Vision-Language Mutual Prompting (VLMP), a novel and unified language-guided framework that leverages text prompts to enhance CSS. Specifically, VLMP consists of vision-to-language prompting and language-to-vision prompting, which bidirectionally model the interactions between visual and linguistic features, thereby facilitating cross-modal complementary information flow. To prevent the predominance of one modality over another, we design a feature integration modulator that modulates and balances feature weights for adaptive multimodal fusion. Our framework is designed to be modular and flexible, allowing for integration with any backbone, including CNNs and transformers. We evaluate VLMP with three diverse datasets: SFDD-H8, QaTa-COV19, and CAMO-COD10K. Extensive experiments demonstrate the effectiveness and superiority of the proposed framework over those of state-of-the-art methods across these datasets. This shift from basic sight to deeper insight in CSS through vision-language integration represents a significant advancement in the field.&lt;/p&gt;</content:encoded></item><item><title>Full-Spectrum Prompt Tuning with Sparse MoE for Open-Set Recognition</title><link>https://doi.org/10.1016/j.neunet.2025.108369</link><guid>10.1016/j.neunet.2025.108369</guid><pubDate>Tue, 02 Dec 2025 16:50:34 +0000</pubDate><dc:creator>Yifei Xie</dc:creator><dc:creator>Chuanxing Geng</dc:creator><dc:creator>Yahao Hu</dc:creator><dc:creator>Man Chen</dc:creator><dc:creator>Jun Chen</dc:creator><dc:creator>Zhisong Pan</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2025.108369</prism:doi><description>Recent advances in open-set recognition leveraging vision-language models (VLMs) predominantly focus on improving textual prompts by exploiting (high-level) visual features from the final layer of the VLM image-encoder. While these approaches demonstrate promising performance, they generally neglect the discriminative yet underutilized (low-level) visual details embedded in shallow layers of the image encoder, which also play a critical role in identifying unknown classes. More critically, despite their significance, integrating such low-level part-based features into textual prompts—typically reflecting high-level conceptual information—remains nontrivial due to inherent disparities in feature representation. To address these issues, we innovatively propose Full-Spectrum Prompt Tuning with Sparse Mixture-of-Experts (FSMoE), which leverages the full-spectrum visual features across different layers to enhance the textual prompts. Specifically, two complementary groups of textual tokens are strategically designed, i.e., high-level textual tokens and low-level textual tokens , where the former interacts with high-level visual features, while the latter for the low-level visual counterparts, thus comprehensively enhancing textual prompts through full-spectrum visual features. Furthermore, to mitigate the redundancy in low-level visual details, a sparse Mixture-of-Experts mechanism is introduced to adaptively select and weight the appropriate features from all low-level visual features through collaborative efforts among multiple experts. Besides, a routing consistency contrastive loss is also employed to further enforce intra-class consistency among experts. Extensive experiments demonstrate the effectiveness of our FSMoE.
Published: 2025-12-02T16:50:34+00:00
Venue: Neural Networks
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yifei Xie; Chuanxing Geng; Yahao Hu; Man Chen; Jun Chen; Zhisong Pan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2025.108369"&gt;10.1016/j.neunet.2025.108369&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;Recent advances in open-set recognition leveraging vision-language models (VLMs) predominantly focus on improving textual prompts by exploiting (high-level) visual features from the final layer of the VLM image-encoder. While these approaches demonstrate promising performance, they generally neglect the discriminative yet underutilized (low-level) visual details embedded in shallow layers of the image encoder, which also play a critical role in identifying unknown classes. More critically, despite their significance, integrating such low-level part-based features into textual prompts—typically reflecting high-level conceptual information—remains nontrivial due to inherent disparities in feature representation. To address these issues, we innovatively propose Full-Spectrum Prompt Tuning with Sparse Mixture-of-Experts (FSMoE), which leverages the full-spectrum visual features across different layers to enhance the textual prompts. Specifically, two complementary groups of textual tokens are strategically designed, i.e., high-level textual tokens and low-level textual tokens , where the former interacts with high-level visual features, while the latter for the low-level visual counterparts, thus comprehensively enhancing textual prompts through full-spectrum visual features. Furthermore, to mitigate the redundancy in low-level visual details, a sparse Mixture-of-Experts mechanism is introduced to adaptively select and weight the appropriate features from all low-level visual features through collaborative efforts among multiple experts. Besides, a routing consistency contrastive loss is also employed to further enforce intra-class consistency among experts. Extensive experiments demonstrate the effectiveness of our FSMoE.&lt;/p&gt;</content:encoded></item><item><title>GeoDiT: A Diffusion-based Vision-Language Model for Geospatial Understanding</title><link>https://arxiv.org/abs/2512.02505v1</link><guid>http://arxiv.org/abs/2512.02505v1</guid><pubDate>Tue, 02 Dec 2025 07:59:46 +0000</pubDate><dc:creator>Jiaqi Liu</dc:creator><dc:creator>Ronghao Fu</dc:creator><dc:creator>Haoran Liu</dc:creator><dc:creator>Lang Sun</dc:creator><dc:creator>Bo Yang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Autoregressive models are structurally misaligned with the inherently parallel nature of geospatial understanding, forcing a rigid sequential narrative onto scenes and fundamentally hindering the generation of structured and coherent outputs. We challenge this paradigm by reframing geospatial generation as a parallel refinement process, enabling a holistic, coarse-to-fine synthesis that resolves all semantic elements simultaneously. To operationalize this, we introduce GeoDiT, the first diffusion-based vision-language model tailored for the geospatial domain. Extensive experiments demonstrate that GeoDiT establishes a new state-of-the-art on benchmarks requiring structured, object-centric outputs. It achieves significant gains in image captioning, visual grounding, and multi-object detection, precisely the tasks where autoregressive models falter. Our work validates that aligning the generative process with the data's intrinsic structure is key to unlocking superior performance in complex geospatial analysis.
Published: 2025-12-02T07:59:46+00:00
Venue: arXiv
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaqi Liu; Ronghao Fu; Haoran Liu; Lang Sun; Bo Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;Autoregressive models are structurally misaligned with the inherently parallel nature of geospatial understanding, forcing a rigid sequential narrative onto scenes and fundamentally hindering the generation of structured and coherent outputs. We challenge this paradigm by reframing geospatial generation as a parallel refinement process, enabling a holistic, coarse-to-fine synthesis that resolves all semantic elements simultaneously. To operationalize this, we introduce GeoDiT, the first diffusion-based vision-language model tailored for the geospatial domain. Extensive experiments demonstrate that GeoDiT establishes a new state-of-the-art on benchmarks requiring structured, object-centric outputs. It achieves significant gains in image captioning, visual grounding, and multi-object detection, precisely the tasks where autoregressive models falter. Our work validates that aligning the generative process with the data&amp;#x27;s intrinsic structure is key to unlocking superior performance in complex geospatial analysis.&lt;/p&gt;</content:encoded></item><item><title>CartoMapQA: A Fundamental Benchmark Dataset Evaluating Vision-Language Models on Cartographic Map Understanding</title><link>https://arxiv.org/abs/2512.03558v1</link><guid>http://arxiv.org/abs/2512.03558v1</guid><pubDate>Wed, 03 Dec 2025 08:25:22 +0000</pubDate><dc:creator>Huy Quang Ung</dc:creator><dc:creator>Guillaume Habault</dc:creator><dc:creator>Yasutaka Nishimura</dc:creator><dc:creator>Hao Niu</dc:creator><dc:creator>Roberto Legaspi</dc:creator><dc:creator>Tomoki Oya</dc:creator><dc:creator>Ryoichi Kojima</dc:creator><dc:creator>Masato Taya</dc:creator><dc:creator>Chihiro Ono</dc:creator><dc:creator>Atsunori Minamikawa</dc:creator><dc:creator>Yan Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The rise of Visual-Language Models (LVLMs) has unlocked new possibilities for seamlessly integrating visual and textual information. However, their ability to interpret cartographic maps remains largely unexplored. In this paper, we introduce CartoMapQA, a benchmark specifically designed to evaluate LVLMs' understanding of cartographic maps through question-answering tasks. The dataset includes over 2000 samples, each composed of a cartographic map, a question (with open-ended or multiple-choice answers), and a ground-truth answer. These tasks span key low-, mid- and high-level map interpretation skills, including symbol recognition, embedded information extraction, scale interpretation, and route-based reasoning. Our evaluation of both open-source and proprietary LVLMs reveals persistent challenges: models frequently struggle with map-specific semantics, exhibit limited geospatial reasoning, and are prone to Optical Character Recognition (OCR)-related errors. By isolating these weaknesses, CartoMapQA offers a valuable tool for guiding future improvements in LVLM architectures. Ultimately, it supports the development of models better equipped for real-world applications that depend on robust and reliable map understanding, such as navigation, geographic search, and urban planning. Our source code and data are openly available to the research community at: https://github.com/ungquanghuy-kddi/CartoMapQA.git
Published: 2025-12-03T08:25:22+00:00
Venue: arXiv
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Huy Quang Ung; Guillaume Habault; Yasutaka Nishimura; Hao Niu; Roberto Legaspi; Tomoki Oya; Ryoichi Kojima; Masato Taya; Chihiro Ono; Atsunori Minamikawa; Yan Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;The rise of Visual-Language Models (LVLMs) has unlocked new possibilities for seamlessly integrating visual and textual information. However, their ability to interpret cartographic maps remains largely unexplored. In this paper, we introduce CartoMapQA, a benchmark specifically designed to evaluate LVLMs&amp;#x27; understanding of cartographic maps through question-answering tasks. The dataset includes over 2000 samples, each composed of a cartographic map, a question (with open-ended or multiple-choice answers), and a ground-truth answer. These tasks span key low-, mid- and high-level map interpretation skills, including symbol recognition, embedded information extraction, scale interpretation, and route-based reasoning. Our evaluation of both open-source and proprietary LVLMs reveals persistent challenges: models frequently struggle with map-specific semantics, exhibit limited geospatial reasoning, and are prone to Optical Character Recognition (OCR)-related errors. By isolating these weaknesses, CartoMapQA offers a valuable tool for guiding future improvements in LVLM architectures. Ultimately, it supports the development of models better equipped for real-world applications that depend on robust and reliable map understanding, such as navigation, geographic search, and urban planning. Our source code and data are openly available to the research community at: https://github.com/ungquanghuy-kddi/CartoMapQA.git&lt;/p&gt;</content:encoded></item><item><title>Which is playing a key role on sampling-based large-scale GNNs, sampling or iteration?</title><link>https://doi.org/10.1016/j.neucom.2025.132246</link><guid>10.1016/j.neucom.2025.132246</guid><pubDate>Tue, 02 Dec 2025 00:23:10 +0000</pubDate><dc:creator>Bo Jiang</dc:creator><dc:creator>Lijun Dong</dc:creator><dc:creator>Xiaoyue Peng</dc:creator><dc:creator>Renyao Chen</dc:creator><dc:creator>Chao Liu</dc:creator><dc:creator>Hong Yao</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132246</prism:doi><description>Graph Neural Networks (GNNs) is a set of deep-learning-based methods that model graph data. Due to their excellent performance and powerful explainability, GNNs are widely used for a variety of graph analysis tasks. How to scale GNNs to large-scale graph datasets has been a hot research topic in recent years, and several approaches have been proposed to solve this problem. Recently training GNNs by sampling graphs has become a popular direction. This paper reviews current methods for training GNNs using sampled subgraphs and provides detailed theoretical analysis of these sampling methods. On this basis, this study explored the effectiveness of various sampling algorithms, including methods based on graph structure, structural entropy, information entropy, and iterative sampling training methods. Moreover, for sampled graph training, the paper summarizes a unified framework of three steps, namely subgraph sampling, iterative process, and GNNs module. Experiments show that different sampling methods can achieve similar performance with the same sampling size, while iterative methods can accelerate the training of GNNs. Finally, this paper provides a detailed theoretical explanation of this phenomenon.
Published: 2025-12-02T00:23:10+00:00
Venue: Neurocomputing
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bo Jiang; Lijun Dong; Xiaoyue Peng; Renyao Chen; Chao Liu; Hong Yao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132246"&gt;10.1016/j.neucom.2025.132246&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Graph Neural Networks (GNNs) is a set of deep-learning-based methods that model graph data. Due to their excellent performance and powerful explainability, GNNs are widely used for a variety of graph analysis tasks. How to scale GNNs to large-scale graph datasets has been a hot research topic in recent years, and several approaches have been proposed to solve this problem. Recently training GNNs by sampling graphs has become a popular direction. This paper reviews current methods for training GNNs using sampled subgraphs and provides detailed theoretical analysis of these sampling methods. On this basis, this study explored the effectiveness of various sampling algorithms, including methods based on graph structure, structural entropy, information entropy, and iterative sampling training methods. Moreover, for sampled graph training, the paper summarizes a unified framework of three steps, namely subgraph sampling, iterative process, and GNNs module. Experiments show that different sampling methods can achieve similar performance with the same sampling size, while iterative methods can accelerate the training of GNNs. Finally, this paper provides a detailed theoretical explanation of this phenomenon.&lt;/p&gt;</content:encoded></item><item><title>DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images</title><link>https://arxiv.org/abs/2512.03004v1</link><guid>http://arxiv.org/abs/2512.03004v1</guid><pubDate>Tue, 02 Dec 2025 18:29:18 +0000</pubDate><dc:creator>Xiaoxue Chen</dc:creator><dc:creator>Ziyi Xiong</dc:creator><dc:creator>Yuantao Chen</dc:creator><dc:creator>Gen Li</dc:creator><dc:creator>Nan Wang</dc:creator><dc:creator>Hongcheng Luo</dc:creator><dc:creator>Long Chen</dc:creator><dc:creator>Haiyang Sun</dc:creator><dc:creator>Bing Wang</dc:creator><dc:creator>Guang Chen</dc:creator><dc:creator>Hangjun Ye</dc:creator><dc:creator>Hongyang Li</dc:creator><dc:creator>Ya-Qin Zhang</dc:creator><dc:creator>Hao Zhao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.
Published: 2025-12-02T18:29:18+00:00
Venue: arXiv
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoxue Chen; Ziyi Xiong; Yuantao Chen; Gen Li; Nan Wang; Hongcheng Luo; Long Chen; Haiyang Sun; Bing Wang; Guang Chen; Hangjun Ye; Hongyang Li; Ya-Qin Zhang; Hao Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.&lt;/p&gt;</content:encoded></item><item><title>See, Think, Learn: A Self-Taught Multimodal Reasoner</title><link>https://arxiv.org/abs/2512.02456v1</link><guid>http://arxiv.org/abs/2512.02456v1</guid><pubDate>Tue, 02 Dec 2025 06:30:10 +0000</pubDate><dc:creator>Sourabh Sharma</dc:creator><dc:creator>Sonam Gupta</dc:creator><dc:creator>Sadbhawna</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language Models (VLMs) have achieved remarkable progress in integrating visual perception with language understanding. However, effective multimodal reasoning requires both accurate perception and robust reasoning, and weakness in either limits the performance of VLMs. Prior efforts to enhance reasoning often depend on high-quality chain-of-thought (CoT) data, obtained via labor-intensive human annotations, costly proprietary models, or self-training methods that overlook perception. To address these limitations, we propose a simple yet effective self-training framework called See-Think-Learn (STL). At its core, STL introduces a structured reasoning template that encourages the model to see before thinking, first extracting visual attributes in textual form, then using them to guide reasoning. The framework jointly improves perception and reasoning by having the model generate and learn from its own structured rationales in a self-training loop. Furthermore, we augment the training data with negative rationales, i.e. explanations that justify why certain answer choices are incorrect, to enhance the model's ability to distinguish between correct and misleading responses. This fosters more discriminative and robust learning. Experiments across diverse domains show that STL consistently outperforms baselines trained directly only on answers or self-generated reasoning, while qualitative analysis confirms the high quality of its rationales. STL thus provides a cost-effective solution to enhance multimodal reasoning ability of VLMs.
Published: 2025-12-02T06:30:10+00:00
Venue: arXiv
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sourabh Sharma; Sonam Gupta; Sadbhawna&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Vision-Language Models (VLMs) have achieved remarkable progress in integrating visual perception with language understanding. However, effective multimodal reasoning requires both accurate perception and robust reasoning, and weakness in either limits the performance of VLMs. Prior efforts to enhance reasoning often depend on high-quality chain-of-thought (CoT) data, obtained via labor-intensive human annotations, costly proprietary models, or self-training methods that overlook perception. To address these limitations, we propose a simple yet effective self-training framework called See-Think-Learn (STL). At its core, STL introduces a structured reasoning template that encourages the model to see before thinking, first extracting visual attributes in textual form, then using them to guide reasoning. The framework jointly improves perception and reasoning by having the model generate and learn from its own structured rationales in a self-training loop. Furthermore, we augment the training data with negative rationales, i.e. explanations that justify why certain answer choices are incorrect, to enhance the model&amp;#x27;s ability to distinguish between correct and misleading responses. This fosters more discriminative and robust learning. Experiments across diverse domains show that STL consistently outperforms baselines trained directly only on answers or self-generated reasoning, while qualitative analysis confirms the high quality of its rationales. STL thus provides a cost-effective solution to enhance multimodal reasoning ability of VLMs.&lt;/p&gt;</content:encoded></item><item><title>ComCon: Complementary-Contradictory Regularization for Multimodal Knowledge Graph Completion</title><link>https://doi.org/10.1016/j.inffus.2025.104016</link><guid>10.1016/j.inffus.2025.104016</guid><pubDate>Wed, 03 Dec 2025 17:16:36 +0000</pubDate><dc:creator>Jie Chen</dc:creator><dc:creator>Wuyang Zhang</dc:creator><dc:creator>Shu Zhao</dc:creator><dc:creator>Yunxia Yin</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104016</prism:doi><description>Multimodal Knowledge Graphs (MMKGs) extend traditional knowledge graphs by incorporating multimodal data, enriching the representation of entities from multiple perspectives. Most MMKGs are inherently incomplete, thus requiring Multimodal Knowledge Graph Completion (MMKGC) for missing triple prediction. MMKGC differs from traditional KGC in the integration of diverse modalities, such as textual and visual modalities, for a more comprehensive representation. However, inherent cross-modal semantic discrepancies in unified representations lead to misalignment and accuracy degradation. To resolve this, we propose ComCon, an effective regularization model. Our key insight is to explicitly decompose the unified representation into two distinct components: complementary features, where textual and visual modalities provide mutually reinforcing information to enhance the representation, and contradictory features, which capture the conflicting signals and inconsistencies between modalities. By regularizing the interactions between these features, ComCon effectively mitigates semantic discrepancies and enhances representation learning. Furthermore, we implement a Weighted Negative Sampling (WNS) to discern potential false negatives and diminish their impact by minimizing the score margin. Comprehensive experiments on the DB15K and MKG-W datasets demonstrate that our ComCon outperforms state-of-the-art baselines. Our code and datasets are released at https://github.com/wyZhang016/ComCon .
Published: 2025-12-03T17:16:36+00:00
Venue: Information Fusion
Score: 0.773 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jie Chen; Wuyang Zhang; Shu Zhao; Yunxia Yin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104016"&gt;10.1016/j.inffus.2025.104016&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal Knowledge Graphs (MMKGs) extend traditional knowledge graphs by incorporating multimodal data, enriching the representation of entities from multiple perspectives. Most MMKGs are inherently incomplete, thus requiring Multimodal Knowledge Graph Completion (MMKGC) for missing triple prediction. MMKGC differs from traditional KGC in the integration of diverse modalities, such as textual and visual modalities, for a more comprehensive representation. However, inherent cross-modal semantic discrepancies in unified representations lead to misalignment and accuracy degradation. To resolve this, we propose ComCon, an effective regularization model. Our key insight is to explicitly decompose the unified representation into two distinct components: complementary features, where textual and visual modalities provide mutually reinforcing information to enhance the representation, and contradictory features, which capture the conflicting signals and inconsistencies between modalities. By regularizing the interactions between these features, ComCon effectively mitigates semantic discrepancies and enhances representation learning. Furthermore, we implement a Weighted Negative Sampling (WNS) to discern potential false negatives and diminish their impact by minimizing the score margin. Comprehensive experiments on the DB15K and MKG-W datasets demonstrate that our ComCon outperforms state-of-the-art baselines. Our code and datasets are released at https://github.com/wyZhang016/ComCon .&lt;/p&gt;</content:encoded></item><item><title>SLSM-Net: Sparse LiDAR Point Clouds Supervised Stereo Matching</title><link>https://doi.org/10.1109/tmm.2025.3639911</link><guid>10.1109/tmm.2025.3639911</guid><pubDate>Wed, 03 Dec 2025 18:43:18 +0000</pubDate><dc:creator>Ze Zong</dc:creator><dc:creator>Cheng Wu</dc:creator><dc:creator>Jie Xie</dc:creator><dc:creator>Jin Zhang</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2025.3639911</prism:doi><description>Deep learning has achieved significant success in stereo matching, with its training process often supervised by LiDAR measurements. However, the sparsity of real-world LiDAR data limits the ability of deep models to extract effective features from stereo images. To address this issue, a novel deep learning-based framework called sparse LiDAR point cloud supervised stereo matching (SLSM-Net) is proposed. Specifically, dense reconstruction of sparse single-frame point clouds is first designed to avoid the error introduction with the mergence of multi-frame point clouds. To effectively densify point clouds of objects in local areas, stereo images are utilized as supervision information to train the deep models. Furthermore, a coarse-to-fine structure of the deep model is designed for stereo matching. A self-supervised learning strategy, which employs a photometric consistency constraint, is second proposed along with fully supervised learning to obtain dense and precise supervision information. This stage generates coarse disparity maps from stereo images. Finally, to fully leverage the complementary characteristics of LiDAR and stereo cameras, multi-scale feature fusion of point clouds and stereo images is performed by a residual block, where the feature maps of point clouds are derived from the densification reconstruction. This stage refines the results. Experimental results indicate that SLSM-Net outperforms current state-of-the-art methods, demonstrating superior performance in stereo matching.
Published: 2025-12-03T18:43:18+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.772 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ze Zong; Cheng Wu; Jie Xie; Jin Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2025.3639911"&gt;10.1109/tmm.2025.3639911&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (must_read)&lt;/p&gt;
&lt;p&gt;Deep learning has achieved significant success in stereo matching, with its training process often supervised by LiDAR measurements. However, the sparsity of real-world LiDAR data limits the ability of deep models to extract effective features from stereo images. To address this issue, a novel deep learning-based framework called sparse LiDAR point cloud supervised stereo matching (SLSM-Net) is proposed. Specifically, dense reconstruction of sparse single-frame point clouds is first designed to avoid the error introduction with the mergence of multi-frame point clouds. To effectively densify point clouds of objects in local areas, stereo images are utilized as supervision information to train the deep models. Furthermore, a coarse-to-fine structure of the deep model is designed for stereo matching. A self-supervised learning strategy, which employs a photometric consistency constraint, is second proposed along with fully supervised learning to obtain dense and precise supervision information. This stage generates coarse disparity maps from stereo images. Finally, to fully leverage the complementary characteristics of LiDAR and stereo cameras, multi-scale feature fusion of point clouds and stereo images is performed by a residual block, where the feature maps of point clouds are derived from the densification reconstruction. This stage refines the results. Experimental results indicate that SLSM-Net outperforms current state-of-the-art methods, demonstrating superior performance in stereo matching.&lt;/p&gt;</content:encoded></item><item><title>GeoBridge: A Semantic-Anchored Multi-View Foundation Model Bridging Images and Text for Geo-Localization</title><link>https://arxiv.org/abs/2512.02697v1</link><guid>http://arxiv.org/abs/2512.02697v1</guid><pubDate>Tue, 02 Dec 2025 12:28:22 +0000</pubDate><dc:creator>Zixuan Song</dc:creator><dc:creator>Jing Zhang</dc:creator><dc:creator>Di Wang</dc:creator><dc:creator>Zidie Zhou</dc:creator><dc:creator>Wenbin Liu</dc:creator><dc:creator>Haonan Guo</dc:creator><dc:creator>En Wang</dc:creator><dc:creator>Bo Du</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image. However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable. It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image). To address these challenges, we propose GeoBridge, a foundation model that performs bidirectional matching across views and supports language-to-image retrieval. Going beyond traditional satellite-centric formulations, GeoBridge builds on a novel semantic-anchor mechanism that bridges multi-view features through textual descriptions for robust, flexible localization. In support of this task, we construct GeoLoc, the first large-scale, cross-modal, and multi-view aligned dataset comprising over 50,000 pairs of drone, street-view panorama, and satellite images as well as their textual descriptions, collected from 36 countries, ensuring both geographic and semantic alignment. We performed broad evaluations across multiple tasks. Experiments confirm that GeoLoc pre-training markedly improves geo-location accuracy for GeoBridge while promoting cross-domain generalization and cross-modal knowledge transfer. The dataset, source code, and pretrained models were released at https://github.com/MiliLab/GeoBridge.
Published: 2025-12-02T12:28:22+00:00
Venue: arXiv
Score: 0.772 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zixuan Song; Jing Zhang; Di Wang; Zidie Zhou; Wenbin Liu; Haonan Guo; En Wang; Bo Du&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (must_read)&lt;/p&gt;
&lt;p&gt;Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image. However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable. It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image). To address these challenges, we propose GeoBridge, a foundation model that performs bidirectional matching across views and supports language-to-image retrieval. Going beyond traditional satellite-centric formulations, GeoBridge builds on a novel semantic-anchor mechanism that bridges multi-view features through textual descriptions for robust, flexible localization. In support of this task, we construct GeoLoc, the first large-scale, cross-modal, and multi-view aligned dataset comprising over 50,000 pairs of drone, street-view panorama, and satellite images as well as their textual descriptions, collected from 36 countries, ensuring both geographic and semantic alignment. We performed broad evaluations across multiple tasks. Experiments confirm that GeoLoc pre-training markedly improves geo-location accuracy for GeoBridge while promoting cross-domain generalization and cross-modal knowledge transfer. The dataset, source code, and pretrained models were released at https://github.com/MiliLab/GeoBridge.&lt;/p&gt;</content:encoded></item><item><title>S2ML: Spatio-Spectral Mutual Learning for Depth Completion</title><link>https://doi.org/10.1109/tmm.2025.3639949</link><guid>10.1109/tmm.2025.3639949</guid><pubDate>Wed, 03 Dec 2025 18:43:18 +0000</pubDate><dc:creator>Zihui Zhao</dc:creator><dc:creator>Yifei Zhang</dc:creator><dc:creator>Zheng Wang</dc:creator><dc:creator>Yang Li</dc:creator><dc:creator>Kui Jiang</dc:creator><dc:creator>Zihan Geng</dc:creator><dc:creator>Chia-Wen Lin</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2025.3639949</prism:doi><description>The raw depth images captured by RGB-D cameras using Time-of-Flight (TOF) or structured light often suffer from incomplete depth values due to weak reflections, boundary shadows, and artifacts, which limit their applications in downstream vision tasks. Existing methods address this problem through depth completion in the image domain, but they overlook the physical characteristics of raw depth images. It has been observed that the presence of invalid depth areas alters the frequency distribution pattern. In this work, we propose a Spatio-Spectral Mutual Learning framework (S2ML) to harmonize the advantages of both spatial and frequency domains for depth completion. Specifically, we consider the distinct properties of amplitude and phase spectra and devise a dedicated spectral fusion module. Meanwhile, the local and global correlations between spatial-domain and frequency-domain features are calculated in a unified embedding space. The gradual mutual representation and refinement encourage the network to fully explore complementary physical characteristics and priors for more accurate depth completion. Extensive experiments demonstrate the effectiveness of our proposed S2ML method, outperforming the state-of-the-art method CFormer by 0.828 dB and 0.834 dB on the NYU-Depth V2 and SUN RGB-D datasets, respectively.
Published: 2025-12-03T18:43:18+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.771 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zihui Zhao; Yifei Zhang; Zheng Wang; Yang Li; Kui Jiang; Zihan Geng; Chia-Wen Lin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2025.3639949"&gt;10.1109/tmm.2025.3639949&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (must_read)&lt;/p&gt;
&lt;p&gt;The raw depth images captured by RGB-D cameras using Time-of-Flight (TOF) or structured light often suffer from incomplete depth values due to weak reflections, boundary shadows, and artifacts, which limit their applications in downstream vision tasks. Existing methods address this problem through depth completion in the image domain, but they overlook the physical characteristics of raw depth images. It has been observed that the presence of invalid depth areas alters the frequency distribution pattern. In this work, we propose a Spatio-Spectral Mutual Learning framework (S2ML) to harmonize the advantages of both spatial and frequency domains for depth completion. Specifically, we consider the distinct properties of amplitude and phase spectra and devise a dedicated spectral fusion module. Meanwhile, the local and global correlations between spatial-domain and frequency-domain features are calculated in a unified embedding space. The gradual mutual representation and refinement encourage the network to fully explore complementary physical characteristics and priors for more accurate depth completion. Extensive experiments demonstrate the effectiveness of our proposed S2ML method, outperforming the state-of-the-art method CFormer by 0.828 dB and 0.834 dB on the NYU-Depth V2 and SUN RGB-D datasets, respectively.&lt;/p&gt;</content:encoded></item><item><title>Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles</title><link>https://arxiv.org/abs/2512.03454v1</link><guid>http://arxiv.org/abs/2512.03454v1</guid><pubDate>Wed, 03 Dec 2025 05:14:16 +0000</pubDate><dc:creator>Haicheng Liao</dc:creator><dc:creator>Huanming Shen</dc:creator><dc:creator>Bonan Wang</dc:creator><dc:creator>Yongkang Li</dc:creator><dc:creator>Yihong Tang</dc:creator><dc:creator>Chengyue Wang</dc:creator><dc:creator>Dingyi Zhuang</dc:creator><dc:creator>Kehua Chen</dc:creator><dc:creator>Hai Yang</dc:creator><dc:creator>Chengzhong Xu</dc:creator><dc:creator>Zhenning Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.
Published: 2025-12-03T05:14:16+00:00
Venue: arXiv
Score: 0.770 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haicheng Liao; Huanming Shen; Bonan Wang; Yongkang Li; Yihong Tang; Chengyue Wang; Dingyi Zhuang; Kehua Chen; Hai Yang; Chengzhong Xu; Zhenning Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (must_read)&lt;/p&gt;
&lt;p&gt;Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.&lt;/p&gt;</content:encoded></item><item><title>Structural-Spectral Graph Convolution with Evidential Edge Learning for Hyperspectral Image Clustering</title><link>https://doi.org/10.1109/tip.2025.3636098</link><guid>10.1109/tip.2025.3636098</guid><pubDate>Tue, 02 Dec 2025 18:50:39 +0000</pubDate><dc:creator>Jianhan Qi</dc:creator><dc:creator>Yuheng Jia</dc:creator><dc:creator>Hui Liu</dc:creator><dc:creator>Junhui Hou</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3636098</prism:doi><description>Hyperspectral image (HSI) clustering groups pixels into clusters without labeled data, which is an important yet challenging task. For large-scale HSIs, most methods rely on superpixel segmentation and perform superpixel-level clustering based on graph neural networks (GNNs). However, existing GNNs cannot fully exploit the spectral information of the input HSI, and the inaccurate superpixel topological graph may lead to the confusion of different class semantics during information aggregation. To address these challenges, we first propose a structural-spectral graph convolutional operator (SSGCO) tailored for graph-structured HSI superpixels to improve their representation quality through the co-extraction of spatial and spectral features. Second, we propose an evidence-guided adaptive edge learning (EGAEL) module that adaptively predicts and refines edge weights in the superpixel topological graph. We integrate the proposed method into a contrastive learning framework to achieve clustering, where representation learning and clustering are simultaneously conducted. Experiments demonstrate that the proposed method improves clustering accuracy by 2.61%, 6.06%, 4.96% and 3.15% over the best compared methods on four HSI datasets. Our code is available at https://github.com/jhqi/SSGCO-EGAEL.
Published: 2025-12-02T18:50:39+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.769 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianhan Qi; Yuheng Jia; Hui Liu; Junhui Hou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3636098"&gt;10.1109/tip.2025.3636098&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.769 (must_read)&lt;/p&gt;
&lt;p&gt;Hyperspectral image (HSI) clustering groups pixels into clusters without labeled data, which is an important yet challenging task. For large-scale HSIs, most methods rely on superpixel segmentation and perform superpixel-level clustering based on graph neural networks (GNNs). However, existing GNNs cannot fully exploit the spectral information of the input HSI, and the inaccurate superpixel topological graph may lead to the confusion of different class semantics during information aggregation. To address these challenges, we first propose a structural-spectral graph convolutional operator (SSGCO) tailored for graph-structured HSI superpixels to improve their representation quality through the co-extraction of spatial and spectral features. Second, we propose an evidence-guided adaptive edge learning (EGAEL) module that adaptively predicts and refines edge weights in the superpixel topological graph. We integrate the proposed method into a contrastive learning framework to achieve clustering, where representation learning and clustering are simultaneously conducted. Experiments demonstrate that the proposed method improves clustering accuracy by 2.61%, 6.06%, 4.96% and 3.15% over the best compared methods on four HSI datasets. Our code is available at https://github.com/jhqi/SSGCO-EGAEL.&lt;/p&gt;</content:encoded></item><item><title>CrossHypergraph: Consistent High-order Semantic Network for Few-shot Image Classification</title><link>https://doi.org/10.1109/tmm.2025.3639903</link><guid>10.1109/tmm.2025.3639903</guid><pubDate>Wed, 03 Dec 2025 18:43:18 +0000</pubDate><dc:creator>Yucheng Zhang</dc:creator><dc:creator>Hao Wang</dc:creator><dc:creator>Shuo Zhang</dc:creator><dc:creator>Biao Leng</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2025.3639903</prism:doi><description>Few-shot classification is a challenging task that recognizes novel classes by learning from few training instances. Metric-based models are currently the most effective solutions for few-shot classification. In these models, patch feature distances between query instances and support classes are calculated to achieve classification. However, it is difficult for patch-based methods to mine semantic information of support and query instances, leading to inaccurate feature similarity measures. To address these problems, we propose to construct CrossHypergraph based on hypergraph modeling. Specifically, we first align the local prototype vertices of support and query instances to model consistent hypergraph structures. Then a vertex-hyperedge-vertex-based interactive feature updating mechanism is designed to generate CrossHypergraph representation with consistent high-order semantic information for support and query instances. Based on the CrossHypergraph, we propose a consistent high-order semantic network, in which the high-order semantic-based weighted metric strategy is designed to achieve accurate classification. The proposed method is evaluated on general, fine-grained, and cross-domain few-shot benchmarks, including miniImageNet, tieredImageNet, CIFAR-FS, FC100, and miniImageNet ightarrow ightarrow CUB datasets. Experimental results show that our CrossHypergraph-based few-shot classifier generates consistent high-order semantic features, and achieves state-of-the-art performance on both 1-shot and 5-shot tasks.
Published: 2025-12-03T18:43:18+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.768 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yucheng Zhang; Hao Wang; Shuo Zhang; Biao Leng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2025.3639903"&gt;10.1109/tmm.2025.3639903&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.768 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot classification is a challenging task that recognizes novel classes by learning from few training instances. Metric-based models are currently the most effective solutions for few-shot classification. In these models, patch feature distances between query instances and support classes are calculated to achieve classification. However, it is difficult for patch-based methods to mine semantic information of support and query instances, leading to inaccurate feature similarity measures. To address these problems, we propose to construct CrossHypergraph based on hypergraph modeling. Specifically, we first align the local prototype vertices of support and query instances to model consistent hypergraph structures. Then a vertex-hyperedge-vertex-based interactive feature updating mechanism is designed to generate CrossHypergraph representation with consistent high-order semantic information for support and query instances. Based on the CrossHypergraph, we propose a consistent high-order semantic network, in which the high-order semantic-based weighted metric strategy is designed to achieve accurate classification. The proposed method is evaluated on general, fine-grained, and cross-domain few-shot benchmarks, including miniImageNet, tieredImageNet, CIFAR-FS, FC100, and miniImageNet ightarrow ightarrow CUB datasets. Experimental results show that our CrossHypergraph-based few-shot classifier generates consistent high-order semantic features, and achieves state-of-the-art performance on both 1-shot and 5-shot tasks.&lt;/p&gt;</content:encoded></item><item><title>Triple-View Knowledge Distillation for Semi-Supervised Semantic Segmentation</title><link>https://doi.org/10.1109/tmm.2025.3639923</link><guid>10.1109/tmm.2025.3639923</guid><pubDate>Wed, 03 Dec 2025 18:43:18 +0000</pubDate><dc:creator>Ping Li</dc:creator><dc:creator>Junjie Chen</dc:creator><dc:creator>Li Yuan</dc:creator><dc:creator>Xianghua Xu</dc:creator><dc:creator>Mingli Song</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2025.3639923</prism:doi><description>To alleviate the expensive human labeling problem, semi-supervised semantic segmentation utilizes a few labeled images along with an abundance of unlabeled images to predict the pixel-level label maps with the same size. Previous methods often rely on co-training with two convolutional networks with the same architecture but different initialization, which fails to capture sufficiently diverse features. This limitation motivates us to employ tri-training and design a triple-view encoder to utilize encoders with different architectures to derive diverse features, while leveraging knowledge distillation to capture complementary semantics among these encoders. Moreover, existing approaches simply concatenate features from both encoder and decoder, and the simple concatenation requires a large memory cost. This inspires us to present a dual-frequency decoder that selects those important features by projecting the spatial-domain features into the frequency domain, where a dual-frequency channel attention mechanism is applied to evaluate the feature importance. Therefore, we propose a Triple-view Knowledge Distillation framework, termed TriKD, for semi-supervised semantic segmentation. It comprises the triple-view encoder and the dual-frequency decoder. Extensive experiments conducted on two benchmarks, i.e., Pascal VOC 2012 and Cityscapes, validate the superiority of our method, achieving a satisfying tradeoff between precision and inference speed. Our code is available at GitHub.
Published: 2025-12-03T18:43:18+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.766 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ping Li; Junjie Chen; Li Yuan; Xianghua Xu; Mingli Song&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2025.3639923"&gt;10.1109/tmm.2025.3639923&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.766 (must_read)&lt;/p&gt;
&lt;p&gt;To alleviate the expensive human labeling problem, semi-supervised semantic segmentation utilizes a few labeled images along with an abundance of unlabeled images to predict the pixel-level label maps with the same size. Previous methods often rely on co-training with two convolutional networks with the same architecture but different initialization, which fails to capture sufficiently diverse features. This limitation motivates us to employ tri-training and design a triple-view encoder to utilize encoders with different architectures to derive diverse features, while leveraging knowledge distillation to capture complementary semantics among these encoders. Moreover, existing approaches simply concatenate features from both encoder and decoder, and the simple concatenation requires a large memory cost. This inspires us to present a dual-frequency decoder that selects those important features by projecting the spatial-domain features into the frequency domain, where a dual-frequency channel attention mechanism is applied to evaluate the feature importance. Therefore, we propose a Triple-view Knowledge Distillation framework, termed TriKD, for semi-supervised semantic segmentation. It comprises the triple-view encoder and the dual-frequency decoder. Extensive experiments conducted on two benchmarks, i.e., Pascal VOC 2012 and Cityscapes, validate the superiority of our method, achieving a satisfying tradeoff between precision and inference speed. Our code is available at GitHub.&lt;/p&gt;</content:encoded></item><item><title>Artemis: Structured Visual Reasoning for Perception Policy Learning</title><link>https://arxiv.org/abs/2512.01988v1</link><guid>http://arxiv.org/abs/2512.01988v1</guid><pubDate>Mon, 01 Dec 2025 18:45:30 +0000</pubDate><dc:creator>Wei Tang</dc:creator><dc:creator>Yanpeng Sun</dc:creator><dc:creator>Shan Zhang</dc:creator><dc:creator>Xiaofan Li</dc:creator><dc:creator>Piotr Koniusz</dc:creator><dc:creator>Wei Li</dc:creator><dc:creator>Na Zhao</dc:creator><dc:creator>Zechao Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent reinforcement-learning frameworks for visual perception policy have begun to incorporate intermediate reasoning chains expressed in natural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains perform semantic reasoning in an unstructured linguistic space, visual perception requires reasoning in a spatial and object-centric space. In response, we introduce Artemis, a perception-policy learning framework that performs structured proposal-based reasoning, where each intermediate step is represented as a (label, bounding-box) pair capturing a verifiable visual state. This design enables explicit tracking of intermediate states, direct supervision for proposal quality, and avoids ambiguity introduced by language-based reasoning. Artemis is built on Qwen2.5-VL-3B, achieves strong performance on grounding and detection task and exhibits substantial generalization to counting and geometric-perception tasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on general MLLM benchmarks, illustrating that spatially grounded reasoning provides a principled route toward scalable and general perception policies.
Published: 2025-12-01T18:45:30+00:00
Venue: arXiv
Score: 0.766 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wei Tang; Yanpeng Sun; Shan Zhang; Xiaofan Li; Piotr Koniusz; Wei Li; Na Zhao; Zechao Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.766 (must_read)&lt;/p&gt;
&lt;p&gt;Recent reinforcement-learning frameworks for visual perception policy have begun to incorporate intermediate reasoning chains expressed in natural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains perform semantic reasoning in an unstructured linguistic space, visual perception requires reasoning in a spatial and object-centric space. In response, we introduce Artemis, a perception-policy learning framework that performs structured proposal-based reasoning, where each intermediate step is represented as a (label, bounding-box) pair capturing a verifiable visual state. This design enables explicit tracking of intermediate states, direct supervision for proposal quality, and avoids ambiguity introduced by language-based reasoning. Artemis is built on Qwen2.5-VL-3B, achieves strong performance on grounding and detection task and exhibits substantial generalization to counting and geometric-perception tasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on general MLLM benchmarks, illustrating that spatially grounded reasoning provides a principled route toward scalable and general perception policies.&lt;/p&gt;</content:encoded></item><item><title>Prompt Learning with Knowledge Regularization for Pre-trained Vision-Language Models</title><link>https://doi.org/10.1109/tmm.2025.3639886</link><guid>10.1109/tmm.2025.3639886</guid><pubDate>Wed, 03 Dec 2025 18:43:18 +0000</pubDate><dc:creator>Boyang Guo</dc:creator><dc:creator>Liang Li</dc:creator><dc:creator>Jiehua Zhang</dc:creator><dc:creator>Yaoqi Sun</dc:creator><dc:creator>Chenggang Yan</dc:creator><dc:creator>Xichun Sheng</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2025.3639886</prism:doi><description>Prompt learning is an effective way to adapt pre-trained models to downstream tasks by training a small number of additional learnable prompts. Recent studies address several early challenges by combining generalized knowledge from frozen pre-trained VL models with task-specific knowledge from training data as guidance for prompt learning. However, existing methods still struggle with the generalization-adaptation (GA) trade-off dilemma: excessive reliance on generalized knowledge hinders adaptation to downstream tasks, while overemphasis on task-specific knowledge undermines the inherent generalization capabilities of pre-trained models. To address this issue, we propose a novel prompt learning method called Prompt Learning with Knowledge Regularization (PLKR). PLKR effectively mitigates the GA trade-off dilemma by offering greater flexibility in adapting to task-specific knowledge while minimizing the disruption of pre-trained knowledge. Specifically, we propose category-invariant and topology-invariant knowledge regularization to preserve generalized knowledge: the former enhances category-level discriminative capabilities while allowing flexible task-specific learning, and the latter maintains global topological stability during adaptation to new tasks. Through the proposed regularization, PLKR improves the performance on both base and new tasks. We evaluate the effectiveness of our approach on four representative tasks over 11 datasets. Experimental results show our method outperforms existing SOTA methods by a large margin.
Published: 2025-12-03T18:43:18+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.766 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Boyang Guo; Liang Li; Jiehua Zhang; Yaoqi Sun; Chenggang Yan; Xichun Sheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2025.3639886"&gt;10.1109/tmm.2025.3639886&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.766 (must_read)&lt;/p&gt;
&lt;p&gt;Prompt learning is an effective way to adapt pre-trained models to downstream tasks by training a small number of additional learnable prompts. Recent studies address several early challenges by combining generalized knowledge from frozen pre-trained VL models with task-specific knowledge from training data as guidance for prompt learning. However, existing methods still struggle with the generalization-adaptation (GA) trade-off dilemma: excessive reliance on generalized knowledge hinders adaptation to downstream tasks, while overemphasis on task-specific knowledge undermines the inherent generalization capabilities of pre-trained models. To address this issue, we propose a novel prompt learning method called Prompt Learning with Knowledge Regularization (PLKR). PLKR effectively mitigates the GA trade-off dilemma by offering greater flexibility in adapting to task-specific knowledge while minimizing the disruption of pre-trained knowledge. Specifically, we propose category-invariant and topology-invariant knowledge regularization to preserve generalized knowledge: the former enhances category-level discriminative capabilities while allowing flexible task-specific learning, and the latter maintains global topological stability during adaptation to new tasks. Through the proposed regularization, PLKR improves the performance on both base and new tasks. We evaluate the effectiveness of our approach on four representative tasks over 11 datasets. Experimental results show our method outperforms existing SOTA methods by a large margin.&lt;/p&gt;</content:encoded></item><item><title>S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance</title><link>https://arxiv.org/abs/2512.01223v1</link><guid>http://arxiv.org/abs/2512.01223v1</guid><pubDate>Mon, 01 Dec 2025 03:08:34 +0000</pubDate><dc:creator>Beining Xu</dc:creator><dc:creator>Siting Zhu</dc:creator><dc:creator>Zhao Jin</dc:creator><dc:creator>Junxian Li</dc:creator><dc:creator>Hesheng Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>3D Visual Grounding (3DVG) focuses on locating objects in 3D scenes based on natural language descriptions, serving as a fundamental task for embodied AI and robotics. Recent advances in Multi-modal Large Language Models (MLLMs) have motivated research into extending them to 3DVG. However, MLLMs primarily process 2D visual inputs and struggle with understanding 3D spatial structure of scenes solely from these limited perspectives. Existing methods mainly utilize viewpoint-dependent rendering of reconstructed point clouds to provide explicit structural guidance for MLLMs in 3DVG tasks, leading to inefficiency and limited spatial reasoning. To address this issue, we propose S$^2$-MLLM, an efficient framework that enhances spatial reasoning in MLLMs through implicit spatial reasoning. We introduce a spatial guidance strategy that leverages the structure awareness of feed-forward 3D reconstruction. By acquiring 3D structural understanding during training, our model can implicitly reason about 3D scenes without relying on inefficient point cloud reconstruction. Moreover, we propose a structure-enhanced module (SE), which first employs intra-view and inter-view attention mechanisms to capture dependencies within views and correspondences across views. The module further integrates multi-level position encoding to associate visual representations with spatial positions and viewpoint information, enabling more accurate structural understanding. Extensive experiments demonstrate that S$^2$-MLLM unifies superior performance, generalization, and efficiency, achieving significant performance over existing methods across the ScanRefer, Nr3D, and Sr3D datasets. Code will be available upon acceptance.
Published: 2025-12-01T03:08:34+00:00
Venue: arXiv
Score: 0.766 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Beining Xu; Siting Zhu; Zhao Jin; Junxian Li; Hesheng Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.766 (must_read)&lt;/p&gt;
&lt;p&gt;3D Visual Grounding (3DVG) focuses on locating objects in 3D scenes based on natural language descriptions, serving as a fundamental task for embodied AI and robotics. Recent advances in Multi-modal Large Language Models (MLLMs) have motivated research into extending them to 3DVG. However, MLLMs primarily process 2D visual inputs and struggle with understanding 3D spatial structure of scenes solely from these limited perspectives. Existing methods mainly utilize viewpoint-dependent rendering of reconstructed point clouds to provide explicit structural guidance for MLLMs in 3DVG tasks, leading to inefficiency and limited spatial reasoning. To address this issue, we propose S$^2$-MLLM, an efficient framework that enhances spatial reasoning in MLLMs through implicit spatial reasoning. We introduce a spatial guidance strategy that leverages the structure awareness of feed-forward 3D reconstruction. By acquiring 3D structural understanding during training, our model can implicitly reason about 3D scenes without relying on inefficient point cloud reconstruction. Moreover, we propose a structure-enhanced module (SE), which first employs intra-view and inter-view attention mechanisms to capture dependencies within views and correspondences across views. The module further integrates multi-level position encoding to associate visual representations with spatial positions and viewpoint information, enabling more accurate structural understanding. Extensive experiments demonstrate that S$^2$-MLLM unifies superior performance, generalization, and efficiency, achieving significant performance over existing methods across the ScanRefer, Nr3D, and Sr3D datasets. Code will be available upon acceptance.&lt;/p&gt;</content:encoded></item><item><title>DiR-Net: A Diagnostic and Iterative Rectification Network for Cross-Modal 3D Object Detection</title><link>https://doi.org/10.1016/j.knosys.2025.115023</link><guid>10.1016/j.knosys.2025.115023</guid><pubDate>Tue, 02 Dec 2025 07:54:22 +0000</pubDate><dc:creator>Miaohui Zhang</dc:creator><dc:creator>Shuang Wang</dc:creator><dc:creator>Kunpeng Bi</dc:creator><dc:creator>Ming Xin</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115023</prism:doi><description>Accurate perception of indoor scenes is a cornerstone of embodied intelligence. The core challenge in multi-modal 3D detection lies in achieving precise cross-modal feature alignment. However, existing methods typically rely on static projection, which prevents them from iteratively correcting alignment errors caused by occlusions or calibration inaccuracies, and they lack a mechanism to dynamically allocate computational resources based on fusion quality. To address these limitations, we propose a Diagnostic and Iterative Rectification Network (DiR-Net), which redefines the fusion process from static matching to a dynamic, confidence-based error correction procedure. Our core insight is that fusion quality is quantifiable: a Diagnostic Decision Module (DDM) analyzes the vector differences among initial 3D, 2D, and post-fusion features to compute an alignment confidence score that acts as an intelligent gate, adaptively balancing performance and efficiency. If optimization is deemed necessary, an Iterative Rectification Framework (IRF) module is activated to perform K rounds of refinement. In each iteration, unlike approaches that rely on implicit attention, our Rectification Regression Module (RRM) leverages the current fusion state and 3D geometric context to explicitly regress correction vectors for the 2D sampling coordinates, optimizing alignment with sub-pixel precision. Subsequently, an Internal Fusion Module (IFM) facilitates deep informational complementarity by generating cross-modal context to modulate the 2D and 3D feature streams. Experiments on the SUN RGB-D dataset demonstrate DiR-Net achieves a state-of-the-art performance of 70.68 mAP@0.25, establishing a new record on the benchmark. Our work pioneers a paradigm shift in multi-modal fusion, from one-shot fusion to adaptive, iterative error correction.
Published: 2025-12-02T07:54:22+00:00
Venue: Knowledge-Based Systems
Score: 0.765 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Miaohui Zhang; Shuang Wang; Kunpeng Bi; Ming Xin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115023"&gt;10.1016/j.knosys.2025.115023&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (must_read)&lt;/p&gt;
&lt;p&gt;Accurate perception of indoor scenes is a cornerstone of embodied intelligence. The core challenge in multi-modal 3D detection lies in achieving precise cross-modal feature alignment. However, existing methods typically rely on static projection, which prevents them from iteratively correcting alignment errors caused by occlusions or calibration inaccuracies, and they lack a mechanism to dynamically allocate computational resources based on fusion quality. To address these limitations, we propose a Diagnostic and Iterative Rectification Network (DiR-Net), which redefines the fusion process from static matching to a dynamic, confidence-based error correction procedure. Our core insight is that fusion quality is quantifiable: a Diagnostic Decision Module (DDM) analyzes the vector differences among initial 3D, 2D, and post-fusion features to compute an alignment confidence score that acts as an intelligent gate, adaptively balancing performance and efficiency. If optimization is deemed necessary, an Iterative Rectification Framework (IRF) module is activated to perform K rounds of refinement. In each iteration, unlike approaches that rely on implicit attention, our Rectification Regression Module (RRM) leverages the current fusion state and 3D geometric context to explicitly regress correction vectors for the 2D sampling coordinates, optimizing alignment with sub-pixel precision. Subsequently, an Internal Fusion Module (IFM) facilitates deep informational complementarity by generating cross-modal context to modulate the 2D and 3D feature streams. Experiments on the SUN RGB-D dataset demonstrate DiR-Net achieves a state-of-the-art performance of 70.68 mAP@0.25, establishing a new record on the benchmark. Our work pioneers a paradigm shift in multi-modal fusion, from one-shot fusion to adaptive, iterative error correction.&lt;/p&gt;</content:encoded></item><item><title>GeoVideo: Introducing Geometric Regularization into Video Generation Model</title><link>https://arxiv.org/abs/2512.03453v1</link><guid>http://arxiv.org/abs/2512.03453v1</guid><pubDate>Wed, 03 Dec 2025 05:11:57 +0000</pubDate><dc:creator>Yunpeng Bai</dc:creator><dc:creator>Shaoheng Fang</dc:creator><dc:creator>Chaohui Yu</dc:creator><dc:creator>Fan Wang</dc:creator><dc:creator>Qixing Huang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advances in video generation have enabled the synthesis of high-quality and visually realistic clips using diffusion transformer models. However, most existing approaches operate purely in the 2D pixel space and lack explicit mechanisms for modeling 3D structures, often resulting in temporally inconsistent geometries, implausible motions, and structural artifacts. In this work, we introduce geometric regularization losses into video generation by augmenting latent diffusion models with per-frame depth prediction. We adopted depth as the geometric representation because of the great progress in depth prediction and its compatibility with image-based latent encoders. Specifically, to enforce structural consistency over time, we propose a multi-view geometric loss that aligns the predicted depth maps across frames within a shared 3D coordinate system. Our method bridges the gap between appearance generation and 3D structure modeling, leading to improved spatio-temporal coherence, shape consistency, and physical plausibility. Experiments across multiple datasets show that our approach produces significantly more stable and geometrically consistent results than existing baselines.
Published: 2025-12-03T05:11:57+00:00
Venue: arXiv
Score: 0.765 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yunpeng Bai; Shaoheng Fang; Chaohui Yu; Fan Wang; Qixing Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (must_read)&lt;/p&gt;
&lt;p&gt;Recent advances in video generation have enabled the synthesis of high-quality and visually realistic clips using diffusion transformer models. However, most existing approaches operate purely in the 2D pixel space and lack explicit mechanisms for modeling 3D structures, often resulting in temporally inconsistent geometries, implausible motions, and structural artifacts. In this work, we introduce geometric regularization losses into video generation by augmenting latent diffusion models with per-frame depth prediction. We adopted depth as the geometric representation because of the great progress in depth prediction and its compatibility with image-based latent encoders. Specifically, to enforce structural consistency over time, we propose a multi-view geometric loss that aligns the predicted depth maps across frames within a shared 3D coordinate system. Our method bridges the gap between appearance generation and 3D structure modeling, leading to improved spatio-temporal coherence, shape consistency, and physical plausibility. Experiments across multiple datasets show that our approach produces significantly more stable and geometrically consistent results than existing baselines.&lt;/p&gt;</content:encoded></item><item><title>Exploiting Domain Properties in Language-Driven Domain Generalization for Semantic Segmentation</title><link>https://arxiv.org/abs/2512.03508v1</link><guid>http://arxiv.org/abs/2512.03508v1</guid><pubDate>Wed, 03 Dec 2025 06:58:38 +0000</pubDate><dc:creator>Seogkyu Jeon</dc:creator><dc:creator>Kibeom Hong</dc:creator><dc:creator>Hyeran Byun</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent domain generalized semantic segmentation (DGSS) studies have achieved notable improvements by distilling semantic knowledge from Vision-Language Models (VLMs). However, they overlook the semantic misalignment between visual and textual contexts, which arises due to the rigidity of a fixed context prompt learned on a single source domain. To this end, we present a novel domain generalization framework for semantic segmentation, namely Domain-aware Prompt-driven Masked Transformer (DPMFormer). Firstly, we introduce domain-aware prompt learning to facilitate semantic alignment between visual and textual cues. To capture various domain-specific properties with a single source dataset, we propose domain-aware contrastive learning along with the texture perturbation that diversifies the observable domains. Lastly, to establish a framework resilient against diverse environmental changes, we have proposed the domain-robust consistency learning which guides the model to minimize discrepancies of prediction from original and the augmented images. Through experiments and analyses, we demonstrate the superiority of the proposed framework, which establishes a new state-of-the-art on various DGSS benchmarks. The code is available at https://github.com/jone1222/DPMFormer.
Published: 2025-12-03T06:58:38+00:00
Venue: arXiv
Score: 0.764 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Seogkyu Jeon; Kibeom Hong; Hyeran Byun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.764 (must_read)&lt;/p&gt;
&lt;p&gt;Recent domain generalized semantic segmentation (DGSS) studies have achieved notable improvements by distilling semantic knowledge from Vision-Language Models (VLMs). However, they overlook the semantic misalignment between visual and textual contexts, which arises due to the rigidity of a fixed context prompt learned on a single source domain. To this end, we present a novel domain generalization framework for semantic segmentation, namely Domain-aware Prompt-driven Masked Transformer (DPMFormer). Firstly, we introduce domain-aware prompt learning to facilitate semantic alignment between visual and textual cues. To capture various domain-specific properties with a single source dataset, we propose domain-aware contrastive learning along with the texture perturbation that diversifies the observable domains. Lastly, to establish a framework resilient against diverse environmental changes, we have proposed the domain-robust consistency learning which guides the model to minimize discrepancies of prediction from original and the augmented images. Through experiments and analyses, we demonstrate the superiority of the proposed framework, which establishes a new state-of-the-art on various DGSS benchmarks. The code is available at https://github.com/jone1222/DPMFormer.&lt;/p&gt;</content:encoded></item></channel></rss>