<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Wed, 03 Dec 2025 04:14:00 +0000</lastBuildDate><item><title>Enhancing graph contrastive learning with knowledge graph embedding for recommendation</title><link>https://doi.org/10.1016/j.neucom.2025.132283</link><guid>10.1016/j.neucom.2025.132283</guid><pubDate>Tue, 02 Dec 2025 00:23:17 +0000</pubDate><dc:creator>Tao Xie</dc:creator><dc:creator>Xiaofeng Wang</dc:creator><dc:creator>Tianxiang Lv</dc:creator><dc:creator>Shuaiming Lai</dc:creator><dc:creator>Xiwen Zheng</dc:creator><dc:creator>Daying Quan</dc:creator><dc:creator>Yuanyuan Qi</dc:creator><dc:creator>Xiaofeng Huang</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132283</prism:doi><description>Graph contrastive learning (GCL) has shown considerable effectiveness in enhancing recommendation systems’ performance. Typically, GCL-based recommendation methods integrate graph embedding techniques with contrastive learning mechanisms to align entity representations across different augmented views of the user-item interaction graph. However, most of these methods rely on graph augmentation strategies that involve random structural perturbations, which can lead to the loss of valuable information and the introduction of noise, ultimately hindering model performance. To overcome this challenge, we propose a novel GCL framework that incorporates knowledge graph embeddings for recommendation. By leveraging the relational heterogeneity within the KG, our approach enhances the diversity of augmented views while preserving the intrinsic semantic structures of the user-item graph. Specifically, we apply singular value decomposition for contrastive augmentation, which helps retain more relevant information. Moreover, we design a heterogeneous knowledge attentive aggregator to distill item embeddings from the KG while extracting user embeddings from the interaction graph. Finally, we leverage knowledge-based entity embeddings to strengthen the contrastive recommendation process, thereby improving feature representation and recommendation accuracy. Extensive experiments on real-world datasets demonstrate that our method significantly outperforms existing state-of-the-art techniques, particularly in mitigating data sparsity and the cold-start problem.
Published: 2025-12-02T00:23:17+00:00
Venue: Neurocomputing
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tao Xie; Xiaofeng Wang; Tianxiang Lv; Shuaiming Lai; Xiwen Zheng; Daying Quan; Yuanyuan Qi; Xiaofeng Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132283"&gt;10.1016/j.neucom.2025.132283&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Graph contrastive learning (GCL) has shown considerable effectiveness in enhancing recommendation systems’ performance. Typically, GCL-based recommendation methods integrate graph embedding techniques with contrastive learning mechanisms to align entity representations across different augmented views of the user-item interaction graph. However, most of these methods rely on graph augmentation strategies that involve random structural perturbations, which can lead to the loss of valuable information and the introduction of noise, ultimately hindering model performance. To overcome this challenge, we propose a novel GCL framework that incorporates knowledge graph embeddings for recommendation. By leveraging the relational heterogeneity within the KG, our approach enhances the diversity of augmented views while preserving the intrinsic semantic structures of the user-item graph. Specifically, we apply singular value decomposition for contrastive augmentation, which helps retain more relevant information. Moreover, we design a heterogeneous knowledge attentive aggregator to distill item embeddings from the KG while extracting user embeddings from the interaction graph. Finally, we leverage knowledge-based entity embeddings to strengthen the contrastive recommendation process, thereby improving feature representation and recommendation accuracy. Extensive experiments on real-world datasets demonstrate that our method significantly outperforms existing state-of-the-art techniques, particularly in mitigating data sparsity and the cold-start problem.&lt;/p&gt;</content:encoded></item><item><title>OneThinker: All-in-one Reasoning Model for Image and Video</title><link>https://arxiv.org/abs/2512.03043v1</link><guid>http://arxiv.org/abs/2512.03043v1</guid><pubDate>Tue, 02 Dec 2025 18:59:52 +0000</pubDate><dc:creator>Kaituo Feng</dc:creator><dc:creator>Manyuan Zhang</dc:creator><dc:creator>Hongyu Li</dc:creator><dc:creator>Kaixuan Fan</dc:creator><dc:creator>Shuang Chen</dc:creator><dc:creator>Yilei Jiang</dc:creator><dc:creator>Dian Zheng</dc:creator><dc:creator>Peiwen Sun</dc:creator><dc:creator>Yiyuan Zhang</dc:creator><dc:creator>Haoze Sun</dc:creator><dc:creator>Yan Feng</dc:creator><dc:creator>Peng Pei</dc:creator><dc:creator>Xunliang Cai</dc:creator><dc:creator>Xiangyu Yue</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.
Published: 2025-12-02T18:59:52+00:00
Venue: arXiv
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kaituo Feng; Manyuan Zhang; Hongyu Li; Kaixuan Fan; Shuang Chen; Yilei Jiang; Dian Zheng; Peiwen Sun; Yiyuan Zhang; Haoze Sun; Yan Feng; Peng Pei; Xunliang Cai; Xiangyu Yue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.&lt;/p&gt;</content:encoded></item><item><title>Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling</title><link>https://arxiv.org/abs/2512.01821v1</link><guid>http://arxiv.org/abs/2512.01821v1</guid><pubDate>Mon, 01 Dec 2025 16:01:41 +0000</pubDate><dc:creator>Meng Cao</dc:creator><dc:creator>Haokun Lin</dc:creator><dc:creator>Haoyuan Li</dc:creator><dc:creator>Haoran Tang</dc:creator><dc:creator>Rongtao Xu</dc:creator><dc:creator>Dong An</dc:creator><dc:creator>Xue Liu</dc:creator><dc:creator>Ian Reid</dc:creator><dc:creator>Xiaodan Liang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Spatial reasoning, the ability to understand and interpret the 3D structure of the world, is a critical yet underdeveloped capability in Multimodal Large Language Models (MLLMs). Current methods predominantly rely on verbal descriptive tuning, which suffers from visual illiteracy, i.e., they learn spatial concepts through textual symbols alone, devoid of connection to their visual manifestations. To bridge this gap, this paper introduces MILO, an Implicit spatIaL wOrld modeling paradigm that simulates human-like spatial imagination. MILO integrates a visual generator to provide geometry-aware feedback, thereby implicitly grounding the MLLM's symbolic reasoning in perceptual experience. Complementing this paradigm, we propose RePE (Relative Positional Encoding), a novel encoding scheme that captures relative camera-pose transformations, offering superior performance over absolute coordinate systems. To support the training, we construct GeoGen, a large-scale Geometry-aware Generative dataset with approximately 2,241 videos and 67,827 observation-action-outcome triplets. Experiments demonstrate that our approach significantly enhances spatial reasoning capabilities across multiple baselines and benchmarks, offering a more holistic understanding of 3D space.
Published: 2025-12-01T16:01:41+00:00
Venue: arXiv
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Meng Cao; Haokun Lin; Haoyuan Li; Haoran Tang; Rongtao Xu; Dong An; Xue Liu; Ian Reid; Xiaodan Liang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Spatial reasoning, the ability to understand and interpret the 3D structure of the world, is a critical yet underdeveloped capability in Multimodal Large Language Models (MLLMs). Current methods predominantly rely on verbal descriptive tuning, which suffers from visual illiteracy, i.e., they learn spatial concepts through textual symbols alone, devoid of connection to their visual manifestations. To bridge this gap, this paper introduces MILO, an Implicit spatIaL wOrld modeling paradigm that simulates human-like spatial imagination. MILO integrates a visual generator to provide geometry-aware feedback, thereby implicitly grounding the MLLM&amp;#x27;s symbolic reasoning in perceptual experience. Complementing this paradigm, we propose RePE (Relative Positional Encoding), a novel encoding scheme that captures relative camera-pose transformations, offering superior performance over absolute coordinate systems. To support the training, we construct GeoGen, a large-scale Geometry-aware Generative dataset with approximately 2,241 videos and 67,827 observation-action-outcome triplets. Experiments demonstrate that our approach significantly enhances spatial reasoning capabilities across multiple baselines and benchmarks, offering a more holistic understanding of 3D space.&lt;/p&gt;</content:encoded></item><item><title>Heterogeneous Environment-aware Multimodal Recommendation with Modality Alignment</title><link>https://doi.org/10.1016/j.inffus.2025.103989</link><guid>10.1016/j.inffus.2025.103989</guid><pubDate>Mon, 01 Dec 2025 08:18:37 +0000</pubDate><dc:creator>Ke Shi</dc:creator><dc:creator>Yan Zhang</dc:creator><dc:creator>Miao Zhang</dc:creator><dc:creator>Kui Xiao</dc:creator><dc:creator>Dunhui Yu</dc:creator><dc:creator>Yahui Zhou</dc:creator><dc:creator>Wenxin Huang</dc:creator><dc:creator>Zhifei Li</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.103989</prism:doi><description>Multimodal recommendation systems enhance accuracy by integrating information from different modalities. To address the issue of modality missing in real-world data, various efforts have attempted to restore missing data through data completion. Despite notable improvements, these methods still fail to fully resolve uncertainty from missing information. For different missing scenarios, distinct strategies are required for completion. Therefore, building an environment capable of handling any missing-modality case remains a challenge. To address this, we propose HEARec, a framework that simulates diverse missing-modality cases by generating heterogeneous environments. To construct missing scenarios applicable to various cases, we employ a tailored distribution combined with cyclic shifts to generate multiple environments with different weight groups. Moreover, to avoid directly merging multimodal features into item embeddings, we design independent processors to separately handle neighborhood information. For potential cross-modal inconsistencies, we map each modality embedding into a shared hypergraph space with MSE regularization. Finally, interaction-based modeling and aggregation strategies capture user interests from collaborative signals. Experiments demonstrate that HEARec consistently outperforms state-of-the-art models, achieving up to 4.53% and 6.02% improvements on the Baby and Sports datasets, respectively. Our code is available at https://github.com/HubuKG/HEARec .
Published: 2025-12-01T08:18:37+00:00
Venue: Information Fusion
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ke Shi; Yan Zhang; Miao Zhang; Kui Xiao; Dunhui Yu; Yahui Zhou; Wenxin Huang; Zhifei Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.103989"&gt;10.1016/j.inffus.2025.103989&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal recommendation systems enhance accuracy by integrating information from different modalities. To address the issue of modality missing in real-world data, various efforts have attempted to restore missing data through data completion. Despite notable improvements, these methods still fail to fully resolve uncertainty from missing information. For different missing scenarios, distinct strategies are required for completion. Therefore, building an environment capable of handling any missing-modality case remains a challenge. To address this, we propose HEARec, a framework that simulates diverse missing-modality cases by generating heterogeneous environments. To construct missing scenarios applicable to various cases, we employ a tailored distribution combined with cyclic shifts to generate multiple environments with different weight groups. Moreover, to avoid directly merging multimodal features into item embeddings, we design independent processors to separately handle neighborhood information. For potential cross-modal inconsistencies, we map each modality embedding into a shared hypergraph space with MSE regularization. Finally, interaction-based modeling and aggregation strategies capture user interests from collaborative signals. Experiments demonstrate that HEARec consistently outperforms state-of-the-art models, achieving up to 4.53% and 6.02% improvements on the Baby and Sports datasets, respectively. Our code is available at https://github.com/HubuKG/HEARec .&lt;/p&gt;</content:encoded></item><item><title>SkyMoE: A Vision-Language Foundation Model for Enhancing Geospatial Interpretation with Mixture of Experts</title><link>https://arxiv.org/abs/2512.02517v1</link><guid>http://arxiv.org/abs/2512.02517v1</guid><pubDate>Tue, 02 Dec 2025 08:17:16 +0000</pubDate><dc:creator>Jiaqi Liu</dc:creator><dc:creator>Ronghao Fu</dc:creator><dc:creator>Lang Sun</dc:creator><dc:creator>Haoran Liu</dc:creator><dc:creator>Xiao Yang</dc:creator><dc:creator>Weipeng Zhang</dc:creator><dc:creator>Xu Na</dc:creator><dc:creator>Zhuoran Duan</dc:creator><dc:creator>Bo Yang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The emergence of large vision-language models (VLMs) has significantly enhanced the efficiency and flexibility of geospatial interpretation. However, general-purpose VLMs remain suboptimal for remote sensing (RS) tasks. Existing geospatial VLMs typically adopt a unified modeling strategy and struggle to differentiate between task types and interpretation granularities, limiting their ability to balance local detail perception and global contextual understanding. In this paper, we present SkyMoE, a Mixture-of-Experts (MoE) vision-language model tailored for multimodal, multi-task RS interpretation. SkyMoE employs an adaptive router that generates task- and granularity-aware routing instructions, enabling specialized large language model experts to handle diverse sub-tasks. To further promote expert decoupling and granularity sensitivity, we introduce a context-disentangled augmentation strategy that creates contrastive pairs between local and global features, guiding experts toward level-specific representation learning. We also construct MGRS-Bench, a comprehensive benchmark covering multiple RS interpretation tasks and granularity levels, to evaluate generalization in complex scenarios. Extensive experiments on 21 public datasets demonstrate that SkyMoE achieves state-of-the-art performance across tasks, validating its adaptability, scalability, and superior multi-granularity understanding in remote sensing.
Published: 2025-12-02T08:17:16+00:00
Venue: arXiv
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaqi Liu; Ronghao Fu; Lang Sun; Haoran Liu; Xiao Yang; Weipeng Zhang; Xu Na; Zhuoran Duan; Bo Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;The emergence of large vision-language models (VLMs) has significantly enhanced the efficiency and flexibility of geospatial interpretation. However, general-purpose VLMs remain suboptimal for remote sensing (RS) tasks. Existing geospatial VLMs typically adopt a unified modeling strategy and struggle to differentiate between task types and interpretation granularities, limiting their ability to balance local detail perception and global contextual understanding. In this paper, we present SkyMoE, a Mixture-of-Experts (MoE) vision-language model tailored for multimodal, multi-task RS interpretation. SkyMoE employs an adaptive router that generates task- and granularity-aware routing instructions, enabling specialized large language model experts to handle diverse sub-tasks. To further promote expert decoupling and granularity sensitivity, we introduce a context-disentangled augmentation strategy that creates contrastive pairs between local and global features, guiding experts toward level-specific representation learning. We also construct MGRS-Bench, a comprehensive benchmark covering multiple RS interpretation tasks and granularity levels, to evaluate generalization in complex scenarios. Extensive experiments on 21 public datasets demonstrate that SkyMoE achieves state-of-the-art performance across tasks, validating its adaptability, scalability, and superior multi-granularity understanding in remote sensing.&lt;/p&gt;</content:encoded></item><item><title>Multimodal cross fusion Mamba network for remote sensing image semantic segmentation with complementary masked self-supervision</title><link>https://doi.org/10.1016/j.jag.2025.104960</link><guid>10.1016/j.jag.2025.104960</guid><pubDate>Mon, 01 Dec 2025 08:22:58 +0000</pubDate><dc:creator>Xiao Liu</dc:creator><dc:creator>Tao Wang</dc:creator><dc:creator>Fei Jin</dc:creator><dc:creator>Jie Rui</dc:creator><dc:creator>Shuxiang Wang</dc:creator><dc:creator>Ziheng Huang</dc:creator><dc:creator>Yujie Zou</dc:creator><dc:creator>Xiaowei Yu</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2025.104960</prism:doi><description>Deep learning-based multimodal remote sensing image (RSI) semantic segmentation models have garnered significant attention owing to their ability to leverage complementary information across modalities, resulting in more robust and accurate Earth observation. However, most existing approaches rely predominantly on convolutional neural networks with limited receptive fields or transformer-based architectures that are computationally intensive. In this study, we proposed a multimodal cross fusion Mamba (MCF-Mamba) network for multimodal RSI semantic segmentation, aiming to collaboratively enhance accuracy and efficiency. The proposed network features three core components: a dual-branch VMamba encoder, a multimodal cross-Mamba fusion module, and a U-shaped Mamba decoder. The architecture with a unified structure centered on the selective state space model ensures that it achieves global perception with linear complexity. Furthermore, for addressing the constraint of limited labeled samples in practical applications, we developed a generative multimodal complementary masked self-supervised (CMSS) strategy. It leverages abundant unlabeled RSIs to learn generalized multimodal representations by modeling intermodal complementary consistency. Extensive experiments on three public datasets involving optical-SAR and optical-DEM modalities demonstrated that the proposed network and strategy are superior to other advanced segmentation models and self-supervised methods in land cover mapping and building extraction tasks. The MCF-Mamba network achieves the highest accuracy while significantly reducing both model size and computational cost, and further improves the accuracy and generalization through the CMSS strategy. The source code is available at https://github.com/Xiao-RS/MCFMamba_CMSS .
Published: 2025-12-01T08:22:58+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.776 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiao Liu; Tao Wang; Fei Jin; Jie Rui; Shuxiang Wang; Ziheng Huang; Yujie Zou; Xiaowei Yu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2025.104960"&gt;10.1016/j.jag.2025.104960&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.776 (must_read)&lt;/p&gt;
&lt;p&gt;Deep learning-based multimodal remote sensing image (RSI) semantic segmentation models have garnered significant attention owing to their ability to leverage complementary information across modalities, resulting in more robust and accurate Earth observation. However, most existing approaches rely predominantly on convolutional neural networks with limited receptive fields or transformer-based architectures that are computationally intensive. In this study, we proposed a multimodal cross fusion Mamba (MCF-Mamba) network for multimodal RSI semantic segmentation, aiming to collaboratively enhance accuracy and efficiency. The proposed network features three core components: a dual-branch VMamba encoder, a multimodal cross-Mamba fusion module, and a U-shaped Mamba decoder. The architecture with a unified structure centered on the selective state space model ensures that it achieves global perception with linear complexity. Furthermore, for addressing the constraint of limited labeled samples in practical applications, we developed a generative multimodal complementary masked self-supervised (CMSS) strategy. It leverages abundant unlabeled RSIs to learn generalized multimodal representations by modeling intermodal complementary consistency. Extensive experiments on three public datasets involving optical-SAR and optical-DEM modalities demonstrated that the proposed network and strategy are superior to other advanced segmentation models and self-supervised methods in land cover mapping and building extraction tasks. The MCF-Mamba network achieves the highest accuracy while significantly reducing both model size and computational cost, and further improves the accuracy and generalization through the CMSS strategy. The source code is available at https://github.com/Xiao-RS/MCFMamba_CMSS .&lt;/p&gt;</content:encoded></item><item><title>Diffusion-Based Text-Guided Image Generation with Fine-Grained Spatial Object-Attribute Relationships</title><link>https://doi.org/10.1109/tcsvt.2025.3639218</link><guid>10.1109/tcsvt.2025.3639218</guid><pubDate>Mon, 01 Dec 2025 18:25:50 +0000</pubDate><dc:creator>Fuxiang Wu</dc:creator><dc:creator>Liu Liu</dc:creator><dc:creator>Fusheng Hao</dc:creator><dc:creator>Ziliang Ren</dc:creator><dc:creator>Dacheng Tao</dc:creator><dc:creator>Xinyu Wu</dc:creator><dc:creator>Jun Cheng</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3639218</prism:doi><description>Expressing and controlling fine-grained spatial attributes of objects in large-scale models presents significant challenges, as these spatial attributes are often difficult to describe textually and exhaustive enumeration is impractical. This hinders effective alignment with user preferences regarding spatial attribute-object relationships in fine-grained synthesis tasks. To tackle this problem, we propose AttrObjDiff, a novel framework built on the pre-trained Stable Diffusion model to integrate spatial attribute maps. Firstly, AttrObjDiff constrains the denoising step using trainable cross-attention fusion modules, attribute-enhancing cross-attention and LoRAs. The fusion modules take layout features extracted by a frozen ControlNet and corresponding fine-grained attribute maps as inputs to generate joint constraint features of spatial attribute-object relationships. We leverage attribute-enhancing cross-attention within the U-Net to further refine these spatial attributes. Finally, LoRAs are employed to align with these joint constraint features of finegrained relationships. Secondly, AttrObjDiff enhances the reverse process with lightweight noise reranking models to improve spatial object-attribute alignment. The reranking models select semantic noises related to fine-grained relationships, improving synthesis quality without significantly increasing computational costs. Experimental results demonstrate that our method can generate high-quality images guided by fine-grained spatial object-attribute relationships, improving synthesis controllability and semantic consistency.
Published: 2025-12-01T18:25:50+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fuxiang Wu; Liu Liu; Fusheng Hao; Ziliang Ren; Dacheng Tao; Xinyu Wu; Jun Cheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3639218"&gt;10.1109/tcsvt.2025.3639218&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;Expressing and controlling fine-grained spatial attributes of objects in large-scale models presents significant challenges, as these spatial attributes are often difficult to describe textually and exhaustive enumeration is impractical. This hinders effective alignment with user preferences regarding spatial attribute-object relationships in fine-grained synthesis tasks. To tackle this problem, we propose AttrObjDiff, a novel framework built on the pre-trained Stable Diffusion model to integrate spatial attribute maps. Firstly, AttrObjDiff constrains the denoising step using trainable cross-attention fusion modules, attribute-enhancing cross-attention and LoRAs. The fusion modules take layout features extracted by a frozen ControlNet and corresponding fine-grained attribute maps as inputs to generate joint constraint features of spatial attribute-object relationships. We leverage attribute-enhancing cross-attention within the U-Net to further refine these spatial attributes. Finally, LoRAs are employed to align with these joint constraint features of finegrained relationships. Secondly, AttrObjDiff enhances the reverse process with lightweight noise reranking models to improve spatial object-attribute alignment. The reranking models select semantic noises related to fine-grained relationships, improving synthesis quality without significantly increasing computational costs. Experimental results demonstrate that our method can generate high-quality images guided by fine-grained spatial object-attribute relationships, improving synthesis controllability and semantic consistency.&lt;/p&gt;</content:encoded></item><item><title>Which is playing a key role on sampling-based large-scale GNNs, sampling or iteration?</title><link>https://doi.org/10.1016/j.neucom.2025.132246</link><guid>10.1016/j.neucom.2025.132246</guid><pubDate>Tue, 02 Dec 2025 00:23:10 +0000</pubDate><dc:creator>Bo Jiang</dc:creator><dc:creator>Lijun Dong</dc:creator><dc:creator>Xiaoyue Peng</dc:creator><dc:creator>Renyao Chen</dc:creator><dc:creator>Chao Liu</dc:creator><dc:creator>Hong Yao</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132246</prism:doi><description>Graph Neural Networks (GNNs) is a set of deep-learning-based methods that model graph data. Due to their excellent performance and powerful explainability, GNNs are widely used for a variety of graph analysis tasks. How to scale GNNs to large-scale graph datasets has been a hot research topic in recent years, and several approaches have been proposed to solve this problem. Recently training GNNs by sampling graphs has become a popular direction. This paper reviews current methods for training GNNs using sampled subgraphs and provides detailed theoretical analysis of these sampling methods. On this basis, this study explored the effectiveness of various sampling algorithms, including methods based on graph structure, structural entropy, information entropy, and iterative sampling training methods. Moreover, for sampled graph training, the paper summarizes a unified framework of three steps, namely subgraph sampling, iterative process, and GNNs module. Experiments show that different sampling methods can achieve similar performance with the same sampling size, while iterative methods can accelerate the training of GNNs. Finally, this paper provides a detailed theoretical explanation of this phenomenon.
Published: 2025-12-02T00:23:10+00:00
Venue: Neurocomputing
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bo Jiang; Lijun Dong; Xiaoyue Peng; Renyao Chen; Chao Liu; Hong Yao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132246"&gt;10.1016/j.neucom.2025.132246&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Graph Neural Networks (GNNs) is a set of deep-learning-based methods that model graph data. Due to their excellent performance and powerful explainability, GNNs are widely used for a variety of graph analysis tasks. How to scale GNNs to large-scale graph datasets has been a hot research topic in recent years, and several approaches have been proposed to solve this problem. Recently training GNNs by sampling graphs has become a popular direction. This paper reviews current methods for training GNNs using sampled subgraphs and provides detailed theoretical analysis of these sampling methods. On this basis, this study explored the effectiveness of various sampling algorithms, including methods based on graph structure, structural entropy, information entropy, and iterative sampling training methods. Moreover, for sampled graph training, the paper summarizes a unified framework of three steps, namely subgraph sampling, iterative process, and GNNs module. Experiments show that different sampling methods can achieve similar performance with the same sampling size, while iterative methods can accelerate the training of GNNs. Finally, this paper provides a detailed theoretical explanation of this phenomenon.&lt;/p&gt;</content:encoded></item><item><title>DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images</title><link>https://arxiv.org/abs/2512.03004v1</link><guid>http://arxiv.org/abs/2512.03004v1</guid><pubDate>Tue, 02 Dec 2025 18:29:18 +0000</pubDate><dc:creator>Xiaoxue Chen</dc:creator><dc:creator>Ziyi Xiong</dc:creator><dc:creator>Yuantao Chen</dc:creator><dc:creator>Gen Li</dc:creator><dc:creator>Nan Wang</dc:creator><dc:creator>Hongcheng Luo</dc:creator><dc:creator>Long Chen</dc:creator><dc:creator>Haiyang Sun</dc:creator><dc:creator>Bing Wang</dc:creator><dc:creator>Guang Chen</dc:creator><dc:creator>Hangjun Ye</dc:creator><dc:creator>Hongyang Li</dc:creator><dc:creator>Ya-Qin Zhang</dc:creator><dc:creator>Hao Zhao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.
Published: 2025-12-02T18:29:18+00:00
Venue: arXiv
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoxue Chen; Ziyi Xiong; Yuantao Chen; Gen Li; Nan Wang; Hongcheng Luo; Long Chen; Haiyang Sun; Bing Wang; Guang Chen; Hangjun Ye; Hongyang Li; Ya-Qin Zhang; Hao Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.&lt;/p&gt;</content:encoded></item><item><title>See, Think, Learn: A Self-Taught Multimodal Reasoner</title><link>https://arxiv.org/abs/2512.02456v1</link><guid>http://arxiv.org/abs/2512.02456v1</guid><pubDate>Tue, 02 Dec 2025 06:30:10 +0000</pubDate><dc:creator>Sourabh Sharma</dc:creator><dc:creator>Sonam Gupta</dc:creator><dc:creator>Sadbhawna</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language Models (VLMs) have achieved remarkable progress in integrating visual perception with language understanding. However, effective multimodal reasoning requires both accurate perception and robust reasoning, and weakness in either limits the performance of VLMs. Prior efforts to enhance reasoning often depend on high-quality chain-of-thought (CoT) data, obtained via labor-intensive human annotations, costly proprietary models, or self-training methods that overlook perception. To address these limitations, we propose a simple yet effective self-training framework called See-Think-Learn (STL). At its core, STL introduces a structured reasoning template that encourages the model to see before thinking, first extracting visual attributes in textual form, then using them to guide reasoning. The framework jointly improves perception and reasoning by having the model generate and learn from its own structured rationales in a self-training loop. Furthermore, we augment the training data with negative rationales, i.e. explanations that justify why certain answer choices are incorrect, to enhance the model's ability to distinguish between correct and misleading responses. This fosters more discriminative and robust learning. Experiments across diverse domains show that STL consistently outperforms baselines trained directly only on answers or self-generated reasoning, while qualitative analysis confirms the high quality of its rationales. STL thus provides a cost-effective solution to enhance multimodal reasoning ability of VLMs.
Published: 2025-12-02T06:30:10+00:00
Venue: arXiv
Score: 0.773 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sourabh Sharma; Sonam Gupta; Sadbhawna&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (must_read)&lt;/p&gt;
&lt;p&gt;Vision-Language Models (VLMs) have achieved remarkable progress in integrating visual perception with language understanding. However, effective multimodal reasoning requires both accurate perception and robust reasoning, and weakness in either limits the performance of VLMs. Prior efforts to enhance reasoning often depend on high-quality chain-of-thought (CoT) data, obtained via labor-intensive human annotations, costly proprietary models, or self-training methods that overlook perception. To address these limitations, we propose a simple yet effective self-training framework called See-Think-Learn (STL). At its core, STL introduces a structured reasoning template that encourages the model to see before thinking, first extracting visual attributes in textual form, then using them to guide reasoning. The framework jointly improves perception and reasoning by having the model generate and learn from its own structured rationales in a self-training loop. Furthermore, we augment the training data with negative rationales, i.e. explanations that justify why certain answer choices are incorrect, to enhance the model&amp;#x27;s ability to distinguish between correct and misleading responses. This fosters more discriminative and robust learning. Experiments across diverse domains show that STL consistently outperforms baselines trained directly only on answers or self-generated reasoning, while qualitative analysis confirms the high quality of its rationales. STL thus provides a cost-effective solution to enhance multimodal reasoning ability of VLMs.&lt;/p&gt;</content:encoded></item><item><title>Referring Remote Sensing Image Segmentation via Multi-Scale Spatially-Guided Joint Prediction</title><link>https://doi.org/10.1109/jstars.2025.3638802</link><guid>10.1109/jstars.2025.3638802</guid><pubDate>Mon, 01 Dec 2025 18:24:44 +0000</pubDate><dc:creator>Tianxiang Zhang</dc:creator><dc:creator>Zhaokun Wen</dc:creator><dc:creator>Bo Kong</dc:creator><dc:creator>Kecheng Liu</dc:creator><dc:creator>Yisi Zhang</dc:creator><dc:creator>Peixian Zhuang</dc:creator><dc:creator>Jiangyun Li</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3638802</prism:doi><description>Referring Remote Sensing Image Segmentation (RRSIS) is critical for text-guided environmental monitoring, land cover classification, precision agriculture, and urban planning, requiring precise segmentation of objects in remote sensing imagery guided by textual descriptions. This task is uniquely challenging due to the considerable vision-language gap, broad coverage of remote sensing imagery with diverse categories and small targets, and the presence of clustered, unclear targets with blurred edges. To tackle these issues, we propose STDNet, a novel framework designed to bridge the vision-language gap, enhance multi-scale feature interaction, and improve fine-grained object differentiation. Specifically, STDNet introduces (1) the Spatial Multi-Scale Correlation (SMSC) for improved vision-language feature alignment, (2) the Target-Background TwinStream Decoder (T-BTD) for precise distinction between targets and non-targets, and (3) the Dual-Modal Object Learning Strategy (D-MOLS) for robust multimodal feature reconstruction. Extensive experiments on the benchmark datasets RefSegRS and RRSIS-D demonstrate that STDNet achieves state-of-the-art performance, effectively dealing with the core challenges of RRSIS with enhanced precision and robustness. Consequently, it is envisaged that the proposed STDNet model will be an advantage in the RRSIS task. Datasets and codes are available at https://github.com/wzk913ysq/STDNet.
Published: 2025-12-01T18:24:44+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.771 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tianxiang Zhang; Zhaokun Wen; Bo Kong; Kecheng Liu; Yisi Zhang; Peixian Zhuang; Jiangyun Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3638802"&gt;10.1109/jstars.2025.3638802&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (must_read)&lt;/p&gt;
&lt;p&gt;Referring Remote Sensing Image Segmentation (RRSIS) is critical for text-guided environmental monitoring, land cover classification, precision agriculture, and urban planning, requiring precise segmentation of objects in remote sensing imagery guided by textual descriptions. This task is uniquely challenging due to the considerable vision-language gap, broad coverage of remote sensing imagery with diverse categories and small targets, and the presence of clustered, unclear targets with blurred edges. To tackle these issues, we propose STDNet, a novel framework designed to bridge the vision-language gap, enhance multi-scale feature interaction, and improve fine-grained object differentiation. Specifically, STDNet introduces (1) the Spatial Multi-Scale Correlation (SMSC) for improved vision-language feature alignment, (2) the Target-Background TwinStream Decoder (T-BTD) for precise distinction between targets and non-targets, and (3) the Dual-Modal Object Learning Strategy (D-MOLS) for robust multimodal feature reconstruction. Extensive experiments on the benchmark datasets RefSegRS and RRSIS-D demonstrate that STDNet achieves state-of-the-art performance, effectively dealing with the core challenges of RRSIS with enhanced precision and robustness. Consequently, it is envisaged that the proposed STDNet model will be an advantage in the RRSIS task. Datasets and codes are available at https://github.com/wzk913ysq/STDNet.&lt;/p&gt;</content:encoded></item><item><title>Structural-Spectral Graph Convolution with Evidential Edge Learning for Hyperspectral Image Clustering</title><link>https://doi.org/10.1109/tip.2025.3636098</link><guid>10.1109/tip.2025.3636098</guid><pubDate>Tue, 02 Dec 2025 18:50:39 +0000</pubDate><dc:creator>Jianhan Qi</dc:creator><dc:creator>Yuheng Jia</dc:creator><dc:creator>Hui Liu</dc:creator><dc:creator>Junhui Hou</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3636098</prism:doi><description>Hyperspectral image (HSI) clustering groups pixels into clusters without labeled data, which is an important yet challenging task. For large-scale HSIs, most methods rely on superpixel segmentation and perform superpixel-level clustering based on graph neural networks (GNNs). However, existing GNNs cannot fully exploit the spectral information of the input HSI, and the inaccurate superpixel topological graph may lead to the confusion of different class semantics during information aggregation. To address these challenges, we first propose a structural-spectral graph convolutional operator (SSGCO) tailored for graph-structured HSI superpixels to improve their representation quality through the co-extraction of spatial and spectral features. Second, we propose an evidence-guided adaptive edge learning (EGAEL) module that adaptively predicts and refines edge weights in the superpixel topological graph. We integrate the proposed method into a contrastive learning framework to achieve clustering, where representation learning and clustering are simultaneously conducted. Experiments demonstrate that the proposed method improves clustering accuracy by 2.61%, 6.06%, 4.96% and 3.15% over the best compared methods on four HSI datasets. Our code is available at https://github.com/jhqi/SSGCO-EGAEL.
Published: 2025-12-02T18:50:39+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.769 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianhan Qi; Yuheng Jia; Hui Liu; Junhui Hou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3636098"&gt;10.1109/tip.2025.3636098&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.769 (must_read)&lt;/p&gt;
&lt;p&gt;Hyperspectral image (HSI) clustering groups pixels into clusters without labeled data, which is an important yet challenging task. For large-scale HSIs, most methods rely on superpixel segmentation and perform superpixel-level clustering based on graph neural networks (GNNs). However, existing GNNs cannot fully exploit the spectral information of the input HSI, and the inaccurate superpixel topological graph may lead to the confusion of different class semantics during information aggregation. To address these challenges, we first propose a structural-spectral graph convolutional operator (SSGCO) tailored for graph-structured HSI superpixels to improve their representation quality through the co-extraction of spatial and spectral features. Second, we propose an evidence-guided adaptive edge learning (EGAEL) module that adaptively predicts and refines edge weights in the superpixel topological graph. We integrate the proposed method into a contrastive learning framework to achieve clustering, where representation learning and clustering are simultaneously conducted. Experiments demonstrate that the proposed method improves clustering accuracy by 2.61%, 6.06%, 4.96% and 3.15% over the best compared methods on four HSI datasets. Our code is available at https://github.com/jhqi/SSGCO-EGAEL.&lt;/p&gt;</content:encoded></item><item><title>GeoDiT: A Diffusion-based Vision-Language Model for Geospatial Understanding</title><link>https://arxiv.org/abs/2512.02505v1</link><guid>http://arxiv.org/abs/2512.02505v1</guid><pubDate>Tue, 02 Dec 2025 07:59:46 +0000</pubDate><dc:creator>Jiaqi Liu</dc:creator><dc:creator>Ronghao Fu</dc:creator><dc:creator>Haoran Liu</dc:creator><dc:creator>Lang Sun</dc:creator><dc:creator>Bo Yang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Autoregressive models are structurally misaligned with the inherently parallel nature of geospatial understanding, forcing a rigid sequential narrative onto scenes and fundamentally hindering the generation of structured and coherent outputs. We challenge this paradigm by reframing geospatial generation as a parallel refinement process, enabling a holistic, coarse-to-fine synthesis that resolves all semantic elements simultaneously. To operationalize this, we introduce GeoDiT, the first diffusion-based vision-language model tailored for the geospatial domain. Extensive experiments demonstrate that GeoDiT establishes a new state-of-the-art on benchmarks requiring structured, object-centric outputs. It achieves significant gains in image captioning, visual grounding, and multi-object detection, precisely the tasks where autoregressive models falter. Our work validates that aligning the generative process with the data's intrinsic structure is key to unlocking superior performance in complex geospatial analysis.
Published: 2025-12-02T07:59:46+00:00
Venue: arXiv
Score: 0.768 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaqi Liu; Ronghao Fu; Haoran Liu; Lang Sun; Bo Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.768 (must_read)&lt;/p&gt;
&lt;p&gt;Autoregressive models are structurally misaligned with the inherently parallel nature of geospatial understanding, forcing a rigid sequential narrative onto scenes and fundamentally hindering the generation of structured and coherent outputs. We challenge this paradigm by reframing geospatial generation as a parallel refinement process, enabling a holistic, coarse-to-fine synthesis that resolves all semantic elements simultaneously. To operationalize this, we introduce GeoDiT, the first diffusion-based vision-language model tailored for the geospatial domain. Extensive experiments demonstrate that GeoDiT establishes a new state-of-the-art on benchmarks requiring structured, object-centric outputs. It achieves significant gains in image captioning, visual grounding, and multi-object detection, precisely the tasks where autoregressive models falter. Our work validates that aligning the generative process with the data&amp;#x27;s intrinsic structure is key to unlocking superior performance in complex geospatial analysis.&lt;/p&gt;</content:encoded></item><item><title>GRRSIS: Generalized Referring Remote Sensing Image Segmentation</title><link>https://doi.org/10.1109/tgrs.2025.3639131</link><guid>10.1109/tgrs.2025.3639131</guid><pubDate>Mon, 01 Dec 2025 18:24:33 +0000</pubDate><dc:creator>Wenyu Mi</dc:creator><dc:creator>Jianji Wang</dc:creator><dc:creator>Fuzhen Zhuang</dc:creator><dc:creator>Nanning Zheng</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3639131</prism:doi><description>Referring Remote Sensing Image Segmentation (RRSIS) is a challenging task that involves segmenting target instances within a top-view image guided by a natural language expression. Existing classic RRSIS methods commonly support target expressions only, i.e., the target described by the expression is present in the image. No-target expressions are excluded. Under this constraint, the model may face significant challenges. For instance, a small error, such as a typographical mistake, could cause a complete failure of the model. To overcome this issue, in this paper, we introduce a new benchmark called Generalized Referring Remote Sensing Image Segmentation (GRRSIS), which extends classic RRSIS by allowing expressions to refer to no-target objects. Towards this, we construct the first large-scale dataset for GRRSIS, called GRRSIS-D, which includes multi-target, single-target, and no-target expressions. Core challenges in GRRSIS stem from the fact that objects in aerial images often occupy only a small number of pixels, exhibit significant orientation variations, and present varying levels of recognition difficulty. To tackle these challenges, we propose an Oriented-aware Multi-Scale Network with an Adaptive Angle Sensing module that integrates Adaptive Rotated Convolution and a gating mechanism to capture diverse object orientations while suppressing irrelevant features for more accurate representations. Additionally, we introduce a novel Online Hard Case Mining Loss, which allocates varying levels of attention to foreground and background regions and reshapes the standard loss by down-weighting well-segmented examples, effectively addressing the issues caused by low pixel occupancy and uneven sample difficulty. The proposed approach achieves state-of-the-art performance on both the newly introduced GRRSIS and classic RRSIS tasks.
Published: 2025-12-01T18:24:33+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.768 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenyu Mi; Jianji Wang; Fuzhen Zhuang; Nanning Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3639131"&gt;10.1109/tgrs.2025.3639131&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.768 (must_read)&lt;/p&gt;
&lt;p&gt;Referring Remote Sensing Image Segmentation (RRSIS) is a challenging task that involves segmenting target instances within a top-view image guided by a natural language expression. Existing classic RRSIS methods commonly support target expressions only, i.e., the target described by the expression is present in the image. No-target expressions are excluded. Under this constraint, the model may face significant challenges. For instance, a small error, such as a typographical mistake, could cause a complete failure of the model. To overcome this issue, in this paper, we introduce a new benchmark called Generalized Referring Remote Sensing Image Segmentation (GRRSIS), which extends classic RRSIS by allowing expressions to refer to no-target objects. Towards this, we construct the first large-scale dataset for GRRSIS, called GRRSIS-D, which includes multi-target, single-target, and no-target expressions. Core challenges in GRRSIS stem from the fact that objects in aerial images often occupy only a small number of pixels, exhibit significant orientation variations, and present varying levels of recognition difficulty. To tackle these challenges, we propose an Oriented-aware Multi-Scale Network with an Adaptive Angle Sensing module that integrates Adaptive Rotated Convolution and a gating mechanism to capture diverse object orientations while suppressing irrelevant features for more accurate representations. Additionally, we introduce a novel Online Hard Case Mining Loss, which allocates varying levels of attention to foreground and background regions and reshapes the standard loss by down-weighting well-segmented examples, effectively addressing the issues caused by low pixel occupancy and uneven sample difficulty. The proposed approach achieves state-of-the-art performance on both the newly introduced GRRSIS and classic RRSIS tasks.&lt;/p&gt;</content:encoded></item><item><title>Artemis: Structured Visual Reasoning for Perception Policy Learning</title><link>https://arxiv.org/abs/2512.01988v1</link><guid>http://arxiv.org/abs/2512.01988v1</guid><pubDate>Mon, 01 Dec 2025 18:45:30 +0000</pubDate><dc:creator>Wei Tang</dc:creator><dc:creator>Yanpeng Sun</dc:creator><dc:creator>Shan Zhang</dc:creator><dc:creator>Xiaofan Li</dc:creator><dc:creator>Piotr Koniusz</dc:creator><dc:creator>Wei Li</dc:creator><dc:creator>Na Zhao</dc:creator><dc:creator>Zechao Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent reinforcement-learning frameworks for visual perception policy have begun to incorporate intermediate reasoning chains expressed in natural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains perform semantic reasoning in an unstructured linguistic space, visual perception requires reasoning in a spatial and object-centric space. In response, we introduce Artemis, a perception-policy learning framework that performs structured proposal-based reasoning, where each intermediate step is represented as a (label, bounding-box) pair capturing a verifiable visual state. This design enables explicit tracking of intermediate states, direct supervision for proposal quality, and avoids ambiguity introduced by language-based reasoning. Artemis is built on Qwen2.5-VL-3B, achieves strong performance on grounding and detection task and exhibits substantial generalization to counting and geometric-perception tasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on general MLLM benchmarks, illustrating that spatially grounded reasoning provides a principled route toward scalable and general perception policies.
Published: 2025-12-01T18:45:30+00:00
Venue: arXiv
Score: 0.766 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wei Tang; Yanpeng Sun; Shan Zhang; Xiaofan Li; Piotr Koniusz; Wei Li; Na Zhao; Zechao Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.766 (must_read)&lt;/p&gt;
&lt;p&gt;Recent reinforcement-learning frameworks for visual perception policy have begun to incorporate intermediate reasoning chains expressed in natural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains perform semantic reasoning in an unstructured linguistic space, visual perception requires reasoning in a spatial and object-centric space. In response, we introduce Artemis, a perception-policy learning framework that performs structured proposal-based reasoning, where each intermediate step is represented as a (label, bounding-box) pair capturing a verifiable visual state. This design enables explicit tracking of intermediate states, direct supervision for proposal quality, and avoids ambiguity introduced by language-based reasoning. Artemis is built on Qwen2.5-VL-3B, achieves strong performance on grounding and detection task and exhibits substantial generalization to counting and geometric-perception tasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on general MLLM benchmarks, illustrating that spatially grounded reasoning provides a principled route toward scalable and general perception policies.&lt;/p&gt;</content:encoded></item><item><title>S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance</title><link>https://arxiv.org/abs/2512.01223v1</link><guid>http://arxiv.org/abs/2512.01223v1</guid><pubDate>Mon, 01 Dec 2025 03:08:34 +0000</pubDate><dc:creator>Beining Xu</dc:creator><dc:creator>Siting Zhu</dc:creator><dc:creator>Zhao Jin</dc:creator><dc:creator>Junxian Li</dc:creator><dc:creator>Hesheng Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>3D Visual Grounding (3DVG) focuses on locating objects in 3D scenes based on natural language descriptions, serving as a fundamental task for embodied AI and robotics. Recent advances in Multi-modal Large Language Models (MLLMs) have motivated research into extending them to 3DVG. However, MLLMs primarily process 2D visual inputs and struggle with understanding 3D spatial structure of scenes solely from these limited perspectives. Existing methods mainly utilize viewpoint-dependent rendering of reconstructed point clouds to provide explicit structural guidance for MLLMs in 3DVG tasks, leading to inefficiency and limited spatial reasoning. To address this issue, we propose S$^2$-MLLM, an efficient framework that enhances spatial reasoning in MLLMs through implicit spatial reasoning. We introduce a spatial guidance strategy that leverages the structure awareness of feed-forward 3D reconstruction. By acquiring 3D structural understanding during training, our model can implicitly reason about 3D scenes without relying on inefficient point cloud reconstruction. Moreover, we propose a structure-enhanced module (SE), which first employs intra-view and inter-view attention mechanisms to capture dependencies within views and correspondences across views. The module further integrates multi-level position encoding to associate visual representations with spatial positions and viewpoint information, enabling more accurate structural understanding. Extensive experiments demonstrate that S$^2$-MLLM unifies superior performance, generalization, and efficiency, achieving significant performance over existing methods across the ScanRefer, Nr3D, and Sr3D datasets. Code will be available upon acceptance.
Published: 2025-12-01T03:08:34+00:00
Venue: arXiv
Score: 0.766 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Beining Xu; Siting Zhu; Zhao Jin; Junxian Li; Hesheng Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.766 (must_read)&lt;/p&gt;
&lt;p&gt;3D Visual Grounding (3DVG) focuses on locating objects in 3D scenes based on natural language descriptions, serving as a fundamental task for embodied AI and robotics. Recent advances in Multi-modal Large Language Models (MLLMs) have motivated research into extending them to 3DVG. However, MLLMs primarily process 2D visual inputs and struggle with understanding 3D spatial structure of scenes solely from these limited perspectives. Existing methods mainly utilize viewpoint-dependent rendering of reconstructed point clouds to provide explicit structural guidance for MLLMs in 3DVG tasks, leading to inefficiency and limited spatial reasoning. To address this issue, we propose S$^2$-MLLM, an efficient framework that enhances spatial reasoning in MLLMs through implicit spatial reasoning. We introduce a spatial guidance strategy that leverages the structure awareness of feed-forward 3D reconstruction. By acquiring 3D structural understanding during training, our model can implicitly reason about 3D scenes without relying on inefficient point cloud reconstruction. Moreover, we propose a structure-enhanced module (SE), which first employs intra-view and inter-view attention mechanisms to capture dependencies within views and correspondences across views. The module further integrates multi-level position encoding to associate visual representations with spatial positions and viewpoint information, enabling more accurate structural understanding. Extensive experiments demonstrate that S$^2$-MLLM unifies superior performance, generalization, and efficiency, achieving significant performance over existing methods across the ScanRefer, Nr3D, and Sr3D datasets. Code will be available upon acceptance.&lt;/p&gt;</content:encoded></item><item><title>DiR-Net: A Diagnostic and Iterative Rectification Network for Cross-Modal 3D Object Detection</title><link>https://doi.org/10.1016/j.knosys.2025.115023</link><guid>10.1016/j.knosys.2025.115023</guid><pubDate>Tue, 02 Dec 2025 07:54:22 +0000</pubDate><dc:creator>Miaohui Zhang</dc:creator><dc:creator>Shuang Wang</dc:creator><dc:creator>Kunpeng Bi</dc:creator><dc:creator>Ming Xin</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115023</prism:doi><description>Accurate perception of indoor scenes is a cornerstone of embodied intelligence. The core challenge in multi-modal 3D detection lies in achieving precise cross-modal feature alignment. However, existing methods typically rely on static projection, which prevents them from iteratively correcting alignment errors caused by occlusions or calibration inaccuracies, and they lack a mechanism to dynamically allocate computational resources based on fusion quality. To address these limitations, we propose a Diagnostic and Iterative Rectification Network (DiR-Net), which redefines the fusion process from static matching to a dynamic, confidence-based error correction procedure. Our core insight is that fusion quality is quantifiable: a Diagnostic Decision Module (DDM) analyzes the vector differences among initial 3D, 2D, and post-fusion features to compute an alignment confidence score that acts as an intelligent gate, adaptively balancing performance and efficiency. If optimization is deemed necessary, an Iterative Rectification Framework (IRF) module is activated to perform K rounds of refinement. In each iteration, unlike approaches that rely on implicit attention, our Rectification Regression Module (RRM) leverages the current fusion state and 3D geometric context to explicitly regress correction vectors for the 2D sampling coordinates, optimizing alignment with sub-pixel precision. Subsequently, an Internal Fusion Module (IFM) facilitates deep informational complementarity by generating cross-modal context to modulate the 2D and 3D feature streams. Experiments on the SUN RGB-D dataset demonstrate DiR-Net achieves a state-of-the-art performance of 70.68 mAP@0.25, establishing a new record on the benchmark. Our work pioneers a paradigm shift in multi-modal fusion, from one-shot fusion to adaptive, iterative error correction.
Published: 2025-12-02T07:54:22+00:00
Venue: Knowledge-Based Systems
Score: 0.765 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Miaohui Zhang; Shuang Wang; Kunpeng Bi; Ming Xin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115023"&gt;10.1016/j.knosys.2025.115023&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (must_read)&lt;/p&gt;
&lt;p&gt;Accurate perception of indoor scenes is a cornerstone of embodied intelligence. The core challenge in multi-modal 3D detection lies in achieving precise cross-modal feature alignment. However, existing methods typically rely on static projection, which prevents them from iteratively correcting alignment errors caused by occlusions or calibration inaccuracies, and they lack a mechanism to dynamically allocate computational resources based on fusion quality. To address these limitations, we propose a Diagnostic and Iterative Rectification Network (DiR-Net), which redefines the fusion process from static matching to a dynamic, confidence-based error correction procedure. Our core insight is that fusion quality is quantifiable: a Diagnostic Decision Module (DDM) analyzes the vector differences among initial 3D, 2D, and post-fusion features to compute an alignment confidence score that acts as an intelligent gate, adaptively balancing performance and efficiency. If optimization is deemed necessary, an Iterative Rectification Framework (IRF) module is activated to perform K rounds of refinement. In each iteration, unlike approaches that rely on implicit attention, our Rectification Regression Module (RRM) leverages the current fusion state and 3D geometric context to explicitly regress correction vectors for the 2D sampling coordinates, optimizing alignment with sub-pixel precision. Subsequently, an Internal Fusion Module (IFM) facilitates deep informational complementarity by generating cross-modal context to modulate the 2D and 3D feature streams. Experiments on the SUN RGB-D dataset demonstrate DiR-Net achieves a state-of-the-art performance of 70.68 mAP@0.25, establishing a new record on the benchmark. Our work pioneers a paradigm shift in multi-modal fusion, from one-shot fusion to adaptive, iterative error correction.&lt;/p&gt;</content:encoded></item><item><title>CC-FMO: Camera-Conditioned Zero-Shot Single Image to 3D Scene Generation with Foundation Model Orchestration</title><link>https://arxiv.org/abs/2512.00493v1</link><guid>http://arxiv.org/abs/2512.00493v1</guid><pubDate>Sat, 29 Nov 2025 14:01:13 +0000</pubDate><dc:creator>Boshi Tang</dc:creator><dc:creator>Henry Zheng</dc:creator><dc:creator>Rui Huang</dc:creator><dc:creator>Gao Huang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>High-quality 3D scene generation from a single image is crucial for AR/VR and embodied AI applications. Early approaches struggle to generalize due to reliance on specialized models trained on curated small datasets. While recent advancements in large-scale 3D foundation models have significantly enhanced instance-level generation, coherent scene generation remains a challenge, where performance is limited by inaccurate per-object pose estimations and spatial inconsistency. To this end, this paper introduces CC-FMO, a zero-shot, camera-conditioned pipeline for single-image to 3D scene generation that jointly conforms to the object layout in input image and preserves instance fidelity. CC-FMO employs a hybrid instance generator that combines semantics-aware vector-set representation with detail-rich structured latent representation, yielding object geometries that are both semantically plausible and high-quality. Furthermore, CC-FMO enables the application of foundational pose estimation models in the scene generation task via a simple yet effective camera-conditioned scale-solving algorithm, to enforce scene-level coherence. Extensive experiments demonstrate that CC-FMO consistently generates high-fidelity camera-aligned compositional scenes, outperforming all state-of-the-art methods.
Published: 2025-11-29T14:01:13+00:00
Venue: arXiv
Score: 0.764 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Boshi Tang; Henry Zheng; Rui Huang; Gao Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.764 (must_read)&lt;/p&gt;
&lt;p&gt;High-quality 3D scene generation from a single image is crucial for AR/VR and embodied AI applications. Early approaches struggle to generalize due to reliance on specialized models trained on curated small datasets. While recent advancements in large-scale 3D foundation models have significantly enhanced instance-level generation, coherent scene generation remains a challenge, where performance is limited by inaccurate per-object pose estimations and spatial inconsistency. To this end, this paper introduces CC-FMO, a zero-shot, camera-conditioned pipeline for single-image to 3D scene generation that jointly conforms to the object layout in input image and preserves instance fidelity. CC-FMO employs a hybrid instance generator that combines semantics-aware vector-set representation with detail-rich structured latent representation, yielding object geometries that are both semantically plausible and high-quality. Furthermore, CC-FMO enables the application of foundational pose estimation models in the scene generation task via a simple yet effective camera-conditioned scale-solving algorithm, to enforce scene-level coherence. Extensive experiments demonstrate that CC-FMO consistently generates high-fidelity camera-aligned compositional scenes, outperforming all state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Geometry Gated Multi-view Stereo for 3D Reconstruction</title><link>https://doi.org/10.1016/j.neucom.2025.132264</link><guid>10.1016/j.neucom.2025.132264</guid><pubDate>Mon, 01 Dec 2025 16:08:04 +0000</pubDate><dc:creator>Han Li</dc:creator><dc:creator>Guohua Gou</dc:creator><dc:creator>Hao Zhang</dc:creator><dc:creator>Weicheng Jiang</dc:creator><dc:creator>Haigang Sui</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132264</prism:doi><description>Multi-view stereo (MVS) aims to reconstruct accurate 3D scenes from multiple images. Currently, deep learning-based MVS methods typically estimate depth maps by regressing cost volumes. Therefore, the accuracy of geometric information encoded in the cost volume and the aggregation methods are crucial to the performance of MVS reconstruction. However, existing approaches lack sufficient optimization in cost volume construction and interaction. Moreover, conventional 3D convolutions often result in high computational complexity.
To address these challenges, this work proposes a Geometry-gated Multi-view Stereo Network (GGMVS), aiming to optimize feature representation in cost volume construction and the cost volume fusion mechanism, thereby improving both the accuracy and efficiency of MVS reconstruction. First, we design a Geometric Matching Enhancement Network (GME) to optimize the quality of cost volume construction. GME captures fine-grained features from multiple views and achieves dynamic feature propagation in a top-down manner. Second, we introduce a Cross-attention Volume Fusion Module (CVF) to strengthen inter-scale cost volume interactions. CVF leverages a cross-attention mechanism to globally integrate information from cost volumes at different scales, facilitating effective multi-scale geometric information fusion. Finally, we propose a Gated Volume Fusion Module (GVF) to enable refined filtering of cost volume information. GVF generates gating signals to dynamically filter and integrate high-confidence information from different cost volumes, providing precise inputs for the aggregation unit.
Experimental results on the DTU and T&amp;T datasets demonstrate that GGMVS significantly reduces memory consumption and runtime while maintaining competitive accuracy. Furthermore, validation on the ETH3D dataset further confirms the excellent generalization capability of GGMVS.
Published: 2025-12-01T16:08:04+00:00
Venue: Neurocomputing
Score: 0.764 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Han Li; Guohua Gou; Hao Zhang; Weicheng Jiang; Haigang Sui&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132264"&gt;10.1016/j.neucom.2025.132264&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.764 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-view stereo (MVS) aims to reconstruct accurate 3D scenes from multiple images. Currently, deep learning-based MVS methods typically estimate depth maps by regressing cost volumes. Therefore, the accuracy of geometric information encoded in the cost volume and the aggregation methods are crucial to the performance of MVS reconstruction. However, existing approaches lack sufficient optimization in cost volume construction and interaction. Moreover, conventional 3D convolutions often result in high computational complexity.
To address these challenges, this work proposes a Geometry-gated Multi-view Stereo Network (GGMVS), aiming to optimize feature representation in cost volume construction and the cost volume fusion mechanism, thereby improving both the accuracy and efficiency of MVS reconstruction. First, we design a Geometric Matching Enhancement Network (GME) to optimize the quality of cost volume construction. GME captures fine-grained features from multiple views and achieves dynamic feature propagation in a top-down manner. Second, we introduce a Cross-attention Volume Fusion Module (CVF) to strengthen inter-scale cost volume interactions. CVF leverages a cross-attention mechanism to globally integrate information from cost volumes at different scales, facilitating effective multi-scale geometric information fusion. Finally, we propose a Gated Volume Fusion Module (GVF) to enable refined filtering of cost volume information. GVF generates gating signals to dynamically filter and integrate high-confidence information from different cost volumes, providing precise inputs for the aggregation unit.
Experimental results on the DTU and T&amp;amp;T datasets demonstrate that GGMVS significantly reduces memory consumption and runtime while maintaining competitive accuracy. Furthermore, validation on the ETH3D dataset further confirms the excellent generalization capability of GGMVS.&lt;/p&gt;</content:encoded></item><item><title>EAG3R: Event-Augmented 3D Geometry Estimation for Dynamic and Extreme-Lighting Scenes</title><link>https://arxiv.org/abs/2512.00771v1</link><guid>http://arxiv.org/abs/2512.00771v1</guid><pubDate>Sun, 30 Nov 2025 08:05:28 +0000</pubDate><dc:creator>Xiaoshan Wu</dc:creator><dc:creator>Yifei Yu</dc:creator><dc:creator>Xiaoyang Lyu</dc:creator><dc:creator>Yihua Huang</dc:creator><dc:creator>Bo Wang</dc:creator><dc:creator>Baoheng Zhang</dc:creator><dc:creator>Zhongrui Wang</dc:creator><dc:creator>Xiaojuan Qi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Robust 3D geometry estimation from videos is critical for applications such as autonomous navigation, SLAM, and 3D scene reconstruction. Recent methods like DUSt3R demonstrate that regressing dense pointmaps from image pairs enables accurate and efficient pose-free reconstruction. However, existing RGB-only approaches struggle under real-world conditions involving dynamic objects and extreme illumination, due to the inherent limitations of conventional cameras. In this paper, we propose EAG3R, a novel geometry estimation framework that augments pointmap-based reconstruction with asynchronous event streams. Built upon the MonST3R backbone, EAG3R introduces two key innovations: (1) a retinex-inspired image enhancement module and a lightweight event adapter with SNR-aware fusion mechanism that adaptively combines RGB and event features based on local reliability; and (2) a novel event-based photometric consistency loss that reinforces spatiotemporal coherence during global optimization. Our method enables robust geometry estimation in challenging dynamic low-light scenes without requiring retraining on night-time data. Extensive experiments demonstrate that EAG3R significantly outperforms state-of-the-art RGB-only baselines across monocular depth estimation, camera pose tracking, and dynamic reconstruction tasks.
Published: 2025-11-30T08:05:28+00:00
Venue: arXiv
Score: 0.763 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoshan Wu; Yifei Yu; Xiaoyang Lyu; Yihua Huang; Bo Wang; Baoheng Zhang; Zhongrui Wang; Xiaojuan Qi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.763 (must_read)&lt;/p&gt;
&lt;p&gt;Robust 3D geometry estimation from videos is critical for applications such as autonomous navigation, SLAM, and 3D scene reconstruction. Recent methods like DUSt3R demonstrate that regressing dense pointmaps from image pairs enables accurate and efficient pose-free reconstruction. However, existing RGB-only approaches struggle under real-world conditions involving dynamic objects and extreme illumination, due to the inherent limitations of conventional cameras. In this paper, we propose EAG3R, a novel geometry estimation framework that augments pointmap-based reconstruction with asynchronous event streams. Built upon the MonST3R backbone, EAG3R introduces two key innovations: (1) a retinex-inspired image enhancement module and a lightweight event adapter with SNR-aware fusion mechanism that adaptively combines RGB and event features based on local reliability; and (2) a novel event-based photometric consistency loss that reinforces spatiotemporal coherence during global optimization. Our method enables robust geometry estimation in challenging dynamic low-light scenes without requiring retraining on night-time data. Extensive experiments demonstrate that EAG3R significantly outperforms state-of-the-art RGB-only baselines across monocular depth estimation, camera pose tracking, and dynamic reconstruction tasks.&lt;/p&gt;</content:encoded></item><item><title>RS-ISRefiner: Towards Better Adapting Vision Foundation Models for Interactive Segmentation of Remote Sensing Images</title><link>https://arxiv.org/abs/2512.00718v1</link><guid>http://arxiv.org/abs/2512.00718v1</guid><pubDate>Sun, 30 Nov 2025 04:12:43 +0000</pubDate><dc:creator>Deliang Wang</dc:creator><dc:creator>Peng Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Interactive image segmentation(IIS) plays a critical role in generating precise annotations for remote sensing imagery, where objects often exhibit scale variations, irregular boundaries and complex backgrounds. However, existing IIS methods, primarily designed for natural images, struggle to generalize to remote sensing domains due to limited annotated data and computational overhead. To address these challenges, we proposed RS-ISRefiner, a novel click-based IIS framework tailored for remote sensing images. The framework employs an adapter-based tuning strategy that preserves the general representations of Vision Foundation Models while enabling efficient learning of remote sensing-specific spatial and boundary characteristics. A hybrid attention mechanism integrating convolutional local modeling with Transformer-based global reasoning enhances robustness against scale diversity and scene complexity. Furthermore, an improved probability map modulation scheme effectively incorporates historical user interactions, yielding more stable iterative refinement and higher boundary fidelity. Comprehensive experiments on six remote sensing datasets, including iSAID, ISPRS Potsdam, SandBar, NWPU, LoveDA Urban and WHUBuilding, demonstrate that RS-ISRefiner consistently outperforms state-of-the-art IIS methods in terms of segmentation accuracy, efficiency and interaction cost. These results confirm the effectiveness and generalizability of our framework, making it highly suitable for high-quality instance segmentation in practical remote sensing scenarios.
Published: 2025-11-30T04:12:43+00:00
Venue: arXiv
Score: 0.763 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Deliang Wang; Peng Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.763 (must_read)&lt;/p&gt;
&lt;p&gt;Interactive image segmentation(IIS) plays a critical role in generating precise annotations for remote sensing imagery, where objects often exhibit scale variations, irregular boundaries and complex backgrounds. However, existing IIS methods, primarily designed for natural images, struggle to generalize to remote sensing domains due to limited annotated data and computational overhead. To address these challenges, we proposed RS-ISRefiner, a novel click-based IIS framework tailored for remote sensing images. The framework employs an adapter-based tuning strategy that preserves the general representations of Vision Foundation Models while enabling efficient learning of remote sensing-specific spatial and boundary characteristics. A hybrid attention mechanism integrating convolutional local modeling with Transformer-based global reasoning enhances robustness against scale diversity and scene complexity. Furthermore, an improved probability map modulation scheme effectively incorporates historical user interactions, yielding more stable iterative refinement and higher boundary fidelity. Comprehensive experiments on six remote sensing datasets, including iSAID, ISPRS Potsdam, SandBar, NWPU, LoveDA Urban and WHUBuilding, demonstrate that RS-ISRefiner consistently outperforms state-of-the-art IIS methods in terms of segmentation accuracy, efficiency and interaction cost. These results confirm the effectiveness and generalizability of our framework, making it highly suitable for high-quality instance segmentation in practical remote sensing scenarios.&lt;/p&gt;</content:encoded></item><item><title>Multilingual Training-Free Remote Sensing Image Captioning</title><link>https://arxiv.org/abs/2512.00887v1</link><guid>http://arxiv.org/abs/2512.00887v1</guid><pubDate>Sun, 30 Nov 2025 13:16:42 +0000</pubDate><dc:creator>Carlos Rebelo</dc:creator><dc:creator>Gil Rocha</dc:creator><dc:creator>João Daniel Silva</dc:creator><dc:creator>Bruno Martins</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Remote sensing image captioning has advanced rapidly through encoder--decoder models, although the reliance on large annotated datasets and the focus on English restricts global applicability. To address these limitations, we propose the first training-free multilingual approach, based on retrieval-augmented prompting. For a given aerial image, we employ a domain-adapted SigLIP2 encoder to retrieve related captions and few-shot examples from a datastore, which are then provided to a language model. We explore two variants: an image-blind setup, where a multilingual Large Language Model (LLM) generates the caption from textual prompts alone, and an image-aware setup, where a Vision--Language Model (VLM) jointly processes the prompt and the input image. To improve the coherence of the retrieved content, we introduce a graph-based re-ranking strategy using PageRank on a graph of images and captions. Experiments on four benchmark datasets across ten languages demonstrate that our approach is competitive with fully supervised English-only systems and generalizes to other languages. Results also highlight the importance of re-ranking with PageRank, yielding up to 35% improvements in performance metrics. Additionally, it was observed that while VLMs tend to generate visually grounded but lexically diverse captions, LLMs can achieve stronger BLEU and CIDEr scores. Lastly, directly generating captions in the target language consistently outperforms other translation-based strategies. Overall, our work delivers one of the first systematic evaluations of multilingual, training-free captioning for remote sensing imagery, advancing toward more inclusive and scalable multimodal Earth observation systems.
Published: 2025-11-30T13:16:42+00:00
Venue: arXiv
Score: 0.762 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Carlos Rebelo; Gil Rocha; João Daniel Silva; Bruno Martins&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.762 (must_read)&lt;/p&gt;
&lt;p&gt;Remote sensing image captioning has advanced rapidly through encoder--decoder models, although the reliance on large annotated datasets and the focus on English restricts global applicability. To address these limitations, we propose the first training-free multilingual approach, based on retrieval-augmented prompting. For a given aerial image, we employ a domain-adapted SigLIP2 encoder to retrieve related captions and few-shot examples from a datastore, which are then provided to a language model. We explore two variants: an image-blind setup, where a multilingual Large Language Model (LLM) generates the caption from textual prompts alone, and an image-aware setup, where a Vision--Language Model (VLM) jointly processes the prompt and the input image. To improve the coherence of the retrieved content, we introduce a graph-based re-ranking strategy using PageRank on a graph of images and captions. Experiments on four benchmark datasets across ten languages demonstrate that our approach is competitive with fully supervised English-only systems and generalizes to other languages. Results also highlight the importance of re-ranking with PageRank, yielding up to 35% improvements in performance metrics. Additionally, it was observed that while VLMs tend to generate visually grounded but lexically diverse captions, LLMs can achieve stronger BLEU and CIDEr scores. Lastly, directly generating captions in the target language consistently outperforms other translation-based strategies. Overall, our work delivers one of the first systematic evaluations of multilingual, training-free captioning for remote sensing imagery, advancing toward more inclusive and scalable multimodal Earth observation systems.&lt;/p&gt;</content:encoded></item><item><title>LumiX: Structured and Coherent Text-to-Intrinsic Generation</title><link>https://arxiv.org/abs/2512.02781v1</link><guid>http://arxiv.org/abs/2512.02781v1</guid><pubDate>Tue, 02 Dec 2025 13:56:02 +0000</pubDate><dc:creator>Xu Han</dc:creator><dc:creator>Biao Zhang</dc:creator><dc:creator>Xiangjun Tang</dc:creator><dc:creator>Xianzhi Li</dc:creator><dc:creator>Peter Wonka</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present LumiX, a structured diffusion framework for coherent text-to-intrinsic generation. Conditioned on text prompts, LumiX jointly generates a comprehensive set of intrinsic maps (e.g., albedo, irradiance, normal, depth, and final color), providing a structured and physically consistent description of an underlying scene. This is enabled by two key contributions: 1) Query-Broadcast Attention, a mechanism that ensures structural consistency by sharing queries across all maps in each self-attention block. 2) Tensor LoRA, a tensor-based adaptation that parameter-efficiently models cross-map relations for efficient joint training. Together, these designs enable stable joint diffusion training and unified generation of multiple intrinsic properties. Experiments show that LumiX produces coherent and physically meaningful results, achieving 23% higher alignment and a better preference score (0.19 vs. -0.41) compared to the state of the art, and it can also perform image-conditioned intrinsic decomposition within the same framework.
Published: 2025-12-02T13:56:02+00:00
Venue: arXiv
Score: 0.762 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xu Han; Biao Zhang; Xiangjun Tang; Xianzhi Li; Peter Wonka&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.762 (must_read)&lt;/p&gt;
&lt;p&gt;We present LumiX, a structured diffusion framework for coherent text-to-intrinsic generation. Conditioned on text prompts, LumiX jointly generates a comprehensive set of intrinsic maps (e.g., albedo, irradiance, normal, depth, and final color), providing a structured and physically consistent description of an underlying scene. This is enabled by two key contributions: 1) Query-Broadcast Attention, a mechanism that ensures structural consistency by sharing queries across all maps in each self-attention block. 2) Tensor LoRA, a tensor-based adaptation that parameter-efficiently models cross-map relations for efficient joint training. Together, these designs enable stable joint diffusion training and unified generation of multiple intrinsic properties. Experiments show that LumiX produces coherent and physically meaningful results, achieving 23% higher alignment and a better preference score (0.19 vs. -0.41) compared to the state of the art, and it can also perform image-conditioned intrinsic decomposition within the same framework.&lt;/p&gt;</content:encoded></item><item><title>Cross-Scene Hyperspectral Image Classification Network with Dynamic Perturbation and Self-Knowledge Distillation</title><link>https://doi.org/10.1109/tgrs.2025.3639448</link><guid>10.1109/tgrs.2025.3639448</guid><pubDate>Tue, 02 Dec 2025 18:48:15 +0000</pubDate><dc:creator>Yuhang Hong</dc:creator><dc:creator>Zhixi Feng</dc:creator><dc:creator>Shuyuan Yang</dc:creator><dc:creator>Zhihao Chang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3639448</prism:doi><description>Cross-scene hyperspectral image (HSI) classification faces spectral-spatial feature distribution shifts resulting from cross-domain heterogeneity, which has become a critical challenge that urgently needs resolution in the field of remote sensing intelligent interpretation. To address this distribution shift, the mainstream approach is Domain Generalization (DG). However, existing HSI DG methods primarily focus on inter-class separability, while paying relatively less attention to cross-domain transferability. To overcome the limitation that existing methods mainly focus on inter-class separability, this study proposes a cross-scene HSI classification network, termed DPSKDnet. By synergistically employing dynamic perturbation-based destylization and self-knowledge distillation modeling mechanisms, DPSKDnet constructs domain-invariant representations with strong generalization capabilities in the feature space. Specifically, this study first builds a generator based on dynamic perturbation destylization to mine source domain (SD) invariant features and generate extended domain (ED) samples. Subsequently, a Fourier Augmentation Module is utilized to optimize the frequency domain representations of the SD, ED, and their combination-generated intermediate domain, obtaining frequency-enhanced representations. To effectively improve the model’s ability to capture domain-invariant features, a sample pair distillation loss is devised. This loss, informed by multi-domain mixed data input, guides the discriminator in online self-supervised learning. The overall accuracy of this method on Loukia, Houston2018, and Pavia Center increased by 0.48%, 0.81%, and 1.5%, respectively, compared to state-of-the-art methods. The code is available on the website: https://github.com/Yuhang-Hong/TGRS_DPSKDnet.
Published: 2025-12-02T18:48:15+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.760 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuhang Hong; Zhixi Feng; Shuyuan Yang; Zhihao Chang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3639448"&gt;10.1109/tgrs.2025.3639448&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.760 (must_read)&lt;/p&gt;
&lt;p&gt;Cross-scene hyperspectral image (HSI) classification faces spectral-spatial feature distribution shifts resulting from cross-domain heterogeneity, which has become a critical challenge that urgently needs resolution in the field of remote sensing intelligent interpretation. To address this distribution shift, the mainstream approach is Domain Generalization (DG). However, existing HSI DG methods primarily focus on inter-class separability, while paying relatively less attention to cross-domain transferability. To overcome the limitation that existing methods mainly focus on inter-class separability, this study proposes a cross-scene HSI classification network, termed DPSKDnet. By synergistically employing dynamic perturbation-based destylization and self-knowledge distillation modeling mechanisms, DPSKDnet constructs domain-invariant representations with strong generalization capabilities in the feature space. Specifically, this study first builds a generator based on dynamic perturbation destylization to mine source domain (SD) invariant features and generate extended domain (ED) samples. Subsequently, a Fourier Augmentation Module is utilized to optimize the frequency domain representations of the SD, ED, and their combination-generated intermediate domain, obtaining frequency-enhanced representations. To effectively improve the model’s ability to capture domain-invariant features, a sample pair distillation loss is devised. This loss, informed by multi-domain mixed data input, guides the discriminator in online self-supervised learning. The overall accuracy of this method on Loukia, Houston2018, and Pavia Center increased by 0.48%, 0.81%, and 1.5%, respectively, compared to state-of-the-art methods. The code is available on the website: https://github.com/Yuhang-Hong/TGRS_DPSKDnet.&lt;/p&gt;</content:encoded></item><item><title>D-RGCN: Software Defect Prediction Based on Dual Directed Dependency Graph Reconstruction</title><link>https://doi.org/10.1016/j.inffus.2025.104001</link><guid>10.1016/j.inffus.2025.104001</guid><pubDate>Tue, 02 Dec 2025 16:16:42 +0000</pubDate><dc:creator>Qing Shen</dc:creator><dc:creator>Yuanying Lu</dc:creator><dc:creator>Jiacheng Fei</dc:creator><dc:creator>Zhenfang Liu</dc:creator><dc:creator>Jing Xu</dc:creator><dc:creator>Jungang Lou</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104001</prism:doi><description>Graph representation method has been widely applied in software defect prediction in recent years due to its efficiency and adaptability. However, existing works primarily consider undirected relationships among code classes, lacking modeling of directed causal relationships and conditional independence among classes. Moreover, conventional graph neural networks struggle to effectively handle directed graph structures. In light of these challenges, we propose a defect prediction model named D-RGCN, focusing on modeling the external structure features of software programs and dependencies among classes. We restructure the directed class dependency graph corresponding to the code using graph embedding and directed graph reconstruction algorithms, introduce Relationship Graph Convolutional Networks for dual feature completion and fusion, and perform defect prediction. Experimental results on 10 opensource projects demonstrate that the proposed model outperforms the baseline models, with average improvements of 6.8%-24.9%, 8.9%-20.2% and 16.7%-40.0% in terms of AUC, F1-Score, and MCC. Additionally, we also explored the effectiveness of the dual feature completion and fusion mechanism.
Published: 2025-12-02T16:16:42+00:00
Venue: Information Fusion
Score: 0.760 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qing Shen; Yuanying Lu; Jiacheng Fei; Zhenfang Liu; Jing Xu; Jungang Lou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104001"&gt;10.1016/j.inffus.2025.104001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.760 (must_read)&lt;/p&gt;
&lt;p&gt;Graph representation method has been widely applied in software defect prediction in recent years due to its efficiency and adaptability. However, existing works primarily consider undirected relationships among code classes, lacking modeling of directed causal relationships and conditional independence among classes. Moreover, conventional graph neural networks struggle to effectively handle directed graph structures. In light of these challenges, we propose a defect prediction model named D-RGCN, focusing on modeling the external structure features of software programs and dependencies among classes. We restructure the directed class dependency graph corresponding to the code using graph embedding and directed graph reconstruction algorithms, introduce Relationship Graph Convolutional Networks for dual feature completion and fusion, and perform defect prediction. Experimental results on 10 opensource projects demonstrate that the proposed model outperforms the baseline models, with average improvements of 6.8%-24.9%, 8.9%-20.2% and 16.7%-40.0% in terms of AUC, F1-Score, and MCC. Additionally, we also explored the effectiveness of the dual feature completion and fusion mechanism.&lt;/p&gt;</content:encoded></item><item><title>Deep Positional Encoders For Graph Classification</title><link>https://doi.org/10.1016/j.patcog.2025.112828</link><guid>10.1016/j.patcog.2025.112828</guid><pubDate>Tue, 02 Dec 2025 07:47:38 +0000</pubDate><dc:creator>Ahmed Begga</dc:creator><dc:creator>Miguel Ángel Lozano</dc:creator><dc:creator>Francisco Escolano</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112828</prism:doi><description>Structural Pattern Recognition (SPR) includes the study of graphs as encoders of non-sequential and permutation-invariant patterns. In this regard, Graph Neural Networks (GNNs) are paving the way towards ”inductive SPR” where classical structural problems such as graph classification can be approached through learnable priors. However, since graphs do not have a canonical order, existing GNNs struggle to learn the structural role of each node in the graph, which becomes key in graph classification. In this paper, we address this problem by making Spectral Graph Theory ”inductive”, i.e. by learning the eigenvectors of the graph Laplacian, and then using them as positional encoders (PEs). Our experiments show that we improve significantly the SOTA of GNN-based graph classification.
Published: 2025-12-02T07:47:38+00:00
Venue: Pattern Recognition
Score: 0.757 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ahmed Begga; Miguel Ángel Lozano; Francisco Escolano&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112828"&gt;10.1016/j.patcog.2025.112828&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.757 (must_read)&lt;/p&gt;
&lt;p&gt;Structural Pattern Recognition (SPR) includes the study of graphs as encoders of non-sequential and permutation-invariant patterns. In this regard, Graph Neural Networks (GNNs) are paving the way towards ”inductive SPR” where classical structural problems such as graph classification can be approached through learnable priors. However, since graphs do not have a canonical order, existing GNNs struggle to learn the structural role of each node in the graph, which becomes key in graph classification. In this paper, we address this problem by making Spectral Graph Theory ”inductive”, i.e. by learning the eigenvectors of the graph Laplacian, and then using them as positional encoders (PEs). Our experiments show that we improve significantly the SOTA of GNN-based graph classification.&lt;/p&gt;</content:encoded></item><item><title>Unlocking Pseudolabel Potential and Alignment for Unpaired Cross-Modality Adaptation in Remote Sensing Image Segmentation</title><link>https://doi.org/10.1109/tnnls.2025.3635883</link><guid>10.1109/tnnls.2025.3635883</guid><pubDate>Tue, 02 Dec 2025 18:49:00 +0000</pubDate><dc:creator>Zhengyi Xu</dc:creator><dc:creator>Jie Geng</dc:creator><dc:creator>Wen Jiang</dc:creator><dc:creator>Shuai Song</dc:creator><prism:publicationName>IEEE Transactions on Neural Networks and Learning Systems</prism:publicationName><prism:doi>10.1109/tnnls.2025.3635883</prism:doi><description>With the growth of multisource sensor technology, multimodal learning has become pivotal in remote sensing (RS) image segmentation. Despite its potential, current methods face challenges in acquiring large-scale paired samples. When annotated optical images are available, but synthetic aperture radar (SAR) images lack annotations, learning discriminative features for SAR images from optical images becomes difficult. Unsupervised domain adaptation (UDA) offers a potential solution to this challenge, which we refer to as unpaired cross-modality UDA. In this article, we propose unlocking pseudolabel potential and alignment (ULPA) for unpaired cross-modality adaptation in RS image segmentation, a novel one-stage adaptation framework designed to enhance cross-modality knowledge transfer. Our approach employs a prototypical multidomain alignment (PMDA) strategy, which reduces the modality gap through contrastive learning between features and prototypes of identical classes across different modalities. In addition, we introduce the unreliable-sample-guided feature contrast (UFC) loss to address the underutilization of unreliable pixels during training. This strategy separates reliable and unreliable pixels based on prediction confidence, assigning unreliable pixels to a category-wise queue of negative samples, thus ensuring all candidate pixels contribute to the training process. Extensive experiments show that the integration of PMDA and UFC loss can lead to more effective cross-modality domain alignment and substantially boost the model’s generalization capability.
Published: 2025-12-02T18:49:00+00:00
Venue: IEEE Transactions on Neural Networks and Learning Systems
Score: 0.756 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhengyi Xu; Jie Geng; Wen Jiang; Shuai Song&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Neural Networks and Learning Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tnnls.2025.3635883"&gt;10.1109/tnnls.2025.3635883&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.756 (must_read)&lt;/p&gt;
&lt;p&gt;With the growth of multisource sensor technology, multimodal learning has become pivotal in remote sensing (RS) image segmentation. Despite its potential, current methods face challenges in acquiring large-scale paired samples. When annotated optical images are available, but synthetic aperture radar (SAR) images lack annotations, learning discriminative features for SAR images from optical images becomes difficult. Unsupervised domain adaptation (UDA) offers a potential solution to this challenge, which we refer to as unpaired cross-modality UDA. In this article, we propose unlocking pseudolabel potential and alignment (ULPA) for unpaired cross-modality adaptation in RS image segmentation, a novel one-stage adaptation framework designed to enhance cross-modality knowledge transfer. Our approach employs a prototypical multidomain alignment (PMDA) strategy, which reduces the modality gap through contrastive learning between features and prototypes of identical classes across different modalities. In addition, we introduce the unreliable-sample-guided feature contrast (UFC) loss to address the underutilization of unreliable pixels during training. This strategy separates reliable and unreliable pixels based on prediction confidence, assigning unreliable pixels to a category-wise queue of negative samples, thus ensuring all candidate pixels contribute to the training process. Extensive experiments show that the integration of PMDA and UFC loss can lead to more effective cross-modality domain alignment and substantially boost the model’s generalization capability.&lt;/p&gt;</content:encoded></item><item><title>DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling</title><link>https://arxiv.org/abs/2512.03000v1</link><guid>http://arxiv.org/abs/2512.03000v1</guid><pubDate>Tue, 02 Dec 2025 18:24:27 +0000</pubDate><dc:creator>Kairun Wen</dc:creator><dc:creator>Yuzhi Huang</dc:creator><dc:creator>Runyu Chen</dc:creator><dc:creator>Hui Zheng</dc:creator><dc:creator>Yunlong Lin</dc:creator><dc:creator>Panwang Pan</dc:creator><dc:creator>Chenxin Li</dc:creator><dc:creator>Wenyan Cong</dc:creator><dc:creator>Jian Zhang</dc:creator><dc:creator>Junbin Lu</dc:creator><dc:creator>Chenguo Lin</dc:creator><dc:creator>Dilin Wang</dc:creator><dc:creator>Zhicheng Yan</dc:creator><dc:creator>Hongyu Xu</dc:creator><dc:creator>Justin Theiss</dc:creator><dc:creator>Yue Huang</dc:creator><dc:creator>Xinghao Ding</dc:creator><dc:creator>Rakesh Ranjan</dc:creator><dc:creator>Zhiwen Fan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consists of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.
Published: 2025-12-02T18:24:27+00:00
Venue: arXiv
Score: 0.756 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kairun Wen; Yuzhi Huang; Runyu Chen; Hui Zheng; Yunlong Lin; Panwang Pan; Chenxin Li; Wenyan Cong; Jian Zhang; Junbin Lu; Chenguo Lin; Dilin Wang; Zhicheng Yan; Hongyu Xu; Justin Theiss; Yue Huang; Xinghao Ding; Rakesh Ranjan; Zhiwen Fan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.756 (must_read)&lt;/p&gt;
&lt;p&gt;Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consists of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.&lt;/p&gt;</content:encoded></item><item><title>Hierarchical Attention and Dynamic Consistency Filtering for Robust Aerial Multi-view Stereo</title><link>https://doi.org/10.1016/j.knosys.2025.114973</link><guid>10.1016/j.knosys.2025.114973</guid><pubDate>Mon, 01 Dec 2025 16:48:13 +0000</pubDate><dc:creator>Lina Wang</dc:creator><dc:creator>Jiayu Zhang</dc:creator><dc:creator>Ziqing Wang</dc:creator><dc:creator>Shuheng Liu</dc:creator><dc:creator>Jiangfeng She</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.114973</prism:doi><description>Existing deep learning-based multi-view stereo (MVS) methods for aerial imagery primarily rely on conventional convolutional neural network (CNN) for feature extraction. Although CNN are effective at capturing local patterns, their fixed receptive fields constrain cross-scale dependency modeling and global context reasoning, which becomes particularly challenging under the large-scale variations and complex textures characteristic of aerial scenes. To address these issues, we propose HADC-MVSNet, a novel aerial MVS framework that integrates hierarchical attention with a spatially adaptive receptive field design. Specifically, the Dynamic Swin-Transformer Feature Extraction module (DSFE) enables flexible adaptation to diverse aerial datasets while effectively modeling both global and cross-scale context. In addition, Feature Fusion with Deformable Convolution (FFDC) are incorporated to enhance local geometric modeling, thereby improving the adaptability of feature fusion and strengthening cross-view alignment. Furthermore, HADC-MVSNet introduces a Multi-stage photometric consistency with geometric consistency module (MPG). By combining multi-view photometric and geometric constraints to refine depth map fusion, it adaptively suppresses outliers and enhances robustness under varying scene complexities. As a result, this method achieves more reliable depth fusion, higher-quality point screening, and ultimately, more complete and accurate 3D point cloud reconstruction. Extensive experiments on multiple aerial image datasets demonstrate that HADC-MVSNet outperforms state-of-the-art methods in both depth estimation accuracy and reconstruction robustness, confirming its effectiveness and practicality in robust aerial multi-view stereo vision.
Published: 2025-12-01T16:48:13+00:00
Venue: Knowledge-Based Systems
Score: 0.756 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lina Wang; Jiayu Zhang; Ziqing Wang; Shuheng Liu; Jiangfeng She&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.114973"&gt;10.1016/j.knosys.2025.114973&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.756 (must_read)&lt;/p&gt;
&lt;p&gt;Existing deep learning-based multi-view stereo (MVS) methods for aerial imagery primarily rely on conventional convolutional neural network (CNN) for feature extraction. Although CNN are effective at capturing local patterns, their fixed receptive fields constrain cross-scale dependency modeling and global context reasoning, which becomes particularly challenging under the large-scale variations and complex textures characteristic of aerial scenes. To address these issues, we propose HADC-MVSNet, a novel aerial MVS framework that integrates hierarchical attention with a spatially adaptive receptive field design. Specifically, the Dynamic Swin-Transformer Feature Extraction module (DSFE) enables flexible adaptation to diverse aerial datasets while effectively modeling both global and cross-scale context. In addition, Feature Fusion with Deformable Convolution (FFDC) are incorporated to enhance local geometric modeling, thereby improving the adaptability of feature fusion and strengthening cross-view alignment. Furthermore, HADC-MVSNet introduces a Multi-stage photometric consistency with geometric consistency module (MPG). By combining multi-view photometric and geometric constraints to refine depth map fusion, it adaptively suppresses outliers and enhances robustness under varying scene complexities. As a result, this method achieves more reliable depth fusion, higher-quality point screening, and ultimately, more complete and accurate 3D point cloud reconstruction. Extensive experiments on multiple aerial image datasets demonstrate that HADC-MVSNet outperforms state-of-the-art methods in both depth estimation accuracy and reconstruction robustness, confirming its effectiveness and practicality in robust aerial multi-view stereo vision.&lt;/p&gt;</content:encoded></item><item><title>GeoBridge: A Semantic-Anchored Multi-View Foundation Model Bridging Images and Text for Geo-Localization</title><link>https://arxiv.org/abs/2512.02697v1</link><guid>http://arxiv.org/abs/2512.02697v1</guid><pubDate>Tue, 02 Dec 2025 12:28:22 +0000</pubDate><dc:creator>Zixuan Song</dc:creator><dc:creator>Jing Zhang</dc:creator><dc:creator>Di Wang</dc:creator><dc:creator>Zidie Zhou</dc:creator><dc:creator>Wenbin Liu</dc:creator><dc:creator>Haonan Guo</dc:creator><dc:creator>En Wang</dc:creator><dc:creator>Bo Du</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image. However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable. It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image). To address these challenges, we propose GeoBridge, a foundation model that performs bidirectional matching across views and supports language-to-image retrieval. Going beyond traditional satellite-centric formulations, GeoBridge builds on a novel semantic-anchor mechanism that bridges multi-view features through textual descriptions for robust, flexible localization. In support of this task, we construct GeoLoc, the first large-scale, cross-modal, and multi-view aligned dataset comprising over 50,000 pairs of drone, street-view panorama, and satellite images as well as their textual descriptions, collected from 36 countries, ensuring both geographic and semantic alignment. We performed broad evaluations across multiple tasks. Experiments confirm that GeoLoc pre-training markedly improves geo-location accuracy for GeoBridge while promoting cross-domain generalization and cross-modal knowledge transfer. The dataset, source code, and pretrained models were released at https://github.com/MiliLab/GeoBridge.
Published: 2025-12-02T12:28:22+00:00
Venue: arXiv
Score: 0.755 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zixuan Song; Jing Zhang; Di Wang; Zidie Zhou; Wenbin Liu; Haonan Guo; En Wang; Bo Du&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.755 (must_read)&lt;/p&gt;
&lt;p&gt;Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image. However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable. It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image). To address these challenges, we propose GeoBridge, a foundation model that performs bidirectional matching across views and supports language-to-image retrieval. Going beyond traditional satellite-centric formulations, GeoBridge builds on a novel semantic-anchor mechanism that bridges multi-view features through textual descriptions for robust, flexible localization. In support of this task, we construct GeoLoc, the first large-scale, cross-modal, and multi-view aligned dataset comprising over 50,000 pairs of drone, street-view panorama, and satellite images as well as their textual descriptions, collected from 36 countries, ensuring both geographic and semantic alignment. We performed broad evaluations across multiple tasks. Experiments confirm that GeoLoc pre-training markedly improves geo-location accuracy for GeoBridge while promoting cross-domain generalization and cross-modal knowledge transfer. The dataset, source code, and pretrained models were released at https://github.com/MiliLab/GeoBridge.&lt;/p&gt;</content:encoded></item></channel></rss>