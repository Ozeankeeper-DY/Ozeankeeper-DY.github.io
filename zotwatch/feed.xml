<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Fri, 05 Dec 2025 04:18:47 +0000</lastBuildDate><item><title>UrbanMMCL: Urban region representations via multi-modal and multi-graph self-supervised contrastive learning</title><link>https://doi.org/10.1016/j.isprsjprs.2025.11.012</link><guid>10.1016/j.isprsjprs.2025.11.012</guid><pubDate>Wed, 03 Dec 2025 23:57:12 +0000</pubDate><dc:creator>Jinzhou Cao</dc:creator><dc:creator>Jiashi Chen</dc:creator><dc:creator>Xiangxu Wang</dc:creator><dc:creator>Weiming Huang</dc:creator><dc:creator>Dongsheng Chen</dc:creator><dc:creator>Tianhong Zhao</dc:creator><dc:creator>Wei Tu</dc:creator><dc:creator>Qingquan Li</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.11.012</prism:doi><description>Urban region representation learning has emerged as a fundamental approach for diverse urban analytics tasks, where each neighborhood is encoded as a dense embedding vector for effective downstream applications. However, existing approaches suffer from insufficient multi-modal alignment and inadequate spatial relationship modeling, limiting their representation quality and generalizability. To address these challenges, we propose UrbanMMCL, a novel self-supervised framework that integrates multi-modal multi-view contrastive pre-training with unified fine-tuning for comprehensive urban representation learning. UrbanMMCL employs a dual-stage architecture. First, cross-modal contrastive learning aligns diverse data modalities including remote sensing imagery, street view imagery, location encodings, and Vision–Language Model (VLM)-generated textual descriptions. Second, multi-view adaptive graph contrastive learning captures complex spatial relationships across human mobility, functional similarity, and geographic distance perspectives. The framework then fine-tunes all parameters with the learned representations for effective adaptation to downstream tasks. Comprehensive experiments demonstrate that UrbanMMCL consistently outperforms state-of-the-art methods across pollutant emission prediction, population density estimation, and land use classification with minimal fine-tuning requirements, thereby advancing foundation model development for diverse Geo-AI applications.
Published: 2025-12-03T23:57:12+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.824 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jinzhou Cao; Jiashi Chen; Xiangxu Wang; Weiming Huang; Dongsheng Chen; Tianhong Zhao; Wei Tu; Qingquan Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.11.012"&gt;10.1016/j.isprsjprs.2025.11.012&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.824 (must_read)&lt;/p&gt;
&lt;p&gt;Urban region representation learning has emerged as a fundamental approach for diverse urban analytics tasks, where each neighborhood is encoded as a dense embedding vector for effective downstream applications. However, existing approaches suffer from insufficient multi-modal alignment and inadequate spatial relationship modeling, limiting their representation quality and generalizability. To address these challenges, we propose UrbanMMCL, a novel self-supervised framework that integrates multi-modal multi-view contrastive pre-training with unified fine-tuning for comprehensive urban representation learning. UrbanMMCL employs a dual-stage architecture. First, cross-modal contrastive learning aligns diverse data modalities including remote sensing imagery, street view imagery, location encodings, and Vision–Language Model (VLM)-generated textual descriptions. Second, multi-view adaptive graph contrastive learning captures complex spatial relationships across human mobility, functional similarity, and geographic distance perspectives. The framework then fine-tunes all parameters with the learned representations for effective adaptation to downstream tasks. Comprehensive experiments demonstrate that UrbanMMCL consistently outperforms state-of-the-art methods across pollutant emission prediction, population density estimation, and land use classification with minimal fine-tuning requirements, thereby advancing foundation model development for diverse Geo-AI applications.&lt;/p&gt;</content:encoded></item><item><title>GeoViS: Geospatially Rewarded Visual Search for Remote Sensing Visual Grounding</title><link>https://arxiv.org/abs/2512.02715v1</link><guid>http://arxiv.org/abs/2512.02715v1</guid><pubDate>Tue, 02 Dec 2025 12:45:52 +0000</pubDate><dc:creator>Peirong Zhang</dc:creator><dc:creator>Yidan Zhang</dc:creator><dc:creator>Luxiao Xu</dc:creator><dc:creator>Jinliang Lin</dc:creator><dc:creator>Zonghao Guo</dc:creator><dc:creator>Fengxiang Wang</dc:creator><dc:creator>Xue Yang</dc:creator><dc:creator>Kaiwen Wei</dc:creator><dc:creator>Lei Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advances in multimodal large language models(MLLMs) have led to remarkable progress in visual grounding, enabling fine-grained cross-modal alignment between textual queries and image regions. However, transferring such capabilities to remote sensing imagery remains challenging, as targets are often extremely small within kilometer-scale scenes, and queries typically involve intricate geospatial relations such as relative positions, spatial hierarchies, or contextual dependencies across distant objects. To address these challenges, we propose GeoViS, a Geospatially Rewarded Visual Search framework that reformulates remote sensing visual grounding as a progressive search-and-reasoning process. Rather than directly predicting the target location in a single step, GeoViS actively explores the global image through a tree-structured sequence of visual cues, integrating multimodal perception, spatial reasoning, and reward-guided exploration to refine geospatial hypotheses iteratively. This design enables the model to detect subtle small-scale targets while maintaining holistic scene awareness. Extensive experiments on five remote sensing grounding benchmarks demonstrate that GeoViS achieves precise geospatial understanding and consistently surpasses existing methods across key visual grounding metrics, highlighting its strong cross-domain generalization and interpretability.
Published: 2025-12-02T12:45:52+00:00
Venue: arXiv
Score: 0.820 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Peirong Zhang; Yidan Zhang; Luxiao Xu; Jinliang Lin; Zonghao Guo; Fengxiang Wang; Xue Yang; Kaiwen Wei; Lei Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.820 (must_read)&lt;/p&gt;
&lt;p&gt;Recent advances in multimodal large language models(MLLMs) have led to remarkable progress in visual grounding, enabling fine-grained cross-modal alignment between textual queries and image regions. However, transferring such capabilities to remote sensing imagery remains challenging, as targets are often extremely small within kilometer-scale scenes, and queries typically involve intricate geospatial relations such as relative positions, spatial hierarchies, or contextual dependencies across distant objects. To address these challenges, we propose GeoViS, a Geospatially Rewarded Visual Search framework that reformulates remote sensing visual grounding as a progressive search-and-reasoning process. Rather than directly predicting the target location in a single step, GeoViS actively explores the global image through a tree-structured sequence of visual cues, integrating multimodal perception, spatial reasoning, and reward-guided exploration to refine geospatial hypotheses iteratively. This design enables the model to detect subtle small-scale targets while maintaining holistic scene awareness. Extensive experiments on five remote sensing grounding benchmarks demonstrate that GeoViS achieves precise geospatial understanding and consistently surpasses existing methods across key visual grounding metrics, highlighting its strong cross-domain generalization and interpretability.&lt;/p&gt;</content:encoded></item><item><title>Cross-Stain Contrastive Learning for Paired Immunohistochemistry and Histopathology Slide Representation Learning</title><link>https://arxiv.org/abs/2512.03577v1</link><guid>http://arxiv.org/abs/2512.03577v1</guid><pubDate>Wed, 03 Dec 2025 09:00:27 +0000</pubDate><dc:creator>Yizhi Zhang</dc:creator><dc:creator>Lei Fan</dc:creator><dc:creator>Zhulin Tao</dc:creator><dc:creator>Donglin Di</dc:creator><dc:creator>Yang Song</dc:creator><dc:creator>Sidong Liu</dc:creator><dc:creator>Cong Cong</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Universal, transferable whole-slide image (WSI) representations are central to computational pathology. Incorporating multiple markers (e.g., immunohistochemistry, IHC) alongside H&amp;E enriches H&amp;E-based features with diverse, biologically meaningful information. However, progress is limited by the scarcity of well-aligned multi-stain datasets. Inter-stain misalignment shifts corresponding tissue across slides, hindering consistent patch-level features and degrading slide-level embeddings. To address this, we curated a slide-level aligned, five-stain dataset (H&amp;E, HER2, KI67, ER, PGR) to enable paired H&amp;E-IHC learning and robust cross-stain representation. Leveraging this dataset, we propose Cross-Stain Contrastive Learning (CSCL), a two-stage pretraining framework with a lightweight adapter trained using patch-wise contrastive alignment to improve the compatibility of H&amp;E features with corresponding IHC-derived contextual cues, and slide-level representation learning with Multiple Instance Learning (MIL), which uses a cross-stain attention fusion module to integrate stain-specific patch features and a cross-stain global alignment module to enforce consistency among slide-level embeddings across different stains. Experiments on cancer subtype classification, IHC biomarker status classification, and survival prediction show consistent gains, yielding high-quality, transferable H&amp;E slide-level representations. The code and data are available at https://github.com/lily-zyz/CSCL.
Published: 2025-12-03T09:00:27+00:00
Venue: arXiv
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yizhi Zhang; Lei Fan; Zhulin Tao; Donglin Di; Yang Song; Sidong Liu; Cong Cong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;Universal, transferable whole-slide image (WSI) representations are central to computational pathology. Incorporating multiple markers (e.g., immunohistochemistry, IHC) alongside H&amp;amp;E enriches H&amp;amp;E-based features with diverse, biologically meaningful information. However, progress is limited by the scarcity of well-aligned multi-stain datasets. Inter-stain misalignment shifts corresponding tissue across slides, hindering consistent patch-level features and degrading slide-level embeddings. To address this, we curated a slide-level aligned, five-stain dataset (H&amp;amp;E, HER2, KI67, ER, PGR) to enable paired H&amp;amp;E-IHC learning and robust cross-stain representation. Leveraging this dataset, we propose Cross-Stain Contrastive Learning (CSCL), a two-stage pretraining framework with a lightweight adapter trained using patch-wise contrastive alignment to improve the compatibility of H&amp;amp;E features with corresponding IHC-derived contextual cues, and slide-level representation learning with Multiple Instance Learning (MIL), which uses a cross-stain attention fusion module to integrate stain-specific patch features and a cross-stain global alignment module to enforce consistency among slide-level embeddings across different stains. Experiments on cancer subtype classification, IHC biomarker status classification, and survival prediction show consistent gains, yielding high-quality, transferable H&amp;amp;E slide-level representations. The code and data are available at https://github.com/lily-zyz/CSCL.&lt;/p&gt;</content:encoded></item><item><title>AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views</title><link>https://doi.org/10.1145/3763326</link><guid>10.1145/3763326</guid><pubDate>Thu, 04 Dec 2025 17:15:39 +0000</pubDate><dc:creator>Lihan Jiang</dc:creator><dc:creator>Yucheng Mao</dc:creator><dc:creator>Linning Xu</dc:creator><dc:creator>Tao Lu</dc:creator><dc:creator>Kerui Ren</dc:creator><dc:creator>Yichen Jin</dc:creator><dc:creator>Xudong Xu</dc:creator><dc:creator>Mulin Yu</dc:creator><dc:creator>Jiangmiao Pang</dc:creator><dc:creator>Feng Zhao</dc:creator><dc:creator>Dahua Lin</dc:creator><dc:creator>Bo Dai</dc:creator><prism:publicationName>ACM Transactions on Graphics</prism:publicationName><prism:doi>10.1145/3763326</prism:doi><description>We introduce AnySplat, a feed-forward network for novel-view synthesis from uncalibrated image collections. In contrast to traditional neural-rendering pipelines that demand known camera poses and per-scene optimization, or recent feed-forward methods that buckle under the computational weight of dense views—our model predicts everything in one shot. A single forward pass yields a set of 3D Gaussian primitives encoding both scene geometry and appearance, and the corresponding camera intrinsics and extrinsics for each input image. This unified design scales effortlessly to casually captured, multi-view datasets without any pose annotations. In extensive zero-shot evaluations, AnySplat matches the quality of pose-aware baselines in both sparse- and dense-view scenarios while surpassing existing pose-free approaches. Moreover, it greatly reduces rendering latency compared to optimization-based neural fields, bringing real-time novel-view synthesis within reach for unconstrained capture settings. Project page: https://city-super.github.io/anysplat/.
Published: 2025-12-04T17:15:39+00:00
Venue: ACM Transactions on Graphics
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lihan Jiang; Yucheng Mao; Linning Xu; Tao Lu; Kerui Ren; Yichen Jin; Xudong Xu; Mulin Yu; Jiangmiao Pang; Feng Zhao; Dahua Lin; Bo Dai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ACM Transactions on Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1145/3763326"&gt;10.1145/3763326&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;We introduce AnySplat, a feed-forward network for novel-view synthesis from uncalibrated image collections. In contrast to traditional neural-rendering pipelines that demand known camera poses and per-scene optimization, or recent feed-forward methods that buckle under the computational weight of dense views—our model predicts everything in one shot. A single forward pass yields a set of 3D Gaussian primitives encoding both scene geometry and appearance, and the corresponding camera intrinsics and extrinsics for each input image. This unified design scales effortlessly to casually captured, multi-view datasets without any pose annotations. In extensive zero-shot evaluations, AnySplat matches the quality of pose-aware baselines in both sparse- and dense-view scenarios while surpassing existing pose-free approaches. Moreover, it greatly reduces rendering latency compared to optimization-based neural fields, bringing real-time novel-view synthesis within reach for unconstrained capture settings. Project page: https://city-super.github.io/anysplat/.&lt;/p&gt;</content:encoded></item><item><title>Constituency-Tree-Induced Vision-Language Alignment for Multimodal Large Language Models</title><link>https://doi.org/10.1109/tcsvt.2025.3639574</link><guid>10.1109/tcsvt.2025.3639574</guid><pubDate>Wed, 03 Dec 2025 18:44:30 +0000</pubDate><dc:creator>Yingchen Zhai</dc:creator><dc:creator>Ning Xu</dc:creator><dc:creator>Hongshuo Tian</dc:creator><dc:creator>Bolun Zheng</dc:creator><dc:creator>Chenggang Yan</dc:creator><dc:creator>Jinbo Cao</dc:creator><dc:creator>Rongbao Kang</dc:creator><dc:creator>An-An Liu</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3639574</prism:doi><description>Multimodal large language models (MLLMs) integrate sophisticated large vision models (LVMs) to empower large language models (LLMs) with vision ability to perceive, reason, and interact in vision-language (V-L) tasks, while the modality bridge between two specialists becomes the bottleneck that translates visual signals into linguistic representations. However, most of the existing methods train the modality bridge with coarse-grained image-text pairs, neglecting the structural mapping between V-L semantics that facilitates modality translation from LVMs to LLMs. To mitigate this, we propose a Constituency-Tree-Induced Multimodal Bridging mechanism (CTIMB) that learns the fine-grained connection from LVMs to LLMs by the structural guidance from multi-modal constituency tree. Our approach consists of: 1) the multi-modal constituency-tree parser that jointly exploits the semantic structure of vision and language; 2) the lightweight connector that translates visual signals into linguistic representation and re-arranges them according to the constituency-tree structure; 3) the dynamic construction loss that aids in aligning the semantic structures derived from the tree parser and the connector. The CTIMB can learn the fine-grained mapping between visual and linguistic semantics, seamlessly bridge the LVMs and LLMs to enhance V-L tasks, and is more cost-efficient compared with current methods. Extensive experiments have demonstrated that our method more accurately interprets the visual features, enabling LLMs to conduct downstream tasks more effectively, and achieve superior performance with less training cost.
Published: 2025-12-03T18:44:30+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yingchen Zhai; Ning Xu; Hongshuo Tian; Bolun Zheng; Chenggang Yan; Jinbo Cao; Rongbao Kang; An-An Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3639574"&gt;10.1109/tcsvt.2025.3639574&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal large language models (MLLMs) integrate sophisticated large vision models (LVMs) to empower large language models (LLMs) with vision ability to perceive, reason, and interact in vision-language (V-L) tasks, while the modality bridge between two specialists becomes the bottleneck that translates visual signals into linguistic representations. However, most of the existing methods train the modality bridge with coarse-grained image-text pairs, neglecting the structural mapping between V-L semantics that facilitates modality translation from LVMs to LLMs. To mitigate this, we propose a Constituency-Tree-Induced Multimodal Bridging mechanism (CTIMB) that learns the fine-grained connection from LVMs to LLMs by the structural guidance from multi-modal constituency tree. Our approach consists of: 1) the multi-modal constituency-tree parser that jointly exploits the semantic structure of vision and language; 2) the lightweight connector that translates visual signals into linguistic representation and re-arranges them according to the constituency-tree structure; 3) the dynamic construction loss that aids in aligning the semantic structures derived from the tree parser and the connector. The CTIMB can learn the fine-grained mapping between visual and linguistic semantics, seamlessly bridge the LVMs and LLMs to enhance V-L tasks, and is more cost-efficient compared with current methods. Extensive experiments have demonstrated that our method more accurately interprets the visual features, enabling LLMs to conduct downstream tasks more effectively, and achieve superior performance with less training cost.&lt;/p&gt;</content:encoded></item><item><title>OneThinker: All-in-one Reasoning Model for Image and Video</title><link>https://arxiv.org/abs/2512.03043v2</link><guid>http://arxiv.org/abs/2512.03043v2</guid><pubDate>Tue, 02 Dec 2025 18:59:52 +0000</pubDate><dc:creator>Kaituo Feng</dc:creator><dc:creator>Manyuan Zhang</dc:creator><dc:creator>Hongyu Li</dc:creator><dc:creator>Kaixuan Fan</dc:creator><dc:creator>Shuang Chen</dc:creator><dc:creator>Yilei Jiang</dc:creator><dc:creator>Dian Zheng</dc:creator><dc:creator>Peiwen Sun</dc:creator><dc:creator>Yiyuan Zhang</dc:creator><dc:creator>Haoze Sun</dc:creator><dc:creator>Yan Feng</dc:creator><dc:creator>Peng Pei</dc:creator><dc:creator>Xunliang Cai</dc:creator><dc:creator>Xiangyu Yue</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.
Published: 2025-12-02T18:59:52+00:00
Venue: arXiv
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kaituo Feng; Manyuan Zhang; Hongyu Li; Kaixuan Fan; Shuang Chen; Yilei Jiang; Dian Zheng; Peiwen Sun; Yiyuan Zhang; Haoze Sun; Yan Feng; Peng Pei; Xunliang Cai; Xiangyu Yue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.&lt;/p&gt;</content:encoded></item><item><title>High-resolution local climate zone mapping via deep mixed-scene decomposition of remote sensing imagery</title><link>https://doi.org/10.1016/j.jag.2025.104999</link><guid>10.1016/j.jag.2025.104999</guid><pubDate>Wed, 03 Dec 2025 10:26:14 +0000</pubDate><dc:creator>Jiayi Li</dc:creator><dc:creator>Xinji Tian</dc:creator><dc:creator>Wenrui Wang</dc:creator><dc:creator>Lilin Tu</dc:creator><dc:creator>Yang Lu</dc:creator><dc:creator>Jie Jiang</dc:creator><dc:creator>Xin Huang</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2025.104999</prism:doi><description>Under rapid urbanization, traditional single-class LCZ mapping methods fail to represent the coexistence of multiple land-cover types within the same block, resulting in blurred boundaries and reduced accuracy for urban heat-island modeling. To address this, LCZ mapping is reformulated as a mixed-scene unmixing task and tackled with a novel deep-learning framework, MSU-Net. Real street-block morphologies from OpenStreetMap are combined with 1 m Google Earth imagery to create multi-scale inputs that preserve both global block layouts and local detail. MSU-Net comprises a primary unmixing branch reinforced by two auxiliary guidance branches—one driven by purified-image semantic cues, the other by sparse local spatial reconstruction, and a Dual Cross-Attention Fusion (DCAF) module that integrates global–local and global–purified features under non-negativity and sum-to-one constraints. Two block-level datasets covering Wuhan and Shenzhen were created. MSU-Net outperforms existing methods on these datasets, boosting overall accuracy by 15–17 %, reducing mean absolute error by over 35 %, and cutting weighted-difference error by around 30 %. Transfer learning further confirms its robustness across cities with distinct morphologies.
Published: 2025-12-03T10:26:14+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiayi Li; Xinji Tian; Wenrui Wang; Lilin Tu; Yang Lu; Jie Jiang; Xin Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2025.104999"&gt;10.1016/j.jag.2025.104999&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Under rapid urbanization, traditional single-class LCZ mapping methods fail to represent the coexistence of multiple land-cover types within the same block, resulting in blurred boundaries and reduced accuracy for urban heat-island modeling. To address this, LCZ mapping is reformulated as a mixed-scene unmixing task and tackled with a novel deep-learning framework, MSU-Net. Real street-block morphologies from OpenStreetMap are combined with 1 m Google Earth imagery to create multi-scale inputs that preserve both global block layouts and local detail. MSU-Net comprises a primary unmixing branch reinforced by two auxiliary guidance branches—one driven by purified-image semantic cues, the other by sparse local spatial reconstruction, and a Dual Cross-Attention Fusion (DCAF) module that integrates global–local and global–purified features under non-negativity and sum-to-one constraints. Two block-level datasets covering Wuhan and Shenzhen were created. MSU-Net outperforms existing methods on these datasets, boosting overall accuracy by 15–17 %, reducing mean absolute error by over 35 %, and cutting weighted-difference error by around 30 %. Transfer learning further confirms its robustness across cities with distinct morphologies.&lt;/p&gt;</content:encoded></item><item><title>Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling</title><link>https://arxiv.org/abs/2512.01821v1</link><guid>http://arxiv.org/abs/2512.01821v1</guid><pubDate>Mon, 01 Dec 2025 16:01:41 +0000</pubDate><dc:creator>Meng Cao</dc:creator><dc:creator>Haokun Lin</dc:creator><dc:creator>Haoyuan Li</dc:creator><dc:creator>Haoran Tang</dc:creator><dc:creator>Rongtao Xu</dc:creator><dc:creator>Dong An</dc:creator><dc:creator>Xue Liu</dc:creator><dc:creator>Ian Reid</dc:creator><dc:creator>Xiaodan Liang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Spatial reasoning, the ability to understand and interpret the 3D structure of the world, is a critical yet underdeveloped capability in Multimodal Large Language Models (MLLMs). Current methods predominantly rely on verbal descriptive tuning, which suffers from visual illiteracy, i.e., they learn spatial concepts through textual symbols alone, devoid of connection to their visual manifestations. To bridge this gap, this paper introduces MILO, an Implicit spatIaL wOrld modeling paradigm that simulates human-like spatial imagination. MILO integrates a visual generator to provide geometry-aware feedback, thereby implicitly grounding the MLLM's symbolic reasoning in perceptual experience. Complementing this paradigm, we propose RePE (Relative Positional Encoding), a novel encoding scheme that captures relative camera-pose transformations, offering superior performance over absolute coordinate systems. To support the training, we construct GeoGen, a large-scale Geometry-aware Generative dataset with approximately 2,241 videos and 67,827 observation-action-outcome triplets. Experiments demonstrate that our approach significantly enhances spatial reasoning capabilities across multiple baselines and benchmarks, offering a more holistic understanding of 3D space.
Published: 2025-12-01T16:01:41+00:00
Venue: arXiv
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Meng Cao; Haokun Lin; Haoyuan Li; Haoran Tang; Rongtao Xu; Dong An; Xue Liu; Ian Reid; Xiaodan Liang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Spatial reasoning, the ability to understand and interpret the 3D structure of the world, is a critical yet underdeveloped capability in Multimodal Large Language Models (MLLMs). Current methods predominantly rely on verbal descriptive tuning, which suffers from visual illiteracy, i.e., they learn spatial concepts through textual symbols alone, devoid of connection to their visual manifestations. To bridge this gap, this paper introduces MILO, an Implicit spatIaL wOrld modeling paradigm that simulates human-like spatial imagination. MILO integrates a visual generator to provide geometry-aware feedback, thereby implicitly grounding the MLLM&amp;#x27;s symbolic reasoning in perceptual experience. Complementing this paradigm, we propose RePE (Relative Positional Encoding), a novel encoding scheme that captures relative camera-pose transformations, offering superior performance over absolute coordinate systems. To support the training, we construct GeoGen, a large-scale Geometry-aware Generative dataset with approximately 2,241 videos and 67,827 observation-action-outcome triplets. Experiments demonstrate that our approach significantly enhances spatial reasoning capabilities across multiple baselines and benchmarks, offering a more holistic understanding of 3D space.&lt;/p&gt;</content:encoded></item><item><title>COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence</title><link>https://arxiv.org/abs/2512.04563v1</link><guid>http://arxiv.org/abs/2512.04563v1</guid><pubDate>Thu, 04 Dec 2025 08:26:04 +0000</pubDate><dc:creator>Zefeng Zhang</dc:creator><dc:creator>Xiangzhao Hao</dc:creator><dc:creator>Hengzhu Tang</dc:creator><dc:creator>Zhenyu Zhang</dc:creator><dc:creator>Jiawei Sheng</dc:creator><dc:creator>Xiaodong Li</dc:creator><dc:creator>Zhenyang Li</dc:creator><dc:creator>Li Gao</dc:creator><dc:creator>Daiting Shi</dc:creator><dc:creator>Dawei Yin</dc:creator><dc:creator>Tingwen Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose \textbf{COOPER}, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average \textbf{6.91\%} improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a \textbf{7.92\%} gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.
Published: 2025-12-04T08:26:04+00:00
Venue: arXiv
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zefeng Zhang; Xiangzhao Hao; Hengzhu Tang; Zhenyu Zhang; Jiawei Sheng; Xiaodong Li; Zhenyang Li; Li Gao; Daiting Shi; Dawei Yin; Tingwen Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose \textbf{COOPER}, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average \textbf{6.91\%} improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a \textbf{7.92\%} gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.&lt;/p&gt;</content:encoded></item><item><title>Generalizing Vision-Language Models with Dedicated Prompt Guidance</title><link>https://arxiv.org/abs/2512.02421v1</link><guid>http://arxiv.org/abs/2512.02421v1</guid><pubDate>Tue, 02 Dec 2025 05:06:17 +0000</pubDate><dc:creator>Xinyao Li</dc:creator><dc:creator>Yinjie Min</dc:creator><dc:creator>Hongbo Chen</dc:creator><dc:creator>Zhekai Du</dc:creator><dc:creator>Fengling Li</dc:creator><dc:creator>Jingjing Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Fine-tuning large pretrained vision-language models (VLMs) has emerged as a prevalent paradigm for downstream adaptation, yet it faces a critical trade-off between domain specificity and domain generalization (DG) ability. Current methods typically fine-tune a universal model on the entire dataset, which potentially compromises the ability to generalize to unseen domains. To fill this gap, we provide a theoretical understanding of the generalization ability for VLM fine-tuning, which reveals that training multiple parameter-efficient expert models on partitioned source domains leads to better generalization than fine-tuning a universal model. Inspired by this finding, we propose a two-step domain-expert-Guided DG (GuiDG) framework. GuiDG first employs prompt tuning to obtain source domain experts, then introduces a Cross-Modal Attention module to guide the fine-tuning of the vision encoder via adaptive expert integration. To better evaluate few-shot DG, we construct ImageNet-DG from ImageNet and its variants. Extensive experiments on standard DG benchmarks and ImageNet-DG demonstrate that GuiDG improves upon state-of-the-art fine-tuning methods while maintaining efficiency.
Published: 2025-12-02T05:06:17+00:00
Venue: arXiv
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinyao Li; Yinjie Min; Hongbo Chen; Zhekai Du; Fengling Li; Jingjing Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;Fine-tuning large pretrained vision-language models (VLMs) has emerged as a prevalent paradigm for downstream adaptation, yet it faces a critical trade-off between domain specificity and domain generalization (DG) ability. Current methods typically fine-tune a universal model on the entire dataset, which potentially compromises the ability to generalize to unseen domains. To fill this gap, we provide a theoretical understanding of the generalization ability for VLM fine-tuning, which reveals that training multiple parameter-efficient expert models on partitioned source domains leads to better generalization than fine-tuning a universal model. Inspired by this finding, we propose a two-step domain-expert-Guided DG (GuiDG) framework. GuiDG first employs prompt tuning to obtain source domain experts, then introduces a Cross-Modal Attention module to guide the fine-tuning of the vision encoder via adaptive expert integration. To better evaluate few-shot DG, we construct ImageNet-DG from ImageNet and its variants. Extensive experiments on standard DG benchmarks and ImageNet-DG demonstrate that GuiDG improves upon state-of-the-art fine-tuning methods while maintaining efficiency.&lt;/p&gt;</content:encoded></item><item><title>From Sight to Insight: Enhancing Confusable Structure Segmentation via Vision-Language Mutual Prompting</title><link>https://doi.org/10.1109/tmm.2025.3639888</link><guid>10.1109/tmm.2025.3639888</guid><pubDate>Wed, 03 Dec 2025 18:43:18 +0000</pubDate><dc:creator>Yixiang Huang</dc:creator><dc:creator>Yihao Zuo</dc:creator><dc:creator>Mengqiu Xu</dc:creator><dc:creator>Kaixin Chen</dc:creator><dc:creator>Ming Wu</dc:creator><dc:creator>Chuang Zhang</dc:creator><dc:creator>Zhanyu Ma</dc:creator><dc:creator>Jun Guo</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2025.3639888</prism:doi><description>Confusable structure segmentation (CSS) is a type of semantic segmentation applied in remote sensing sea fog detection, medical image segmentation, camouflaged object detection, etc. Structural similarity and visual ambiguity are two critical issues in CSS that pose difficulties in distinguishing foreground objects from the background. Current methods focus primarily on enhancing visual representations and do not often incorporate multimodal information, which leads to performance bottlenecks. Inspired by recent achievements in vision-language models, we propose Vision-Language Mutual Prompting (VLMP), a novel and unified language-guided framework that leverages text prompts to enhance CSS. Specifically, VLMP consists of vision-to-language prompting and language-to-vision prompting, which bidirectionally model the interactions between visual and linguistic features, thereby facilitating cross-modal complementary information flow. To prevent the predominance of one modality over another, we design a feature integration modulator that modulates and balances feature weights for adaptive multimodal fusion. Our framework is designed to be modular and flexible, allowing for integration with any backbone, including CNNs and transformers. We evaluate VLMP with three diverse datasets: SFDD-H8, QaTa-COV19, and CAMO-COD10K. Extensive experiments demonstrate the effectiveness and superiority of the proposed framework over those of state-of-the-art methods across these datasets. This shift from basic sight to deeper insight in CSS through vision-language integration represents a significant advancement in the field.
Published: 2025-12-03T18:43:18+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yixiang Huang; Yihao Zuo; Mengqiu Xu; Kaixin Chen; Ming Wu; Chuang Zhang; Zhanyu Ma; Jun Guo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2025.3639888"&gt;10.1109/tmm.2025.3639888&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;Confusable structure segmentation (CSS) is a type of semantic segmentation applied in remote sensing sea fog detection, medical image segmentation, camouflaged object detection, etc. Structural similarity and visual ambiguity are two critical issues in CSS that pose difficulties in distinguishing foreground objects from the background. Current methods focus primarily on enhancing visual representations and do not often incorporate multimodal information, which leads to performance bottlenecks. Inspired by recent achievements in vision-language models, we propose Vision-Language Mutual Prompting (VLMP), a novel and unified language-guided framework that leverages text prompts to enhance CSS. Specifically, VLMP consists of vision-to-language prompting and language-to-vision prompting, which bidirectionally model the interactions between visual and linguistic features, thereby facilitating cross-modal complementary information flow. To prevent the predominance of one modality over another, we design a feature integration modulator that modulates and balances feature weights for adaptive multimodal fusion. Our framework is designed to be modular and flexible, allowing for integration with any backbone, including CNNs and transformers. We evaluate VLMP with three diverse datasets: SFDD-H8, QaTa-COV19, and CAMO-COD10K. Extensive experiments demonstrate the effectiveness and superiority of the proposed framework over those of state-of-the-art methods across these datasets. This shift from basic sight to deeper insight in CSS through vision-language integration represents a significant advancement in the field.&lt;/p&gt;</content:encoded></item><item><title>Structure as an inductive bias for brain–model alignment</title><link>https://doi.org/10.1038/s42256-025-01155-y</link><guid>10.1038/s42256-025-01155-y</guid><pubDate>Thu, 04 Dec 2025 16:01:22 +0000</pubDate><dc:creator>Binxu Wang</dc:creator><dc:creator>Carlos R. Ponce</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01155-y</prism:doi><description>Even before training, convolutional neural networks may reflect the brain’s visual processing principles. A study now shows how structure alone can help to explain the alignment between brains and models.
Published: 2025-12-04T16:01:22+00:00
Venue: Nature Machine Intelligence
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Binxu Wang; Carlos R. Ponce&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01155-y"&gt;10.1038/s42256-025-01155-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;Even before training, convolutional neural networks may reflect the brain’s visual processing principles. A study now shows how structure alone can help to explain the alignment between brains and models.&lt;/p&gt;</content:encoded></item><item><title>GeoDiT: A Diffusion-based Vision-Language Model for Geospatial Understanding</title><link>https://arxiv.org/abs/2512.02505v1</link><guid>http://arxiv.org/abs/2512.02505v1</guid><pubDate>Tue, 02 Dec 2025 07:59:46 +0000</pubDate><dc:creator>Jiaqi Liu</dc:creator><dc:creator>Ronghao Fu</dc:creator><dc:creator>Haoran Liu</dc:creator><dc:creator>Lang Sun</dc:creator><dc:creator>Bo Yang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Autoregressive models are structurally misaligned with the inherently parallel nature of geospatial understanding, forcing a rigid sequential narrative onto scenes and fundamentally hindering the generation of structured and coherent outputs. We challenge this paradigm by reframing geospatial generation as a parallel refinement process, enabling a holistic, coarse-to-fine synthesis that resolves all semantic elements simultaneously. To operationalize this, we introduce GeoDiT, the first diffusion-based vision-language model tailored for the geospatial domain. Extensive experiments demonstrate that GeoDiT establishes a new state-of-the-art on benchmarks requiring structured, object-centric outputs. It achieves significant gains in image captioning, visual grounding, and multi-object detection, precisely the tasks where autoregressive models falter. Our work validates that aligning the generative process with the data's intrinsic structure is key to unlocking superior performance in complex geospatial analysis.
Published: 2025-12-02T07:59:46+00:00
Venue: arXiv
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaqi Liu; Ronghao Fu; Haoran Liu; Lang Sun; Bo Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;Autoregressive models are structurally misaligned with the inherently parallel nature of geospatial understanding, forcing a rigid sequential narrative onto scenes and fundamentally hindering the generation of structured and coherent outputs. We challenge this paradigm by reframing geospatial generation as a parallel refinement process, enabling a holistic, coarse-to-fine synthesis that resolves all semantic elements simultaneously. To operationalize this, we introduce GeoDiT, the first diffusion-based vision-language model tailored for the geospatial domain. Extensive experiments demonstrate that GeoDiT establishes a new state-of-the-art on benchmarks requiring structured, object-centric outputs. It achieves significant gains in image captioning, visual grounding, and multi-object detection, precisely the tasks where autoregressive models falter. Our work validates that aligning the generative process with the data&amp;#x27;s intrinsic structure is key to unlocking superior performance in complex geospatial analysis.&lt;/p&gt;</content:encoded></item><item><title>CartoMapQA: A Fundamental Benchmark Dataset Evaluating Vision-Language Models on Cartographic Map Understanding</title><link>https://arxiv.org/abs/2512.03558v1</link><guid>http://arxiv.org/abs/2512.03558v1</guid><pubDate>Wed, 03 Dec 2025 08:25:22 +0000</pubDate><dc:creator>Huy Quang Ung</dc:creator><dc:creator>Guillaume Habault</dc:creator><dc:creator>Yasutaka Nishimura</dc:creator><dc:creator>Hao Niu</dc:creator><dc:creator>Roberto Legaspi</dc:creator><dc:creator>Tomoki Oya</dc:creator><dc:creator>Ryoichi Kojima</dc:creator><dc:creator>Masato Taya</dc:creator><dc:creator>Chihiro Ono</dc:creator><dc:creator>Atsunori Minamikawa</dc:creator><dc:creator>Yan Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The rise of Visual-Language Models (LVLMs) has unlocked new possibilities for seamlessly integrating visual and textual information. However, their ability to interpret cartographic maps remains largely unexplored. In this paper, we introduce CartoMapQA, a benchmark specifically designed to evaluate LVLMs' understanding of cartographic maps through question-answering tasks. The dataset includes over 2000 samples, each composed of a cartographic map, a question (with open-ended or multiple-choice answers), and a ground-truth answer. These tasks span key low-, mid- and high-level map interpretation skills, including symbol recognition, embedded information extraction, scale interpretation, and route-based reasoning. Our evaluation of both open-source and proprietary LVLMs reveals persistent challenges: models frequently struggle with map-specific semantics, exhibit limited geospatial reasoning, and are prone to Optical Character Recognition (OCR)-related errors. By isolating these weaknesses, CartoMapQA offers a valuable tool for guiding future improvements in LVLM architectures. Ultimately, it supports the development of models better equipped for real-world applications that depend on robust and reliable map understanding, such as navigation, geographic search, and urban planning. Our source code and data are openly available to the research community at: https://github.com/ungquanghuy-kddi/CartoMapQA.git
Published: 2025-12-03T08:25:22+00:00
Venue: arXiv
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Huy Quang Ung; Guillaume Habault; Yasutaka Nishimura; Hao Niu; Roberto Legaspi; Tomoki Oya; Ryoichi Kojima; Masato Taya; Chihiro Ono; Atsunori Minamikawa; Yan Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;The rise of Visual-Language Models (LVLMs) has unlocked new possibilities for seamlessly integrating visual and textual information. However, their ability to interpret cartographic maps remains largely unexplored. In this paper, we introduce CartoMapQA, a benchmark specifically designed to evaluate LVLMs&amp;#x27; understanding of cartographic maps through question-answering tasks. The dataset includes over 2000 samples, each composed of a cartographic map, a question (with open-ended or multiple-choice answers), and a ground-truth answer. These tasks span key low-, mid- and high-level map interpretation skills, including symbol recognition, embedded information extraction, scale interpretation, and route-based reasoning. Our evaluation of both open-source and proprietary LVLMs reveals persistent challenges: models frequently struggle with map-specific semantics, exhibit limited geospatial reasoning, and are prone to Optical Character Recognition (OCR)-related errors. By isolating these weaknesses, CartoMapQA offers a valuable tool for guiding future improvements in LVLM architectures. Ultimately, it supports the development of models better equipped for real-world applications that depend on robust and reliable map understanding, such as navigation, geographic search, and urban planning. Our source code and data are openly available to the research community at: https://github.com/ungquanghuy-kddi/CartoMapQA.git&lt;/p&gt;</content:encoded></item><item><title>DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images</title><link>https://arxiv.org/abs/2512.03004v1</link><guid>http://arxiv.org/abs/2512.03004v1</guid><pubDate>Tue, 02 Dec 2025 18:29:18 +0000</pubDate><dc:creator>Xiaoxue Chen</dc:creator><dc:creator>Ziyi Xiong</dc:creator><dc:creator>Yuantao Chen</dc:creator><dc:creator>Gen Li</dc:creator><dc:creator>Nan Wang</dc:creator><dc:creator>Hongcheng Luo</dc:creator><dc:creator>Long Chen</dc:creator><dc:creator>Haiyang Sun</dc:creator><dc:creator>Bing Wang</dc:creator><dc:creator>Guang Chen</dc:creator><dc:creator>Hangjun Ye</dc:creator><dc:creator>Hongyang Li</dc:creator><dc:creator>Ya-Qin Zhang</dc:creator><dc:creator>Hao Zhao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.
Published: 2025-12-02T18:29:18+00:00
Venue: arXiv
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoxue Chen; Ziyi Xiong; Yuantao Chen; Gen Li; Nan Wang; Hongcheng Luo; Long Chen; Haiyang Sun; Bing Wang; Guang Chen; Hangjun Ye; Hongyang Li; Ya-Qin Zhang; Hao Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.&lt;/p&gt;</content:encoded></item><item><title>CombAlign: Enhancing Model Expressiveness in Unsupervised Graph Alignment</title><link>https://doi.org/10.1109/tkde.2025.3640287</link><guid>10.1109/tkde.2025.3640287</guid><pubDate>Thu, 04 Dec 2025 18:38:39 +0000</pubDate><dc:creator>Songyang Chen</dc:creator><dc:creator>Yu Liu</dc:creator><dc:creator>Lei Zou</dc:creator><dc:creator>Zexuan Wang</dc:creator><dc:creator>Youfang Lin</dc:creator><prism:publicationName>IEEE Transactions on Knowledge and Data Engineering</prism:publicationName><prism:doi>10.1109/tkde.2025.3640287</prism:doi><description>Unsupervised graph alignment finds the node correspondence between a pair of attributed graphs by only exploiting graph structure and node features. One category of recent studies first computes the node representation and then matches nodes with the largest embedding-based similarity, while the other category reduces the problem to optimal transport (OT) via Gromov-Wasserstein learning. However, it remains largely unexplored in the model expressiveness, as well as how theoretical expressivity impacts prediction accuracy. We investigate the model expressiveness from two aspects. First, we characterize the model's discriminative power in distinguishing matched and unmatched node pairs across two graphs. Second, we study the model's capability of guaranteeing node matching properties such as one-to-one matching and mutual alignment. Motivated by our theoretical analysis, we put forward a hybrid approach named CombAlign with stronger expressive power. Specifically, we enable cross-dimensional feature interaction for OT-based learning and propose an embedding-based method inspired by the Weisfeiler-Lehman test. We also apply non-uniform marginals obtained from the embedding-based modules to OT as priors for more expressiveness. Based on that, we propose a traditional algorithm-based refinement, which combines our OT and embedding-based predictions using the ensemble learning strategy and reduces the problem to maximum weight matching. With carefully designed edge weights, we ensure these matching properties and further enhance prediction accuracy. By extensive experiments, we demonstrate a significant improvement of 14.5% in alignment accuracy compared to state-of-the-art approaches and confirm the soundness of our theoretical analysis.
Published: 2025-12-04T18:38:39+00:00
Venue: IEEE Transactions on Knowledge and Data Engineering
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Songyang Chen; Yu Liu; Lei Zou; Zexuan Wang; Youfang Lin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Knowledge and Data Engineering&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tkde.2025.3640287"&gt;10.1109/tkde.2025.3640287&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Unsupervised graph alignment finds the node correspondence between a pair of attributed graphs by only exploiting graph structure and node features. One category of recent studies first computes the node representation and then matches nodes with the largest embedding-based similarity, while the other category reduces the problem to optimal transport (OT) via Gromov-Wasserstein learning. However, it remains largely unexplored in the model expressiveness, as well as how theoretical expressivity impacts prediction accuracy. We investigate the model expressiveness from two aspects. First, we characterize the model&amp;#x27;s discriminative power in distinguishing matched and unmatched node pairs across two graphs. Second, we study the model&amp;#x27;s capability of guaranteeing node matching properties such as one-to-one matching and mutual alignment. Motivated by our theoretical analysis, we put forward a hybrid approach named CombAlign with stronger expressive power. Specifically, we enable cross-dimensional feature interaction for OT-based learning and propose an embedding-based method inspired by the Weisfeiler-Lehman test. We also apply non-uniform marginals obtained from the embedding-based modules to OT as priors for more expressiveness. Based on that, we propose a traditional algorithm-based refinement, which combines our OT and embedding-based predictions using the ensemble learning strategy and reduces the problem to maximum weight matching. With carefully designed edge weights, we ensure these matching properties and further enhance prediction accuracy. By extensive experiments, we demonstrate a significant improvement of 14.5% in alignment accuracy compared to state-of-the-art approaches and confirm the soundness of our theoretical analysis.&lt;/p&gt;</content:encoded></item><item><title>See, Think, Learn: A Self-Taught Multimodal Reasoner</title><link>https://arxiv.org/abs/2512.02456v1</link><guid>http://arxiv.org/abs/2512.02456v1</guid><pubDate>Tue, 02 Dec 2025 06:30:10 +0000</pubDate><dc:creator>Sourabh Sharma</dc:creator><dc:creator>Sonam Gupta</dc:creator><dc:creator>Sadbhawna</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language Models (VLMs) have achieved remarkable progress in integrating visual perception with language understanding. However, effective multimodal reasoning requires both accurate perception and robust reasoning, and weakness in either limits the performance of VLMs. Prior efforts to enhance reasoning often depend on high-quality chain-of-thought (CoT) data, obtained via labor-intensive human annotations, costly proprietary models, or self-training methods that overlook perception. To address these limitations, we propose a simple yet effective self-training framework called See-Think-Learn (STL). At its core, STL introduces a structured reasoning template that encourages the model to see before thinking, first extracting visual attributes in textual form, then using them to guide reasoning. The framework jointly improves perception and reasoning by having the model generate and learn from its own structured rationales in a self-training loop. Furthermore, we augment the training data with negative rationales, i.e. explanations that justify why certain answer choices are incorrect, to enhance the model's ability to distinguish between correct and misleading responses. This fosters more discriminative and robust learning. Experiments across diverse domains show that STL consistently outperforms baselines trained directly only on answers or self-generated reasoning, while qualitative analysis confirms the high quality of its rationales. STL thus provides a cost-effective solution to enhance multimodal reasoning ability of VLMs.
Published: 2025-12-02T06:30:10+00:00
Venue: arXiv
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sourabh Sharma; Sonam Gupta; Sadbhawna&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Vision-Language Models (VLMs) have achieved remarkable progress in integrating visual perception with language understanding. However, effective multimodal reasoning requires both accurate perception and robust reasoning, and weakness in either limits the performance of VLMs. Prior efforts to enhance reasoning often depend on high-quality chain-of-thought (CoT) data, obtained via labor-intensive human annotations, costly proprietary models, or self-training methods that overlook perception. To address these limitations, we propose a simple yet effective self-training framework called See-Think-Learn (STL). At its core, STL introduces a structured reasoning template that encourages the model to see before thinking, first extracting visual attributes in textual form, then using them to guide reasoning. The framework jointly improves perception and reasoning by having the model generate and learn from its own structured rationales in a self-training loop. Furthermore, we augment the training data with negative rationales, i.e. explanations that justify why certain answer choices are incorrect, to enhance the model&amp;#x27;s ability to distinguish between correct and misleading responses. This fosters more discriminative and robust learning. Experiments across diverse domains show that STL consistently outperforms baselines trained directly only on answers or self-generated reasoning, while qualitative analysis confirms the high quality of its rationales. STL thus provides a cost-effective solution to enhance multimodal reasoning ability of VLMs.&lt;/p&gt;</content:encoded></item><item><title>ComCon: Complementary-Contradictory Regularization for Multimodal Knowledge Graph Completion</title><link>https://doi.org/10.1016/j.inffus.2025.104016</link><guid>10.1016/j.inffus.2025.104016</guid><pubDate>Wed, 03 Dec 2025 17:16:36 +0000</pubDate><dc:creator>Jie Chen</dc:creator><dc:creator>Wuyang Zhang</dc:creator><dc:creator>Shu Zhao</dc:creator><dc:creator>Yunxia Yin</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104016</prism:doi><description>Multimodal Knowledge Graphs (MMKGs) extend traditional knowledge graphs by incorporating multimodal data, enriching the representation of entities from multiple perspectives. Most MMKGs are inherently incomplete, thus requiring Multimodal Knowledge Graph Completion (MMKGC) for missing triple prediction. MMKGC differs from traditional KGC in the integration of diverse modalities, such as textual and visual modalities, for a more comprehensive representation. However, inherent cross-modal semantic discrepancies in unified representations lead to misalignment and accuracy degradation. To resolve this, we propose ComCon, an effective regularization model. Our key insight is to explicitly decompose the unified representation into two distinct components: complementary features, where textual and visual modalities provide mutually reinforcing information to enhance the representation, and contradictory features, which capture the conflicting signals and inconsistencies between modalities. By regularizing the interactions between these features, ComCon effectively mitigates semantic discrepancies and enhances representation learning. Furthermore, we implement a Weighted Negative Sampling (WNS) to discern potential false negatives and diminish their impact by minimizing the score margin. Comprehensive experiments on the DB15K and MKG-W datasets demonstrate that our ComCon outperforms state-of-the-art baselines. Our code and datasets are released at https://github.com/wyZhang016/ComCon .
Published: 2025-12-03T17:16:36+00:00
Venue: Information Fusion
Score: 0.773 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jie Chen; Wuyang Zhang; Shu Zhao; Yunxia Yin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104016"&gt;10.1016/j.inffus.2025.104016&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal Knowledge Graphs (MMKGs) extend traditional knowledge graphs by incorporating multimodal data, enriching the representation of entities from multiple perspectives. Most MMKGs are inherently incomplete, thus requiring Multimodal Knowledge Graph Completion (MMKGC) for missing triple prediction. MMKGC differs from traditional KGC in the integration of diverse modalities, such as textual and visual modalities, for a more comprehensive representation. However, inherent cross-modal semantic discrepancies in unified representations lead to misalignment and accuracy degradation. To resolve this, we propose ComCon, an effective regularization model. Our key insight is to explicitly decompose the unified representation into two distinct components: complementary features, where textual and visual modalities provide mutually reinforcing information to enhance the representation, and contradictory features, which capture the conflicting signals and inconsistencies between modalities. By regularizing the interactions between these features, ComCon effectively mitigates semantic discrepancies and enhances representation learning. Furthermore, we implement a Weighted Negative Sampling (WNS) to discern potential false negatives and diminish their impact by minimizing the score margin. Comprehensive experiments on the DB15K and MKG-W datasets demonstrate that our ComCon outperforms state-of-the-art baselines. Our code and datasets are released at https://github.com/wyZhang016/ComCon .&lt;/p&gt;</content:encoded></item><item><title>SLSM-Net: Sparse LiDAR Point Clouds Supervised Stereo Matching</title><link>https://doi.org/10.1109/tmm.2025.3639911</link><guid>10.1109/tmm.2025.3639911</guid><pubDate>Wed, 03 Dec 2025 18:43:18 +0000</pubDate><dc:creator>Ze Zong</dc:creator><dc:creator>Cheng Wu</dc:creator><dc:creator>Jie Xie</dc:creator><dc:creator>Jin Zhang</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2025.3639911</prism:doi><description>Deep learning has achieved significant success in stereo matching, with its training process often supervised by LiDAR measurements. However, the sparsity of real-world LiDAR data limits the ability of deep models to extract effective features from stereo images. To address this issue, a novel deep learning-based framework called sparse LiDAR point cloud supervised stereo matching (SLSM-Net) is proposed. Specifically, dense reconstruction of sparse single-frame point clouds is first designed to avoid the error introduction with the mergence of multi-frame point clouds. To effectively densify point clouds of objects in local areas, stereo images are utilized as supervision information to train the deep models. Furthermore, a coarse-to-fine structure of the deep model is designed for stereo matching. A self-supervised learning strategy, which employs a photometric consistency constraint, is second proposed along with fully supervised learning to obtain dense and precise supervision information. This stage generates coarse disparity maps from stereo images. Finally, to fully leverage the complementary characteristics of LiDAR and stereo cameras, multi-scale feature fusion of point clouds and stereo images is performed by a residual block, where the feature maps of point clouds are derived from the densification reconstruction. This stage refines the results. Experimental results indicate that SLSM-Net outperforms current state-of-the-art methods, demonstrating superior performance in stereo matching.
Published: 2025-12-03T18:43:18+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.772 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ze Zong; Cheng Wu; Jie Xie; Jin Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2025.3639911"&gt;10.1109/tmm.2025.3639911&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (must_read)&lt;/p&gt;
&lt;p&gt;Deep learning has achieved significant success in stereo matching, with its training process often supervised by LiDAR measurements. However, the sparsity of real-world LiDAR data limits the ability of deep models to extract effective features from stereo images. To address this issue, a novel deep learning-based framework called sparse LiDAR point cloud supervised stereo matching (SLSM-Net) is proposed. Specifically, dense reconstruction of sparse single-frame point clouds is first designed to avoid the error introduction with the mergence of multi-frame point clouds. To effectively densify point clouds of objects in local areas, stereo images are utilized as supervision information to train the deep models. Furthermore, a coarse-to-fine structure of the deep model is designed for stereo matching. A self-supervised learning strategy, which employs a photometric consistency constraint, is second proposed along with fully supervised learning to obtain dense and precise supervision information. This stage generates coarse disparity maps from stereo images. Finally, to fully leverage the complementary characteristics of LiDAR and stereo cameras, multi-scale feature fusion of point clouds and stereo images is performed by a residual block, where the feature maps of point clouds are derived from the densification reconstruction. This stage refines the results. Experimental results indicate that SLSM-Net outperforms current state-of-the-art methods, demonstrating superior performance in stereo matching.&lt;/p&gt;</content:encoded></item><item><title>MT-Depth: Multi-task Instance feature analysis for the Depth Completion</title><link>https://arxiv.org/abs/2512.04734v1</link><guid>http://arxiv.org/abs/2512.04734v1</guid><pubDate>Thu, 04 Dec 2025 12:17:33 +0000</pubDate><dc:creator>Abdul Haseeb Nizamani</dc:creator><dc:creator>Dandi Zhou</dc:creator><dc:creator>Xinhai Sun</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Depth completion plays a vital role in 3D perception systems, especially in scenarios where sparse depth data must be densified for tasks such as autonomous driving, robotics, and augmented reality. While many existing approaches rely on semantic segmentation to guide depth completion, they often overlook the benefits of object-level understanding. In this work, we introduce an instance-aware depth completion framework that explicitly integrates binary instance masks as spatial priors to refine depth predictions. Our model combines four main components: a frozen YOLO V11 instance segmentation branch, a U-Net-based depth completion backbone, a cross-attention fusion module, and an attention-guided prediction head. The instance segmentation branch generates per-image foreground masks that guide the depth branch via cross-attention, allowing the network to focus on object-centric regions during refinement. We validate our method on the Virtual KITTI 2 dataset, showing that it achieves lower RMSE compared to both a U-Net-only baseline and previous semantic-guided methods, while maintaining competitive MAE. Qualitative and quantitative results demonstrate that the proposed model effectively enhances depth accuracy near object boundaries, occlusions, and thin structures. Our findings suggest that incorporating instance-aware cues offers a promising direction for improving depth completion without relying on dense semantic labels.
Published: 2025-12-04T12:17:33+00:00
Venue: arXiv
Score: 0.772 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Abdul Haseeb Nizamani; Dandi Zhou; Xinhai Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (must_read)&lt;/p&gt;
&lt;p&gt;Depth completion plays a vital role in 3D perception systems, especially in scenarios where sparse depth data must be densified for tasks such as autonomous driving, robotics, and augmented reality. While many existing approaches rely on semantic segmentation to guide depth completion, they often overlook the benefits of object-level understanding. In this work, we introduce an instance-aware depth completion framework that explicitly integrates binary instance masks as spatial priors to refine depth predictions. Our model combines four main components: a frozen YOLO V11 instance segmentation branch, a U-Net-based depth completion backbone, a cross-attention fusion module, and an attention-guided prediction head. The instance segmentation branch generates per-image foreground masks that guide the depth branch via cross-attention, allowing the network to focus on object-centric regions during refinement. We validate our method on the Virtual KITTI 2 dataset, showing that it achieves lower RMSE compared to both a U-Net-only baseline and previous semantic-guided methods, while maintaining competitive MAE. Qualitative and quantitative results demonstrate that the proposed model effectively enhances depth accuracy near object boundaries, occlusions, and thin structures. Our findings suggest that incorporating instance-aware cues offers a promising direction for improving depth completion without relying on dense semantic labels.&lt;/p&gt;</content:encoded></item><item><title>GeoBridge: A Semantic-Anchored Multi-View Foundation Model Bridging Images and Text for Geo-Localization</title><link>https://arxiv.org/abs/2512.02697v1</link><guid>http://arxiv.org/abs/2512.02697v1</guid><pubDate>Tue, 02 Dec 2025 12:28:22 +0000</pubDate><dc:creator>Zixuan Song</dc:creator><dc:creator>Jing Zhang</dc:creator><dc:creator>Di Wang</dc:creator><dc:creator>Zidie Zhou</dc:creator><dc:creator>Wenbin Liu</dc:creator><dc:creator>Haonan Guo</dc:creator><dc:creator>En Wang</dc:creator><dc:creator>Bo Du</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image. However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable. It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image). To address these challenges, we propose GeoBridge, a foundation model that performs bidirectional matching across views and supports language-to-image retrieval. Going beyond traditional satellite-centric formulations, GeoBridge builds on a novel semantic-anchor mechanism that bridges multi-view features through textual descriptions for robust, flexible localization. In support of this task, we construct GeoLoc, the first large-scale, cross-modal, and multi-view aligned dataset comprising over 50,000 pairs of drone, street-view panorama, and satellite images as well as their textual descriptions, collected from 36 countries, ensuring both geographic and semantic alignment. We performed broad evaluations across multiple tasks. Experiments confirm that GeoLoc pre-training markedly improves geo-location accuracy for GeoBridge while promoting cross-domain generalization and cross-modal knowledge transfer. The dataset, source code, and pretrained models were released at https://github.com/MiliLab/GeoBridge.
Published: 2025-12-02T12:28:22+00:00
Venue: arXiv
Score: 0.772 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zixuan Song; Jing Zhang; Di Wang; Zidie Zhou; Wenbin Liu; Haonan Guo; En Wang; Bo Du&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (must_read)&lt;/p&gt;
&lt;p&gt;Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image. However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable. It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image). To address these challenges, we propose GeoBridge, a foundation model that performs bidirectional matching across views and supports language-to-image retrieval. Going beyond traditional satellite-centric formulations, GeoBridge builds on a novel semantic-anchor mechanism that bridges multi-view features through textual descriptions for robust, flexible localization. In support of this task, we construct GeoLoc, the first large-scale, cross-modal, and multi-view aligned dataset comprising over 50,000 pairs of drone, street-view panorama, and satellite images as well as their textual descriptions, collected from 36 countries, ensuring both geographic and semantic alignment. We performed broad evaluations across multiple tasks. Experiments confirm that GeoLoc pre-training markedly improves geo-location accuracy for GeoBridge while promoting cross-domain generalization and cross-modal knowledge transfer. The dataset, source code, and pretrained models were released at https://github.com/MiliLab/GeoBridge.&lt;/p&gt;</content:encoded></item><item><title>SS4D: Native 4D Generative Model via Structured Spacetime Latents</title><link>https://doi.org/10.1145/3763302</link><guid>10.1145/3763302</guid><pubDate>Thu, 04 Dec 2025 17:15:39 +0000</pubDate><dc:creator>Zhibing Li</dc:creator><dc:creator>Mengchen Zhang</dc:creator><dc:creator>Tong Wu</dc:creator><dc:creator>Jing Tan</dc:creator><dc:creator>Jiaqi Wang</dc:creator><dc:creator>Dahua Lin</dc:creator><prism:publicationName>ACM Transactions on Graphics</prism:publicationName><prism:doi>10.1145/3763302</prism:doi><description>We present SS4D, a native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that construct 4D representations by optimizing over 3D or video generative models, we train a generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is a compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on a pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To support efficient training and inference over long video sequences, we compress the latent sequence along the temporal axis using factorized 4D convolutions and temporal downsampling blocks. In addition, we employ a carefully designed training strategy to enhance robustness against occlusion and motion blur, leading to high-quality generation. Extensive experiments show that SS4D produces spatio-temporally consistent 4D objects with superior quality and efficiency, significantly outperforming state-of-the-art methods on both synthetic and real-world datasets.
Published: 2025-12-04T17:15:39+00:00
Venue: ACM Transactions on Graphics
Score: 0.772 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhibing Li; Mengchen Zhang; Tong Wu; Jing Tan; Jiaqi Wang; Dahua Lin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ACM Transactions on Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1145/3763302"&gt;10.1145/3763302&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (must_read)&lt;/p&gt;
&lt;p&gt;We present SS4D, a native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that construct 4D representations by optimizing over 3D or video generative models, we train a generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is a compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on a pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To support efficient training and inference over long video sequences, we compress the latent sequence along the temporal axis using factorized 4D convolutions and temporal downsampling blocks. In addition, we employ a carefully designed training strategy to enhance robustness against occlusion and motion blur, leading to high-quality generation. Extensive experiments show that SS4D produces spatio-temporally consistent 4D objects with superior quality and efficiency, significantly outperforming state-of-the-art methods on both synthetic and real-world datasets.&lt;/p&gt;</content:encoded></item><item><title>S2ML: Spatio-Spectral Mutual Learning for Depth Completion</title><link>https://doi.org/10.1109/tmm.2025.3639949</link><guid>10.1109/tmm.2025.3639949</guid><pubDate>Wed, 03 Dec 2025 18:43:18 +0000</pubDate><dc:creator>Zihui Zhao</dc:creator><dc:creator>Yifei Zhang</dc:creator><dc:creator>Zheng Wang</dc:creator><dc:creator>Yang Li</dc:creator><dc:creator>Kui Jiang</dc:creator><dc:creator>Zihan Geng</dc:creator><dc:creator>Chia-Wen Lin</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2025.3639949</prism:doi><description>The raw depth images captured by RGB-D cameras using Time-of-Flight (TOF) or structured light often suffer from incomplete depth values due to weak reflections, boundary shadows, and artifacts, which limit their applications in downstream vision tasks. Existing methods address this problem through depth completion in the image domain, but they overlook the physical characteristics of raw depth images. It has been observed that the presence of invalid depth areas alters the frequency distribution pattern. In this work, we propose a Spatio-Spectral Mutual Learning framework (S2ML) to harmonize the advantages of both spatial and frequency domains for depth completion. Specifically, we consider the distinct properties of amplitude and phase spectra and devise a dedicated spectral fusion module. Meanwhile, the local and global correlations between spatial-domain and frequency-domain features are calculated in a unified embedding space. The gradual mutual representation and refinement encourage the network to fully explore complementary physical characteristics and priors for more accurate depth completion. Extensive experiments demonstrate the effectiveness of our proposed S2ML method, outperforming the state-of-the-art method CFormer by 0.828 dB and 0.834 dB on the NYU-Depth V2 and SUN RGB-D datasets, respectively.
Published: 2025-12-03T18:43:18+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.771 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zihui Zhao; Yifei Zhang; Zheng Wang; Yang Li; Kui Jiang; Zihan Geng; Chia-Wen Lin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2025.3639949"&gt;10.1109/tmm.2025.3639949&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (must_read)&lt;/p&gt;
&lt;p&gt;The raw depth images captured by RGB-D cameras using Time-of-Flight (TOF) or structured light often suffer from incomplete depth values due to weak reflections, boundary shadows, and artifacts, which limit their applications in downstream vision tasks. Existing methods address this problem through depth completion in the image domain, but they overlook the physical characteristics of raw depth images. It has been observed that the presence of invalid depth areas alters the frequency distribution pattern. In this work, we propose a Spatio-Spectral Mutual Learning framework (S2ML) to harmonize the advantages of both spatial and frequency domains for depth completion. Specifically, we consider the distinct properties of amplitude and phase spectra and devise a dedicated spectral fusion module. Meanwhile, the local and global correlations between spatial-domain and frequency-domain features are calculated in a unified embedding space. The gradual mutual representation and refinement encourage the network to fully explore complementary physical characteristics and priors for more accurate depth completion. Extensive experiments demonstrate the effectiveness of our proposed S2ML method, outperforming the state-of-the-art method CFormer by 0.828 dB and 0.834 dB on the NYU-Depth V2 and SUN RGB-D datasets, respectively.&lt;/p&gt;</content:encoded></item><item><title>Efficient and Scalable Point Cloud Generation With Sparse Point-Voxel Diffusion Models</title><link>https://doi.org/10.1109/tnnls.2025.3636409</link><guid>10.1109/tnnls.2025.3636409</guid><pubDate>Thu, 04 Dec 2025 18:38:05 +0000</pubDate><dc:creator>Ioannis Romanelis</dc:creator><dc:creator>Vlassis Fotis</dc:creator><dc:creator>Athanasios Kalogeras</dc:creator><dc:creator>Christos Alexakos</dc:creator><dc:creator>Adrian Munteanu</dc:creator><dc:creator>Konstantinos Moustakas</dc:creator><prism:publicationName>IEEE Transactions on Neural Networks and Learning Systems</prism:publicationName><prism:doi>10.1109/tnnls.2025.3636409</prism:doi><description>We propose a novel point cloud U-Net diffusion architecture for 3-D generative modeling capable of generating high-quality and diverse 3-D shapes while maintaining fast generation times. Our network employs a dual-branch architecture, combining the high-resolution representations of points with the computational efficiency of sparse voxels. Our fastest variant outperforms all nondiffusion generative approaches on unconditional shape generation, the most popular benchmark for evaluating point cloud generative models, while our largest model achieves state-of-the-art results among diffusion methods, with a runtime approximately 70% of the previously state-of-the-art point-voxel diffusion (PVD), measured on the same hardware setting. Beyond unconditional generation, we perform extensive evaluations, including conditional generation on all categories of ShapeNet, demonstrating the scalability of our model to larger datasets, and implicit generation, which allows our network to produce high-quality point clouds on fewer timesteps, further decreasing the generation time. Finally, we evaluate the architecture’s performance in point cloud completion and super-resolution. Our model excels in all tasks, establishing it as a state-of-the-art diffusion U-Net for point cloud generative modeling. The code is publicly available at https://github.com/JohnRomanelis/SPVD
Published: 2025-12-04T18:38:05+00:00
Venue: IEEE Transactions on Neural Networks and Learning Systems
Score: 0.770 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ioannis Romanelis; Vlassis Fotis; Athanasios Kalogeras; Christos Alexakos; Adrian Munteanu; Konstantinos Moustakas&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Neural Networks and Learning Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tnnls.2025.3636409"&gt;10.1109/tnnls.2025.3636409&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (must_read)&lt;/p&gt;
&lt;p&gt;We propose a novel point cloud U-Net diffusion architecture for 3-D generative modeling capable of generating high-quality and diverse 3-D shapes while maintaining fast generation times. Our network employs a dual-branch architecture, combining the high-resolution representations of points with the computational efficiency of sparse voxels. Our fastest variant outperforms all nondiffusion generative approaches on unconditional shape generation, the most popular benchmark for evaluating point cloud generative models, while our largest model achieves state-of-the-art results among diffusion methods, with a runtime approximately 70% of the previously state-of-the-art point-voxel diffusion (PVD), measured on the same hardware setting. Beyond unconditional generation, we perform extensive evaluations, including conditional generation on all categories of ShapeNet, demonstrating the scalability of our model to larger datasets, and implicit generation, which allows our network to produce high-quality point clouds on fewer timesteps, further decreasing the generation time. Finally, we evaluate the architecture’s performance in point cloud completion and super-resolution. Our model excels in all tasks, establishing it as a state-of-the-art diffusion U-Net for point cloud generative modeling. The code is publicly available at https://github.com/JohnRomanelis/SPVD&lt;/p&gt;</content:encoded></item><item><title>Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles</title><link>https://arxiv.org/abs/2512.03454v1</link><guid>http://arxiv.org/abs/2512.03454v1</guid><pubDate>Wed, 03 Dec 2025 05:14:16 +0000</pubDate><dc:creator>Haicheng Liao</dc:creator><dc:creator>Huanming Shen</dc:creator><dc:creator>Bonan Wang</dc:creator><dc:creator>Yongkang Li</dc:creator><dc:creator>Yihong Tang</dc:creator><dc:creator>Chengyue Wang</dc:creator><dc:creator>Dingyi Zhuang</dc:creator><dc:creator>Kehua Chen</dc:creator><dc:creator>Hai Yang</dc:creator><dc:creator>Chengzhong Xu</dc:creator><dc:creator>Zhenning Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.
Published: 2025-12-03T05:14:16+00:00
Venue: arXiv
Score: 0.770 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haicheng Liao; Huanming Shen; Bonan Wang; Yongkang Li; Yihong Tang; Chengyue Wang; Dingyi Zhuang; Kehua Chen; Hai Yang; Chengzhong Xu; Zhenning Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (must_read)&lt;/p&gt;
&lt;p&gt;Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.&lt;/p&gt;</content:encoded></item><item><title>SAM3-I: Segment Anything with Instructions</title><link>https://arxiv.org/abs/2512.04585v1</link><guid>http://arxiv.org/abs/2512.04585v1</guid><pubDate>Thu, 04 Dec 2025 09:00:25 +0000</pubDate><dc:creator>Jingjing Li</dc:creator><dc:creator>Yue Feng</dc:creator><dc:creator>Yuchen Guo</dc:creator><dc:creator>Jincai Huang</dc:creator><dc:creator>Yongri Piao</dc:creator><dc:creator>Qi Bi</dc:creator><dc:creator>Miao Zhang</dc:creator><dc:creator>Xiaoqi Zhao</dc:creator><dc:creator>Qiang Chen</dc:creator><dc:creator>Shihao Zou</dc:creator><dc:creator>Wei Ji</dc:creator><dc:creator>Huchuan Lu</dc:creator><dc:creator>Li Cheng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Segment Anything Model 3 (SAM3) has advanced open-vocabulary segmentation through promptable concept segmentation, allowing users to segment all instances corresponding to a given concept, typically specified with short noun-phrase (NP) prompts. While this marks the first integration of language-level concepts within the SAM family, real-world usage typically requires far richer expressions that include attributes, spatial relations, functionalities, actions, states, and even implicit reasoning over instances. Currently, SAM3 relies on external multi-modal agents to convert complex instructions into NPs and then conduct iterative mask filtering. However, these NP-level concepts remain overly coarse, often failing to precisely represent a specific instance. In this work, we present SAM3-I, an enhanced framework that unifies concept-level understanding and instruction-level reasoning within the SAM family. SAM3-I introduces an instruction-aware cascaded adaptation mechanism that progressively aligns expressive instruction semantics with SAM3's existing vision-language representations, enabling direct instruction-following segmentation without sacrificing its original concept-driven capabilities. Furthermore, we design a structured instruction taxonomy spanning concept, simple, and complex levels, and develop a scalable data engine to construct a dataset with diverse instruction-mask pairs. Experiments show that SAM3-I delivers appealing performance, demonstrating that SAM3 can be effectively extended to follow natural-language instructions while preserving its strong concept grounding. We open-source SAM3-I and provide practical fine-tuning workflows, enabling researchers to adapt it to domain-specific applications. The source code is available here.
Published: 2025-12-04T09:00:25+00:00
Venue: arXiv
Score: 0.770 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jingjing Li; Yue Feng; Yuchen Guo; Jincai Huang; Yongri Piao; Qi Bi; Miao Zhang; Xiaoqi Zhao; Qiang Chen; Shihao Zou; Wei Ji; Huchuan Lu; Li Cheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (must_read)&lt;/p&gt;
&lt;p&gt;Segment Anything Model 3 (SAM3) has advanced open-vocabulary segmentation through promptable concept segmentation, allowing users to segment all instances corresponding to a given concept, typically specified with short noun-phrase (NP) prompts. While this marks the first integration of language-level concepts within the SAM family, real-world usage typically requires far richer expressions that include attributes, spatial relations, functionalities, actions, states, and even implicit reasoning over instances. Currently, SAM3 relies on external multi-modal agents to convert complex instructions into NPs and then conduct iterative mask filtering. However, these NP-level concepts remain overly coarse, often failing to precisely represent a specific instance. In this work, we present SAM3-I, an enhanced framework that unifies concept-level understanding and instruction-level reasoning within the SAM family. SAM3-I introduces an instruction-aware cascaded adaptation mechanism that progressively aligns expressive instruction semantics with SAM3&amp;#x27;s existing vision-language representations, enabling direct instruction-following segmentation without sacrificing its original concept-driven capabilities. Furthermore, we design a structured instruction taxonomy spanning concept, simple, and complex levels, and develop a scalable data engine to construct a dataset with diverse instruction-mask pairs. Experiments show that SAM3-I delivers appealing performance, demonstrating that SAM3 can be effectively extended to follow natural-language instructions while preserving its strong concept grounding. We open-source SAM3-I and provide practical fine-tuning workflows, enabling researchers to adapt it to domain-specific applications. The source code is available here.&lt;/p&gt;</content:encoded></item><item><title>RAMEN: Resolution-Adjustable Multimodal Encoder for Earth Observation</title><link>https://arxiv.org/abs/2512.05025v1</link><guid>http://arxiv.org/abs/2512.05025v1</guid><pubDate>Thu, 04 Dec 2025 17:40:17 +0000</pubDate><dc:creator>Nicolas Houdré</dc:creator><dc:creator>Diego Marcos</dc:creator><dc:creator>Hugo Riffaud de Turckheim</dc:creator><dc:creator>Dino Ienco</dc:creator><dc:creator>Laurent Wendling</dc:creator><dc:creator>Camille Kurtz</dc:creator><dc:creator>Sylvain Lobry</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Earth observation (EO) data spans a wide range of spatial, spectral, and temporal resolutions, from high-resolution optical imagery to low resolution multispectral products or radar time series. While recent foundation models have improved multimodal integration for learning meaningful representations, they often expect fixed input resolutions or are based on sensor-specific encoders limiting generalization across heterogeneous EO modalities. To overcome these limitations we introduce RAMEN, a resolution-adjustable multimodal encoder that learns a shared visual representation across EO data in a fully sensor-agnostic manner. RAMEN treats the modality and spatial and temporal resolutions as key input data features, enabling coherent analysis across modalities within a unified latent space. Its main methodological contribution is to define spatial resolution as a controllable output parameter, giving users direct control over the desired level of detail at inference and allowing explicit trade-offs between spatial precision and computational cost. We train a single, unified transformer encoder reconstructing masked multimodal EO data drawn from diverse sources, ensuring generalization across sensors and resolutions. Once pretrained, RAMEN transfers effectively to both known and unseen sensor configurations and outperforms larger state-of-the-art models on the community-standard PANGAEA benchmark, containing various multi-sensor and multi-resolution downstream tasks. Our code and pretrained model are available at https://github.com/nicolashoudre/RAMEN.
Published: 2025-12-04T17:40:17+00:00
Venue: arXiv
Score: 0.769 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Nicolas Houdré; Diego Marcos; Hugo Riffaud de Turckheim; Dino Ienco; Laurent Wendling; Camille Kurtz; Sylvain Lobry&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.769 (must_read)&lt;/p&gt;
&lt;p&gt;Earth observation (EO) data spans a wide range of spatial, spectral, and temporal resolutions, from high-resolution optical imagery to low resolution multispectral products or radar time series. While recent foundation models have improved multimodal integration for learning meaningful representations, they often expect fixed input resolutions or are based on sensor-specific encoders limiting generalization across heterogeneous EO modalities. To overcome these limitations we introduce RAMEN, a resolution-adjustable multimodal encoder that learns a shared visual representation across EO data in a fully sensor-agnostic manner. RAMEN treats the modality and spatial and temporal resolutions as key input data features, enabling coherent analysis across modalities within a unified latent space. Its main methodological contribution is to define spatial resolution as a controllable output parameter, giving users direct control over the desired level of detail at inference and allowing explicit trade-offs between spatial precision and computational cost. We train a single, unified transformer encoder reconstructing masked multimodal EO data drawn from diverse sources, ensuring generalization across sensors and resolutions. Once pretrained, RAMEN transfers effectively to both known and unseen sensor configurations and outperforms larger state-of-the-art models on the community-standard PANGAEA benchmark, containing various multi-sensor and multi-resolution downstream tasks. Our code and pretrained model are available at https://github.com/nicolashoudre/RAMEN.&lt;/p&gt;</content:encoded></item><item><title>TransDiff: Unsupervised Non-line-of-sight Imaging with Aperture-limited Relay Surfaces</title><link>https://doi.org/10.1109/tip.2025.3637694</link><guid>10.1109/tip.2025.3637694</guid><pubDate>Thu, 04 Dec 2025 18:39:34 +0000</pubDate><dc:creator>Xingyu Cui</dc:creator><dc:creator>Huanjing Yue</dc:creator><dc:creator>Shida Sun</dc:creator><dc:creator>Yue Li</dc:creator><dc:creator>Yusen Hou</dc:creator><dc:creator>Zhiwei Xiong</dc:creator><dc:creator>Jingyu Yang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3637694</prism:doi><description>Non-line-of-sight (NLOS) imaging aims to reconstruct scenes hidden from direct view and has broad applications in robotic vision, rescue operations, autonomous driving, and remote sensing. However, most existing methods rely on densely sampled transients from large, continuous relay surfaces, which limits their practicality in real-world scenarios with aperture constraints. To address this limitation, we propose an unsupervised zero-shot framework tailored for confocal NLOS imaging with aperture-limited relay surfaces. Our method leverages latent diffusion models to recover fully-sampled transients from undersampled versions by enforcing measurement consistency during the sampling process. To further improve recovered transient quality, we introduce a progressive recovery strategy that incrementally recovers missing transient values, effectively mitigating the impact of severe aperture limitations. In addition, to suppress error propagation during recovery, we develop a backpropagation-based error correction reconstruction algorithm that refines intermediate recovered transients by enforcing sparsity regularization in the voxel domain, enabling high-fidelity final reconstructions. Extensive experiments on both simulated and real-world datasets validate the robustness and generalization capability of our method across diverse aperture-limited relay surfaces. Notably, our method follows a zero-shot paradigm, requiring only a single pretraining stage without paired data or pattern-specific retraining, making it a more practical and generalizable framework for NLOS imaging.
Published: 2025-12-04T18:39:34+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.768 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xingyu Cui; Huanjing Yue; Shida Sun; Yue Li; Yusen Hou; Zhiwei Xiong; Jingyu Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3637694"&gt;10.1109/tip.2025.3637694&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.768 (must_read)&lt;/p&gt;
&lt;p&gt;Non-line-of-sight (NLOS) imaging aims to reconstruct scenes hidden from direct view and has broad applications in robotic vision, rescue operations, autonomous driving, and remote sensing. However, most existing methods rely on densely sampled transients from large, continuous relay surfaces, which limits their practicality in real-world scenarios with aperture constraints. To address this limitation, we propose an unsupervised zero-shot framework tailored for confocal NLOS imaging with aperture-limited relay surfaces. Our method leverages latent diffusion models to recover fully-sampled transients from undersampled versions by enforcing measurement consistency during the sampling process. To further improve recovered transient quality, we introduce a progressive recovery strategy that incrementally recovers missing transient values, effectively mitigating the impact of severe aperture limitations. In addition, to suppress error propagation during recovery, we develop a backpropagation-based error correction reconstruction algorithm that refines intermediate recovered transients by enforcing sparsity regularization in the voxel domain, enabling high-fidelity final reconstructions. Extensive experiments on both simulated and real-world datasets validate the robustness and generalization capability of our method across diverse aperture-limited relay surfaces. Notably, our method follows a zero-shot paradigm, requiring only a single pretraining stage without paired data or pattern-specific retraining, making it a more practical and generalizable framework for NLOS imaging.&lt;/p&gt;</content:encoded></item><item><title>CrossHypergraph: Consistent High-order Semantic Network for Few-shot Image Classification</title><link>https://doi.org/10.1109/tmm.2025.3639903</link><guid>10.1109/tmm.2025.3639903</guid><pubDate>Wed, 03 Dec 2025 18:43:18 +0000</pubDate><dc:creator>Yucheng Zhang</dc:creator><dc:creator>Hao Wang</dc:creator><dc:creator>Shuo Zhang</dc:creator><dc:creator>Biao Leng</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2025.3639903</prism:doi><description>Few-shot classification is a challenging task that recognizes novel classes by learning from few training instances. Metric-based models are currently the most effective solutions for few-shot classification. In these models, patch feature distances between query instances and support classes are calculated to achieve classification. However, it is difficult for patch-based methods to mine semantic information of support and query instances, leading to inaccurate feature similarity measures. To address these problems, we propose to construct CrossHypergraph based on hypergraph modeling. Specifically, we first align the local prototype vertices of support and query instances to model consistent hypergraph structures. Then a vertex-hyperedge-vertex-based interactive feature updating mechanism is designed to generate CrossHypergraph representation with consistent high-order semantic information for support and query instances. Based on the CrossHypergraph, we propose a consistent high-order semantic network, in which the high-order semantic-based weighted metric strategy is designed to achieve accurate classification. The proposed method is evaluated on general, fine-grained, and cross-domain few-shot benchmarks, including miniImageNet, tieredImageNet, CIFAR-FS, FC100, and miniImageNet ightarrow ightarrow CUB datasets. Experimental results show that our CrossHypergraph-based few-shot classifier generates consistent high-order semantic features, and achieves state-of-the-art performance on both 1-shot and 5-shot tasks.
Published: 2025-12-03T18:43:18+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.768 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yucheng Zhang; Hao Wang; Shuo Zhang; Biao Leng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2025.3639903"&gt;10.1109/tmm.2025.3639903&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.768 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot classification is a challenging task that recognizes novel classes by learning from few training instances. Metric-based models are currently the most effective solutions for few-shot classification. In these models, patch feature distances between query instances and support classes are calculated to achieve classification. However, it is difficult for patch-based methods to mine semantic information of support and query instances, leading to inaccurate feature similarity measures. To address these problems, we propose to construct CrossHypergraph based on hypergraph modeling. Specifically, we first align the local prototype vertices of support and query instances to model consistent hypergraph structures. Then a vertex-hyperedge-vertex-based interactive feature updating mechanism is designed to generate CrossHypergraph representation with consistent high-order semantic information for support and query instances. Based on the CrossHypergraph, we propose a consistent high-order semantic network, in which the high-order semantic-based weighted metric strategy is designed to achieve accurate classification. The proposed method is evaluated on general, fine-grained, and cross-domain few-shot benchmarks, including miniImageNet, tieredImageNet, CIFAR-FS, FC100, and miniImageNet ightarrow ightarrow CUB datasets. Experimental results show that our CrossHypergraph-based few-shot classifier generates consistent high-order semantic features, and achieves state-of-the-art performance on both 1-shot and 5-shot tasks.&lt;/p&gt;</content:encoded></item><item><title>PCF-LLM: Scaling LLMs for Multimodal Understanding of Structured Scientific Data in Photonic Crystal Fiber Sensors</title><link>https://doi.org/10.1016/j.inffus.2025.104022</link><guid>10.1016/j.inffus.2025.104022</guid><pubDate>Thu, 04 Dec 2025 08:08:52 +0000</pubDate><dc:creator>Shengchao Chen</dc:creator><dc:creator>Geyao Hu</dc:creator><dc:creator>Sufen Ren</dc:creator><dc:creator>Ting Shu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104022</prism:doi><description>Photonic crystal fibers (PCFs) exhibit complex and highly tunable structure–property relationships, making them promising for diverse photonic applications but challenging for accurate modeling and inverse design. Traditional numerical solvers offer high fidelity but are computationally expensive, while existing learning-based approaches are typically limited to narrow, single-task objectives and generalize poorly to unseen structures. We define PCF understanding as the capability to jointly reason over numerical PCF geometry–property mappings and textual descriptions, enabling four core tasks: optical property prediction, inverse design suggestion, structural description generation, and property interpretation. To address these, we propose PCF-LLM, a scalable multimodal framework that adapts pretrained large language models (LLMs) for unified PCF understanding. PCF-LLM incorporates a cross-modality alignment mechanism to fuse structured PCF geometry and optical properties with language prompts, and employs parameter-efficient fine-tuning via Low-Rank Adaptation. To enable such modeling, we curate PCF-MM-170K, the first large-scale multimodal PCF dataset comprising 170,000 samples across four representative structures, each annotated with high-fidelity optical simulations and fine-grained textual descriptions. Extensive experiments across multiple LLMs demonstrate that PCF-LLM achieves high accuracy, strong physical consistency, and robust cross-task generalization, advancing the use of LLMs for scientific discovery in photonics.
Published: 2025-12-04T08:08:52+00:00
Venue: Information Fusion
Score: 0.767 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shengchao Chen; Geyao Hu; Sufen Ren; Ting Shu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104022"&gt;10.1016/j.inffus.2025.104022&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.767 (must_read)&lt;/p&gt;
&lt;p&gt;Photonic crystal fibers (PCFs) exhibit complex and highly tunable structure–property relationships, making them promising for diverse photonic applications but challenging for accurate modeling and inverse design. Traditional numerical solvers offer high fidelity but are computationally expensive, while existing learning-based approaches are typically limited to narrow, single-task objectives and generalize poorly to unseen structures. We define PCF understanding as the capability to jointly reason over numerical PCF geometry–property mappings and textual descriptions, enabling four core tasks: optical property prediction, inverse design suggestion, structural description generation, and property interpretation. To address these, we propose PCF-LLM, a scalable multimodal framework that adapts pretrained large language models (LLMs) for unified PCF understanding. PCF-LLM incorporates a cross-modality alignment mechanism to fuse structured PCF geometry and optical properties with language prompts, and employs parameter-efficient fine-tuning via Low-Rank Adaptation. To enable such modeling, we curate PCF-MM-170K, the first large-scale multimodal PCF dataset comprising 170,000 samples across four representative structures, each annotated with high-fidelity optical simulations and fine-grained textual descriptions. Extensive experiments across multiple LLMs demonstrate that PCF-LLM achieves high accuracy, strong physical consistency, and robust cross-task generalization, advancing the use of LLMs for scientific discovery in photonics.&lt;/p&gt;</content:encoded></item></channel></rss>