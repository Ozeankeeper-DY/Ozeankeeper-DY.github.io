<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Mon, 01 Dec 2025 04:17:28 +0000</lastBuildDate><item><title>GLUE3D: General Language Understanding Evaluation for 3D Point Clouds</title><link>https://doi.org/10.1016/j.inffus.2025.104007</link><guid>10.1016/j.inffus.2025.104007</guid><pubDate>Sat, 29 Nov 2025 04:50:54 +0000</pubDate><dc:creator>Giorgio Mariani</dc:creator><dc:creator>Alessandro Raganato</dc:creator><dc:creator>Simone Melzi</dc:creator><dc:creator>Gabriella Pasi</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104007</prism:doi><description>Multimodal Large Language Models have achieved impressive results on text and image benchmarks, yet their capacity to ground language in 3D geometry is still largely unexplored. Existing 3D evaluations are either confined to specialised domains, such as indoor scans, or hampered by poor texture fidelity, and none allow a fair, modality-aligned comparison with the 2D counterparts. Without a rigorous benchmark, it remains unclear whether current 3D-aware models genuinely grasp shape, colour, pose, and quantity, or merely echo memorised textual priors.
Published: 2025-11-29T04:50:54+00:00
Venue: Information Fusion
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Giorgio Mariani; Alessandro Raganato; Simone Melzi; Gabriella Pasi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104007"&gt;10.1016/j.inffus.2025.104007&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal Large Language Models have achieved impressive results on text and image benchmarks, yet their capacity to ground language in 3D geometry is still largely unexplored. Existing 3D evaluations are either confined to specialised domains, such as indoor scans, or hampered by poor texture fidelity, and none allow a fair, modality-aligned comparison with the 2D counterparts. Without a rigorous benchmark, it remains unclear whether current 3D-aware models genuinely grasp shape, colour, pose, and quantity, or merely echo memorised textual priors.&lt;/p&gt;</content:encoded></item><item><title>Guiding the Inner Eye: A Framework for Hierarchical and Flexible Visual Grounded Reasoning</title><link>https://arxiv.org/abs/2511.22172v1</link><guid>http://arxiv.org/abs/2511.22172v1</guid><pubDate>Thu, 27 Nov 2025 07:18:25 +0000</pubDate><dc:creator>Zhaoyang Wei</dc:creator><dc:creator>Wenchao Ding</dc:creator><dc:creator>Yanchao Hao</dc:creator><dc:creator>Xi Chen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Models capable of "thinking with images" by dynamically grounding their reasoning in visual evidence represent a major leap in multimodal AI. However, replicating and advancing this ability is non-trivial, with current methods often trapped between the instability of end-to-end reinforcement learning (RL) and the rigidity of supervised fine-tuning (SFT). This leads to models that either struggle to learn or lack the cognitive flexibility required for complex, real-world scenes. To navigate this dilemma, we introduce GRiP (Guided Reasoning and Perception), a novel two-stage training framework that cultivates robust and flexible visual grounded reasoning by explicitly guiding the model's perceptual focus and logical pathways. GRiP's core lies in its cognitive-enhanced RL stage, which features two key innovations: (1) a Salience-Weighted IoU Reward that incentivizes the model to prioritize the localization of mission-critical objects over trivial distractors, and (2) a Multi-Heuristic Reward that encourages cognitive flexibility by rewarding diverse yet logically valid reasoning pathways. Initialized from the Qwen2.5-VL-7B model, GRiP demonstrates significant performance gains across multiple challenging benchmarks. It achieves state-of-the-art results among open-source models on the highly challenging TreeBench and V* Bench, proving its effectiveness in complex visual reasoning. Our work demonstrates that moving beyond simplistic rewards and instead guiding models with cognitively-inspired signals for what to see and how to think is crucial for unlocking the next level of multimodal intelligence. The code will be made publicly available.
Published: 2025-11-27T07:18:25+00:00
Venue: arXiv
Score: 0.765 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhaoyang Wei; Wenchao Ding; Yanchao Hao; Xi Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (must_read)&lt;/p&gt;
&lt;p&gt;Models capable of &amp;quot;thinking with images&amp;quot; by dynamically grounding their reasoning in visual evidence represent a major leap in multimodal AI. However, replicating and advancing this ability is non-trivial, with current methods often trapped between the instability of end-to-end reinforcement learning (RL) and the rigidity of supervised fine-tuning (SFT). This leads to models that either struggle to learn or lack the cognitive flexibility required for complex, real-world scenes. To navigate this dilemma, we introduce GRiP (Guided Reasoning and Perception), a novel two-stage training framework that cultivates robust and flexible visual grounded reasoning by explicitly guiding the model&amp;#x27;s perceptual focus and logical pathways. GRiP&amp;#x27;s core lies in its cognitive-enhanced RL stage, which features two key innovations: (1) a Salience-Weighted IoU Reward that incentivizes the model to prioritize the localization of mission-critical objects over trivial distractors, and (2) a Multi-Heuristic Reward that encourages cognitive flexibility by rewarding diverse yet logically valid reasoning pathways. Initialized from the Qwen2.5-VL-7B model, GRiP demonstrates significant performance gains across multiple challenging benchmarks. It achieves state-of-the-art results among open-source models on the highly challenging TreeBench and V* Bench, proving its effectiveness in complex visual reasoning. Our work demonstrates that moving beyond simplistic rewards and instead guiding models with cognitively-inspired signals for what to see and how to think is crucial for unlocking the next level of multimodal intelligence. The code will be made publicly available.&lt;/p&gt;</content:encoded></item><item><title>AgriCoT: A Chain-of-Thought Benchmark for Evaluating Reasoning in Vision-Language Models for Agriculture</title><link>https://arxiv.org/abs/2511.23253v1</link><guid>http://arxiv.org/abs/2511.23253v1</guid><pubDate>Fri, 28 Nov 2025 15:02:19 +0000</pubDate><dc:creator>Yibin Wen</dc:creator><dc:creator>Qingmei Li</dc:creator><dc:creator>Zi Ye</dc:creator><dc:creator>Jiarui Zhang</dc:creator><dc:creator>Jing Wu</dc:creator><dc:creator>Zurong Mai</dc:creator><dc:creator>Shuohong Lou</dc:creator><dc:creator>Yuhang Chen</dc:creator><dc:creator>Henglian Huang</dc:creator><dc:creator>Xiaoya Fan</dc:creator><dc:creator>Yang Zhang</dc:creator><dc:creator>Lingyuan Zhao</dc:creator><dc:creator>Haohuan Fu</dc:creator><dc:creator>Huang Jianxi</dc:creator><dc:creator>Juepeng Zheng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advancements in Vision-Language Models (VLMs) have significantly transformed various industries. In agriculture, these dual-modal capabilities offer promising applications such as precision farming, crop monitoring, pest detection, and environmental sustainability. While several Visual Question Answering (VQA) datasets and benchmarks have been developed to evaluate VLM performance, they often fail to adequately assess the critical reasoning and problem-solving skills required in complex agricultural contexts. To address this gap, we introduce AgriCoT, a VQA dataset that incorporates Chain-of-Thought (CoT) reasoning, specifically designed to evaluate the reasoning capabilities of VLMs. With 4,535 carefully curated samples, AgriCoT offers a comprehensive and robust evaluation of reasoning abilities for VLMs, particularly in zero-shot scenarios, by focusing on their capacity to engage in logical reasoning and effective problem-solving. Our evaluations, conducted with 26 representative VLMs, including both proprietary and open-source models, reveal that while some proprietary models excel at answering questions, there is a notable and significant gap in their reasoning capabilities. This underscores the importance of incorporating CoT for more precise and effective assessments. Our dataset are available at https://huggingface.co/datasets/wenyb/AgriCoT.
Published: 2025-11-28T15:02:19+00:00
Venue: arXiv
Score: 0.765 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yibin Wen; Qingmei Li; Zi Ye; Jiarui Zhang; Jing Wu; Zurong Mai; Shuohong Lou; Yuhang Chen; Henglian Huang; Xiaoya Fan; Yang Zhang; Lingyuan Zhao; Haohuan Fu; Huang Jianxi; Juepeng Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (must_read)&lt;/p&gt;
&lt;p&gt;Recent advancements in Vision-Language Models (VLMs) have significantly transformed various industries. In agriculture, these dual-modal capabilities offer promising applications such as precision farming, crop monitoring, pest detection, and environmental sustainability. While several Visual Question Answering (VQA) datasets and benchmarks have been developed to evaluate VLM performance, they often fail to adequately assess the critical reasoning and problem-solving skills required in complex agricultural contexts. To address this gap, we introduce AgriCoT, a VQA dataset that incorporates Chain-of-Thought (CoT) reasoning, specifically designed to evaluate the reasoning capabilities of VLMs. With 4,535 carefully curated samples, AgriCoT offers a comprehensive and robust evaluation of reasoning abilities for VLMs, particularly in zero-shot scenarios, by focusing on their capacity to engage in logical reasoning and effective problem-solving. Our evaluations, conducted with 26 representative VLMs, including both proprietary and open-source models, reveal that while some proprietary models excel at answering questions, there is a notable and significant gap in their reasoning capabilities. This underscores the importance of incorporating CoT for more precise and effective assessments. Our dataset are available at https://huggingface.co/datasets/wenyb/AgriCoT.&lt;/p&gt;</content:encoded></item><item><title>DivineTree: All-in-One 3D Tree Modeling with Diverse and Fused Visual Guidance</title><link>https://doi.org/10.1016/j.inffus.2025.104013</link><guid>10.1016/j.inffus.2025.104013</guid><pubDate>Sat, 29 Nov 2025 08:10:00 +0000</pubDate><dc:creator>Jiabo Xu</dc:creator><dc:creator>Bo Su</dc:creator><dc:creator>Jingbo Wei</dc:creator><dc:creator>Xiangyun Hu</dc:creator><dc:creator>Hengming Dai</dc:creator><dc:creator>Tao Ke</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104013</prism:doi><description>3D tree modeling is crucial in fields ranging from gaming and film production to environmental science. However, current learning-based methods are typically restricted to a single input modality (e.g., only images or only point clouds), and requiring difficult-to-acquire paired training data for each new input type. To overcome these limitations, we propose DivineTree, a novel method that generates 3D trees from diverse visual guidance-including point clouds (LiDAR or image-matched), images (photos, sketches, paintings), and crown polygons-using a single, unified model, in a zero-shot manner without requiring paired data or retraining. DivineTree consists of two core components: 1) An unconditional diffusion model trained on synthetic data to learn the distribution of 3D tree structures, represented as sequences of 4D line segments. 2) A Point Guidance sampling technique that incorporates diverse visual inputs as spatial constraints during the generative process, guiding the diffusion to produce a 3D tree that matches the input. Extensive experiments demonstrate that our method rapidly generates realistic and geometrically accurate 3D trees. On the challenging 10-forest benchmark for crown-to-tree generation, DivineTree achieves state-of-the-art performance in both geometric accuracy and visual realism. Furthermore, our method enables the fusion of multiple inputs, such as combining both side-view and top-view conditions, to generate 3D tree models that simultaneously satisfy multiple constraints.
Published: 2025-11-29T08:10:00+00:00
Venue: Information Fusion
Score: 0.759 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiabo Xu; Bo Su; Jingbo Wei; Xiangyun Hu; Hengming Dai; Tao Ke&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104013"&gt;10.1016/j.inffus.2025.104013&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.759 (must_read)&lt;/p&gt;
&lt;p&gt;3D tree modeling is crucial in fields ranging from gaming and film production to environmental science. However, current learning-based methods are typically restricted to a single input modality (e.g., only images or only point clouds), and requiring difficult-to-acquire paired training data for each new input type. To overcome these limitations, we propose DivineTree, a novel method that generates 3D trees from diverse visual guidance-including point clouds (LiDAR or image-matched), images (photos, sketches, paintings), and crown polygons-using a single, unified model, in a zero-shot manner without requiring paired data or retraining. DivineTree consists of two core components: 1) An unconditional diffusion model trained on synthetic data to learn the distribution of 3D tree structures, represented as sequences of 4D line segments. 2) A Point Guidance sampling technique that incorporates diverse visual inputs as spatial constraints during the generative process, guiding the diffusion to produce a 3D tree that matches the input. Extensive experiments demonstrate that our method rapidly generates realistic and geometrically accurate 3D trees. On the challenging 10-forest benchmark for crown-to-tree generation, DivineTree achieves state-of-the-art performance in both geometric accuracy and visual realism. Furthermore, our method enables the fusion of multiple inputs, such as combining both side-view and top-view conditions, to generate 3D tree models that simultaneously satisfy multiple constraints.&lt;/p&gt;</content:encoded></item><item><title>Video-CoM: Interactive Video Reasoning via Chain of Manipulations</title><link>https://arxiv.org/abs/2511.23477v1</link><guid>http://arxiv.org/abs/2511.23477v1</guid><pubDate>Fri, 28 Nov 2025 18:59:57 +0000</pubDate><dc:creator>Hanoona Rasheed</dc:creator><dc:creator>Mohammed Zumri</dc:creator><dc:creator>Muhammad Maaz</dc:creator><dc:creator>Ming-Hsuan Yang</dc:creator><dc:creator>Fahad Shahbaz Khan</dc:creator><dc:creator>Salman Khan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent multimodal large language models (MLLMs) have advanced video understanding, yet most still "think about videos" ie once a video is encoded, reasoning unfolds entirely in text, treating visual input as a static context. This passive paradigm creates a semantic bottleneck: models cannot rewatch, refocus, or verify evidence, leading to shallow visual reasoning on tasks requiring fine grained spatio temporal understanding. In this work, we introduce Interactive Video Reasoning, a new paradigm that transforms video into an active cognitive workspace, enabling models to "think with videos". Our model, Video CoM, reasons through a Chain of Manipulations (CoM), performing iterative visual actions to gather and refine evidence. To support this behavior, we construct Video CoM Instruct, an 18K instruction tuning dataset curated for multi step manipulation reasoning. Beyond supervised learning, we further optimize the manipulation policy via reinforcement learning with reasoning aware Group Relative Policy Optimization (GRPO). Unlike prior work that relies solely on sparse answer rewards, our method introduces step level reasoning rewards, guiding the model toward grounded and consistent reasoning. Video CoM achieves strong results across nine video reasoning benchmarks, improving average performance by 3.6 percent over recent state of the art models, while training on only 25K SFT and 3K GRPO video samples, significantly fewer than comparable large scale models. Ablation studies demonstrate that reasoning aware rewards improve both accuracy and interpretability. Code: https://github.com/mbzuai-oryx/Video-CoM
Published: 2025-11-28T18:59:57+00:00
Venue: arXiv
Score: 0.758 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hanoona Rasheed; Mohammed Zumri; Muhammad Maaz; Ming-Hsuan Yang; Fahad Shahbaz Khan; Salman Khan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.758 (must_read)&lt;/p&gt;
&lt;p&gt;Recent multimodal large language models (MLLMs) have advanced video understanding, yet most still &amp;quot;think about videos&amp;quot; ie once a video is encoded, reasoning unfolds entirely in text, treating visual input as a static context. This passive paradigm creates a semantic bottleneck: models cannot rewatch, refocus, or verify evidence, leading to shallow visual reasoning on tasks requiring fine grained spatio temporal understanding. In this work, we introduce Interactive Video Reasoning, a new paradigm that transforms video into an active cognitive workspace, enabling models to &amp;quot;think with videos&amp;quot;. Our model, Video CoM, reasons through a Chain of Manipulations (CoM), performing iterative visual actions to gather and refine evidence. To support this behavior, we construct Video CoM Instruct, an 18K instruction tuning dataset curated for multi step manipulation reasoning. Beyond supervised learning, we further optimize the manipulation policy via reinforcement learning with reasoning aware Group Relative Policy Optimization (GRPO). Unlike prior work that relies solely on sparse answer rewards, our method introduces step level reasoning rewards, guiding the model toward grounded and consistent reasoning. Video CoM achieves strong results across nine video reasoning benchmarks, improving average performance by 3.6 percent over recent state of the art models, while training on only 25K SFT and 3K GRPO video samples, significantly fewer than comparable large scale models. Ablation studies demonstrate that reasoning aware rewards improve both accuracy and interpretability. Code: https://github.com/mbzuai-oryx/Video-CoM&lt;/p&gt;</content:encoded></item><item><title>Reducing semantic ambiguity in open-vocabulary remote sensing image segmentation via knowledge graph-enhanced class representations</title><link>https://doi.org/10.1016/j.isprsjprs.2025.11.029</link><guid>10.1016/j.isprsjprs.2025.11.029</guid><pubDate>Sun, 30 Nov 2025 08:00:16 +0000</pubDate><dc:creator>Wubiao Huang</dc:creator><dc:creator>Huchen Li</dc:creator><dc:creator>Shuai Zhang</dc:creator><dc:creator>Fei Deng</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.11.029</prism:doi><description>Open-vocabulary semantic segmentation (OVSS) task presents a new challenge for remote sensing image understanding by requiring the recognition of previously unseen or novel classes during inference. However, existing OVSS methods often suffer from severe semantic ambiguity in land cover classification due to inconsistent naming conventions, hierarchical dependency, and insufficient semantic proximity in the embedding space. To address these issues, we propose KG-OVRSeg, a novel framework that mitigates semantic ambiguity by aggregating structured knowledge from a knowledge graph. This approach significantly enhances intra-class compactness and inter-class separability in the embedding space, thereby fundamentally enhancing class representations. We design a knowledge graph-enhanced class encoder (KGCE) that generates enriched class embeddings by querying hypernym–hyponym and synonym relationships within a localized knowledge graph. These enhanced embeddings are further utilized by a class attention gradual decoder (CAGD), which leverages a class-aware attention mechanism and guidance refinement to guide feature decoding. Extensive experiments on seven publicly available datasets demonstrated that KG-OVRSeg achieves state-of-the-art performance, with a mean mF1 of 51.65% and a mean mIoU of 39.18%, surpassing previous methods by 8.06% mF1 and 6.52% mIoU. Comprehensive ablation and visual analyses confirmed that KGCE significantly improves intra-class semantic compactness and inter-class separability in the embedding space, playing a crucial role in mitigating semantic inconsistency. Our work offers a robust and scalable solution for ambiguity-aware open-vocabulary tasks in remote sensing. The code is publicly available at https://github.com/HuangWBill/KG-OVRSeg .
Published: 2025-11-30T08:00:16+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.757 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wubiao Huang; Huchen Li; Shuai Zhang; Fei Deng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.11.029"&gt;10.1016/j.isprsjprs.2025.11.029&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.757 (must_read)&lt;/p&gt;
&lt;p&gt;Open-vocabulary semantic segmentation (OVSS) task presents a new challenge for remote sensing image understanding by requiring the recognition of previously unseen or novel classes during inference. However, existing OVSS methods often suffer from severe semantic ambiguity in land cover classification due to inconsistent naming conventions, hierarchical dependency, and insufficient semantic proximity in the embedding space. To address these issues, we propose KG-OVRSeg, a novel framework that mitigates semantic ambiguity by aggregating structured knowledge from a knowledge graph. This approach significantly enhances intra-class compactness and inter-class separability in the embedding space, thereby fundamentally enhancing class representations. We design a knowledge graph-enhanced class encoder (KGCE) that generates enriched class embeddings by querying hypernym–hyponym and synonym relationships within a localized knowledge graph. These enhanced embeddings are further utilized by a class attention gradual decoder (CAGD), which leverages a class-aware attention mechanism and guidance refinement to guide feature decoding. Extensive experiments on seven publicly available datasets demonstrated that KG-OVRSeg achieves state-of-the-art performance, with a mean mF1 of 51.65% and a mean mIoU of 39.18%, surpassing previous methods by 8.06% mF1 and 6.52% mIoU. Comprehensive ablation and visual analyses confirmed that KGCE significantly improves intra-class semantic compactness and inter-class separability in the embedding space, playing a crucial role in mitigating semantic inconsistency. Our work offers a robust and scalable solution for ambiguity-aware open-vocabulary tasks in remote sensing. The code is publicly available at https://github.com/HuangWBill/KG-OVRSeg .&lt;/p&gt;</content:encoded></item><item><title>SpaceMind: Camera-Guided Modality Fusion for Spatial Reasoning in Vision-Language Models</title><link>https://arxiv.org/abs/2511.23075v1</link><guid>http://arxiv.org/abs/2511.23075v1</guid><pubDate>Fri, 28 Nov 2025 11:04:21 +0000</pubDate><dc:creator>Ruosen Zhao</dc:creator><dc:creator>Zhikang Zhang</dc:creator><dc:creator>Jialei Xu</dc:creator><dc:creator>Jiahao Chang</dc:creator><dc:creator>Dong Chen</dc:creator><dc:creator>Lingyun Li</dc:creator><dc:creator>Weijian Sun</dc:creator><dc:creator>Zizhuang Wei</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large vision-language models (VLMs) show strong multimodal understanding but still struggle with 3D spatial reasoning, such as distance estimation, size comparison, and cross-view consistency. Existing 3D-aware methods either depend on auxiliary 3D information or enhance RGB-only VLMs with geometry encoders through shallow feature fusion. We propose SpaceMind, a multimodal large language model explicitly designed for spatial reasoning solely from RGB inputs. The model adopts a dual-encoder architecture, integrating VGGT as a spatial understanding encoder and InternViT as a 2D visual encoder. The key idea is to treat the camera representation as an active guiding modality rather than passive metadata. Specifically, SpaceMind introduces a lightweight Camera-Guided Modality Fusion module before the language model to replace shallow fusion. It applies camera-conditioned biasing to spatial tokens, assigns query-independent weights reflecting their geometric importance, and uses the camera embedding to gate the fused representation. Empirically, SpaceMind establishes new state-of-the-art results on VSI-Bench, SQA3D and SPBench, surpassing both open and proprietary systems on VSI-Bench and SPBench by large margins and achieving state-of-the-art performance on SQA3D. These results demonstrate that camera-guided modality fusion is an effective and practical inductive bias for equipping VLMs with genuinely spatially grounded intelligence. We will release code and model checkpoints to support future research.
Published: 2025-11-28T11:04:21+00:00
Venue: arXiv
Score: 0.755 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruosen Zhao; Zhikang Zhang; Jialei Xu; Jiahao Chang; Dong Chen; Lingyun Li; Weijian Sun; Zizhuang Wei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.755 (must_read)&lt;/p&gt;
&lt;p&gt;Large vision-language models (VLMs) show strong multimodal understanding but still struggle with 3D spatial reasoning, such as distance estimation, size comparison, and cross-view consistency. Existing 3D-aware methods either depend on auxiliary 3D information or enhance RGB-only VLMs with geometry encoders through shallow feature fusion. We propose SpaceMind, a multimodal large language model explicitly designed for spatial reasoning solely from RGB inputs. The model adopts a dual-encoder architecture, integrating VGGT as a spatial understanding encoder and InternViT as a 2D visual encoder. The key idea is to treat the camera representation as an active guiding modality rather than passive metadata. Specifically, SpaceMind introduces a lightweight Camera-Guided Modality Fusion module before the language model to replace shallow fusion. It applies camera-conditioned biasing to spatial tokens, assigns query-independent weights reflecting their geometric importance, and uses the camera embedding to gate the fused representation. Empirically, SpaceMind establishes new state-of-the-art results on VSI-Bench, SQA3D and SPBench, surpassing both open and proprietary systems on VSI-Bench and SPBench by large margins and achieving state-of-the-art performance on SQA3D. These results demonstrate that camera-guided modality fusion is an effective and practical inductive bias for equipping VLMs with genuinely spatially grounded intelligence. We will release code and model checkpoints to support future research.&lt;/p&gt;</content:encoded></item><item><title>Geometrically-Constrained Agent for Spatial Reasoning</title><link>https://arxiv.org/abs/2511.22659v1</link><guid>http://arxiv.org/abs/2511.22659v1</guid><pubDate>Thu, 27 Nov 2025 17:50:37 +0000</pubDate><dc:creator>Zeren Chen</dc:creator><dc:creator>Xiaoya Lu</dc:creator><dc:creator>Zhijie Zheng</dc:creator><dc:creator>Pengrui Li</dc:creator><dc:creator>Lehan He</dc:creator><dc:creator>Yijin Zhou</dc:creator><dc:creator>Jing Shao</dc:creator><dc:creator>Bohan Zhuang</dc:creator><dc:creator>Lu Sheng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision Language Models (VLMs) exhibit a fundamental semantic-to-geometric gap in spatial reasoning: they excel at qualitative semantic inference but their reasoning operates within a lossy semantic space, misaligned with high-fidelity geometry. Current paradigms fail to bridge this gap. Training-based methods suffer from an ``oracle paradox,'' learning flawed spatial logic from imperfect oracles. Tool-integrated methods constrain the final computation but critically leave the VLM's planning process unconstrained, resulting in geometrically flawed plans. In this work, we propose Geometrically-Constrained Agent (GCA), a training-free agentic paradigm that resolves this gap by introducing a formal task constraint. Specifically, we strategically decouples the VLM's role into two stages. First, acting as a semantic analyst, the VLM translates the user's ambiguous query into the formal, verifiable task constraint, which defines the reference frame and objective. Second, acting as a task solver, the VLM generates and executes tool calls strictly within the deterministic bounds defined by the constraint. This geometrically-constrained reasoning strategy successfully resolve the semantic-to-geometric gap, yielding a robust and verifiable reasoning pathway for spatial reasoning. Comprehensive experiments demonstrate that GCA achieves SOTA performance on multiple spatial reasoning benchmarks, surpassing existing training-based and tool-integrated methods by ~27%. Please see our homepage at https://gca-spatial-reasoning.github.io.
Published: 2025-11-27T17:50:37+00:00
Venue: arXiv
Score: 0.750 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zeren Chen; Xiaoya Lu; Zhijie Zheng; Pengrui Li; Lehan He; Yijin Zhou; Jing Shao; Bohan Zhuang; Lu Sheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.750 (must_read)&lt;/p&gt;
&lt;p&gt;Vision Language Models (VLMs) exhibit a fundamental semantic-to-geometric gap in spatial reasoning: they excel at qualitative semantic inference but their reasoning operates within a lossy semantic space, misaligned with high-fidelity geometry. Current paradigms fail to bridge this gap. Training-based methods suffer from an ``oracle paradox,&amp;#x27;&amp;#x27; learning flawed spatial logic from imperfect oracles. Tool-integrated methods constrain the final computation but critically leave the VLM&amp;#x27;s planning process unconstrained, resulting in geometrically flawed plans. In this work, we propose Geometrically-Constrained Agent (GCA), a training-free agentic paradigm that resolves this gap by introducing a formal task constraint. Specifically, we strategically decouples the VLM&amp;#x27;s role into two stages. First, acting as a semantic analyst, the VLM translates the user&amp;#x27;s ambiguous query into the formal, verifiable task constraint, which defines the reference frame and objective. Second, acting as a task solver, the VLM generates and executes tool calls strictly within the deterministic bounds defined by the constraint. This geometrically-constrained reasoning strategy successfully resolve the semantic-to-geometric gap, yielding a robust and verifiable reasoning pathway for spatial reasoning. Comprehensive experiments demonstrate that GCA achieves SOTA performance on multiple spatial reasoning benchmarks, surpassing existing training-based and tool-integrated methods by ~27%. Please see our homepage at https://gca-spatial-reasoning.github.io.&lt;/p&gt;</content:encoded></item><item><title>ReAG: Reasoning-Augmented Generation for Knowledge-based Visual Question Answering</title><link>https://arxiv.org/abs/2511.22715v1</link><guid>http://arxiv.org/abs/2511.22715v1</guid><pubDate>Thu, 27 Nov 2025 19:01:02 +0000</pubDate><dc:creator>Alberto Compagnoni</dc:creator><dc:creator>Marco Morini</dc:creator><dc:creator>Sara Sarto</dc:creator><dc:creator>Federico Cocchi</dc:creator><dc:creator>Davide Caffagni</dc:creator><dc:creator>Marcella Cornia</dc:creator><dc:creator>Lorenzo Baraldi</dc:creator><dc:creator>Rita Cucchiara</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal Large Language Models (MLLMs) have shown impressive capabilities in jointly understanding text, images, and videos, often evaluated via Visual Question Answering (VQA). However, even state-of-the-art MLLMs struggle with domain-specific or knowledge-intensive queries, where relevant information is underrepresented in pre-training data. Knowledge-based VQA (KB-VQA) addresses this by retrieving external documents to condition answer generation, but current retrieval-augmented approaches suffer from low precision, noisy passages, and limited reasoning. To address this, we propose ReAG, a novel Reasoning-Augmented Multimodal RAG approach that combines coarse- and fine-grained retrieval with a critic model that filters irrelevant passages, ensuring high-quality additional context. The model follows a multi-stage training strategy leveraging reinforcement learning to enhance reasoning over retrieved content, while supervised fine-tuning serves only as a cold start. Extensive experiments on Encyclopedic-VQA and InfoSeek demonstrate that ReAG significantly outperforms prior methods, improving answer accuracy and providing interpretable reasoning grounded in retrieved evidence. Our source code is publicly available at: https://github.com/aimagelab/ReAG.
Published: 2025-11-27T19:01:02+00:00
Venue: arXiv
Score: 0.749 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Alberto Compagnoni; Marco Morini; Sara Sarto; Federico Cocchi; Davide Caffagni; Marcella Cornia; Lorenzo Baraldi; Rita Cucchiara&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.749 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal Large Language Models (MLLMs) have shown impressive capabilities in jointly understanding text, images, and videos, often evaluated via Visual Question Answering (VQA). However, even state-of-the-art MLLMs struggle with domain-specific or knowledge-intensive queries, where relevant information is underrepresented in pre-training data. Knowledge-based VQA (KB-VQA) addresses this by retrieving external documents to condition answer generation, but current retrieval-augmented approaches suffer from low precision, noisy passages, and limited reasoning. To address this, we propose ReAG, a novel Reasoning-Augmented Multimodal RAG approach that combines coarse- and fine-grained retrieval with a critic model that filters irrelevant passages, ensuring high-quality additional context. The model follows a multi-stage training strategy leveraging reinforcement learning to enhance reasoning over retrieved content, while supervised fine-tuning serves only as a cold start. Extensive experiments on Encyclopedic-VQA and InfoSeek demonstrate that ReAG significantly outperforms prior methods, improving answer accuracy and providing interpretable reasoning grounded in retrieved evidence. Our source code is publicly available at: https://github.com/aimagelab/ReAG.&lt;/p&gt;</content:encoded></item><item><title>Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models</title><link>https://arxiv.org/abs/2511.23478v1</link><guid>http://arxiv.org/abs/2511.23478v1</guid><pubDate>Fri, 28 Nov 2025 18:59:58 +0000</pubDate><dc:creator>Muhammad Maaz</dc:creator><dc:creator>Hanoona Rasheed</dc:creator><dc:creator>Fahad Shahbaz Khan</dc:creator><dc:creator>Salman Khan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reasoning over dynamic visual content remains a central challenge for multimodal large language models. Recent thinking models generate explicit reasoning traces for interpretability; however, their reasoning often appears convincing while being logically inconsistent or weakly grounded in visual evidence. We identify and formalize these issues through two diagnostic metrics: Think Answer Consistency (TAC), which measures the alignment between reasoning and answers, and Video Attention Score (VAS), which captures the extent to which reasoning depends on visual versus textual cues. Analysis across 11 video reasoning benchmarks shows that current models rely heavily on linguistic priors rather than visual content. To address this, we propose a reinforcement learning approach that enhances both temporal precision and reasoning consistency. Our approach combines timestamp aware supervised fine tuning with Group Relative Policy Optimization (GRPO) guided by a novel Temporal Alignment Reward (TAR). This dual step post training stage encourages temporally aligned and causally coherent video reasoning. The resulting model, Video R2, achieves consistently higher TAC, VAS, and accuracy across multiple benchmarks, demonstrating that improvements in temporal alignment and reasoning coherence lead to more accurate and trustworthy video understanding. Our code, dataset, and model will be open sourced.
Published: 2025-11-28T18:59:58+00:00
Venue: arXiv
Score: 0.749 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Muhammad Maaz; Hanoona Rasheed; Fahad Shahbaz Khan; Salman Khan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.749 (must_read)&lt;/p&gt;
&lt;p&gt;Reasoning over dynamic visual content remains a central challenge for multimodal large language models. Recent thinking models generate explicit reasoning traces for interpretability; however, their reasoning often appears convincing while being logically inconsistent or weakly grounded in visual evidence. We identify and formalize these issues through two diagnostic metrics: Think Answer Consistency (TAC), which measures the alignment between reasoning and answers, and Video Attention Score (VAS), which captures the extent to which reasoning depends on visual versus textual cues. Analysis across 11 video reasoning benchmarks shows that current models rely heavily on linguistic priors rather than visual content. To address this, we propose a reinforcement learning approach that enhances both temporal precision and reasoning consistency. Our approach combines timestamp aware supervised fine tuning with Group Relative Policy Optimization (GRPO) guided by a novel Temporal Alignment Reward (TAR). This dual step post training stage encourages temporally aligned and causally coherent video reasoning. The resulting model, Video R2, achieves consistently higher TAC, VAS, and accuracy across multiple benchmarks, demonstrating that improvements in temporal alignment and reasoning coherence lead to more accurate and trustworthy video understanding. Our code, dataset, and model will be open sourced.&lt;/p&gt;</content:encoded></item><item><title>From Illusion to Intention: Visual Rationale Learning for Vision-Language Reasoning</title><link>https://arxiv.org/abs/2511.23031v1</link><guid>http://arxiv.org/abs/2511.23031v1</guid><pubDate>Fri, 28 Nov 2025 09:52:56 +0000</pubDate><dc:creator>Changpeng Wang</dc:creator><dc:creator>Haozhe Wang</dc:creator><dc:creator>Xi Chen</dc:creator><dc:creator>Junhan Liu</dc:creator><dc:creator>Taofeng Xue</dc:creator><dc:creator>Chong Peng</dc:creator><dc:creator>Donglian Qi</dc:creator><dc:creator>Fangzhen Lin</dc:creator><dc:creator>Yunfeng Yan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advances in vision-language reasoning underscore the importance of thinking with images, where models actively ground their reasoning in visual evidence. Yet, prevailing frameworks treat visual actions as optional tools, boosting metrics but leaving reasoning ungrounded and crops ineffective. This gap gives rise to the illusion of thinking with images: models seem visually grounded but rely on context-agnostic actions that neither refine perception nor guide reasoning toward correct answers. We address this problem by reframing visual actions as core reasoning primitives rather than optional tools, which we term visual rationalization, the visual analogue of textual Chain-of-Thought. Building on this insight, we propose Visual Rationale Learning (ViRL), an end-to-end paradigm that grounds training in the visual rationale itself. ViRL integrates (1) Process Supervision with ground-truth rationales, (2) Objective Alignment via step-level reward shaping, and (3) Fine-Grained Credit Assignment to distinguish correct, redundant, and erroneous actions. By ensuring each action contributes meaningfully to the reasoning chain, ViRL enables models to "get the right answer for the right visual reason". Trained purely with end-to-end RL, ViRL achieves state-of-the-art results across benchmarks spanning perception, hallucination, and reasoning. This work establishes visual rationalization as a task-agnostic, process-grounded paradigm for building transparent, verifiable, and trustworthy vision-language models.
Published: 2025-11-28T09:52:56+00:00
Venue: arXiv
Score: 0.746 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Changpeng Wang; Haozhe Wang; Xi Chen; Junhan Liu; Taofeng Xue; Chong Peng; Donglian Qi; Fangzhen Lin; Yunfeng Yan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.746 (must_read)&lt;/p&gt;
&lt;p&gt;Recent advances in vision-language reasoning underscore the importance of thinking with images, where models actively ground their reasoning in visual evidence. Yet, prevailing frameworks treat visual actions as optional tools, boosting metrics but leaving reasoning ungrounded and crops ineffective. This gap gives rise to the illusion of thinking with images: models seem visually grounded but rely on context-agnostic actions that neither refine perception nor guide reasoning toward correct answers. We address this problem by reframing visual actions as core reasoning primitives rather than optional tools, which we term visual rationalization, the visual analogue of textual Chain-of-Thought. Building on this insight, we propose Visual Rationale Learning (ViRL), an end-to-end paradigm that grounds training in the visual rationale itself. ViRL integrates (1) Process Supervision with ground-truth rationales, (2) Objective Alignment via step-level reward shaping, and (3) Fine-Grained Credit Assignment to distinguish correct, redundant, and erroneous actions. By ensuring each action contributes meaningfully to the reasoning chain, ViRL enables models to &amp;quot;get the right answer for the right visual reason&amp;quot;. Trained purely with end-to-end RL, ViRL achieves state-of-the-art results across benchmarks spanning perception, hallucination, and reasoning. This work establishes visual rationalization as a task-agnostic, process-grounded paradigm for building transparent, verifiable, and trustworthy vision-language models.&lt;/p&gt;</content:encoded></item><item><title>Thinking by Doing: Building Efficient World Model Reasoning in LLMs via Multi-turn Interaction</title><link>https://arxiv.org/abs/2511.23476v1</link><guid>http://arxiv.org/abs/2511.23476v1</guid><pubDate>Fri, 28 Nov 2025 18:59:47 +0000</pubDate><dc:creator>Bao Shu</dc:creator><dc:creator>Yan Cai</dc:creator><dc:creator>Jianjian Sun</dc:creator><dc:creator>Chunrui Han</dc:creator><dc:creator>En Yu</dc:creator><dc:creator>Liang Zhao</dc:creator><dc:creator>Jingcheng Hu</dc:creator><dc:creator>Yinmin Zhang</dc:creator><dc:creator>Haoran Lv</dc:creator><dc:creator>Yuang Peng</dc:creator><dc:creator>Zheng Ge</dc:creator><dc:creator>Xiangyu Zhang</dc:creator><dc:creator>Daxin Jiang</dc:creator><dc:creator>Xiangyu Yue</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Developing robust world model reasoning is crucial for large language model (LLM) agents to plan and interact in complex environments. While multi-turn interaction offers a superior understanding of environmental dynamics via authentic feedback, current approaches often impose a rigid reasoning process, which constrains the model's active learning, ultimately hindering efficient world model reasoning. To address these issues, we explore world-model internalization through efficient interaction and active reasoning (WMAct), which liberates the model from structured reasoning, allowing the model to shape thinking directly through its doing, and achieves effective and efficient world model reasoning with two key mechanisms: (1) a reward rescaling mechanism adjusting outcome reward based on action efficacy to incentivize redundancy reduction and purposeful interaction; (2) an interaction frequency annealing strategy to progressively reduce the maximum allowed interaction turns, which compels the model to condense its learning and internalize environmental dynamics rather than over-relying on environmental cues. Our experiments on Sokoban, Maze, and Taxi show that WMAct yields effective world model reasoning capable of resolving tasks in a single turn that previously required multiple interactions and fosters strong transferability to complex environments, improving performance on a suite of reasoning benchmarks.
Published: 2025-11-28T18:59:47+00:00
Venue: arXiv
Score: 0.744 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bao Shu; Yan Cai; Jianjian Sun; Chunrui Han; En Yu; Liang Zhao; Jingcheng Hu; Yinmin Zhang; Haoran Lv; Yuang Peng; Zheng Ge; Xiangyu Zhang; Daxin Jiang; Xiangyu Yue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.744 (must_read)&lt;/p&gt;
&lt;p&gt;Developing robust world model reasoning is crucial for large language model (LLM) agents to plan and interact in complex environments. While multi-turn interaction offers a superior understanding of environmental dynamics via authentic feedback, current approaches often impose a rigid reasoning process, which constrains the model&amp;#x27;s active learning, ultimately hindering efficient world model reasoning. To address these issues, we explore world-model internalization through efficient interaction and active reasoning (WMAct), which liberates the model from structured reasoning, allowing the model to shape thinking directly through its doing, and achieves effective and efficient world model reasoning with two key mechanisms: (1) a reward rescaling mechanism adjusting outcome reward based on action efficacy to incentivize redundancy reduction and purposeful interaction; (2) an interaction frequency annealing strategy to progressively reduce the maximum allowed interaction turns, which compels the model to condense its learning and internalize environmental dynamics rather than over-relying on environmental cues. Our experiments on Sokoban, Maze, and Taxi show that WMAct yields effective world model reasoning capable of resolving tasks in a single turn that previously required multiple interactions and fosters strong transferability to complex environments, improving performance on a suite of reasoning benchmarks.&lt;/p&gt;</content:encoded></item><item><title>A duet of perception and reasoning: CLIP and LLM brainstorming for scene text recognition</title><link>https://doi.org/10.1016/j.neucom.2025.132236</link><guid>10.1016/j.neucom.2025.132236</guid><pubDate>Sat, 29 Nov 2025 16:00:38 +0000</pubDate><dc:creator>Zeguang Jia</dc:creator><dc:creator>Jianming Wang</dc:creator><dc:creator>Kehui Song</dc:creator><dc:creator>Zhilan Wang</dc:creator><dc:creator>Xiaohan Ma</dc:creator><dc:creator>Rize Jin</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132236</prism:doi><description>Deciphering ambiguous or contextually complex text remains a major challenge in the field of Scene Text Recognition (STR). Most existing STR recognizers rely on specialized, small-scale decoders that lack access to higher-level world knowledge and are prone to propagating local prediction errors, making it difficult to perform the higher-order reasoning required in complex contexts. These limitations are especially pronounced when using unimodal visual backbones that are incapable of capturing semantic information. In this study, we propose a novel STR paradigm called the Visual-Linguistic Enhancement Network (VLENet), which aims to jointly enhance visual perception and linguistic reasoning. Specifically, VLENet employs a cross-modal pre-trained model (CLIP) to extract visual representations that are semantically aligned with textual content. Based on the recognizer’s initial visual and linguistic predictions, a large language model (LLM) is prompted to “brainstorm” a diverse set of plausible text candidates. Finally, a carefully designed visual-linguistic matching module computes similarity scores between the original image and each candidate to select the most accurate transcription.We demonstrate the effectiveness of VLENet across a wide range of Chinese and English benchmarks, achieving new state-of-the-art (SOTA) results. Furthermore, our analysis shows that VLENet performs particularly well on challenging datasets such as COCO and Uber, highlighting its strong ability to reason about and correct text in complex real-world scenarios.
Published: 2025-11-29T16:00:38+00:00
Venue: Neurocomputing
Score: 0.743 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zeguang Jia; Jianming Wang; Kehui Song; Zhilan Wang; Xiaohan Ma; Rize Jin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132236"&gt;10.1016/j.neucom.2025.132236&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.743 (must_read)&lt;/p&gt;
&lt;p&gt;Deciphering ambiguous or contextually complex text remains a major challenge in the field of Scene Text Recognition (STR). Most existing STR recognizers rely on specialized, small-scale decoders that lack access to higher-level world knowledge and are prone to propagating local prediction errors, making it difficult to perform the higher-order reasoning required in complex contexts. These limitations are especially pronounced when using unimodal visual backbones that are incapable of capturing semantic information. In this study, we propose a novel STR paradigm called the Visual-Linguistic Enhancement Network (VLENet), which aims to jointly enhance visual perception and linguistic reasoning. Specifically, VLENet employs a cross-modal pre-trained model (CLIP) to extract visual representations that are semantically aligned with textual content. Based on the recognizer’s initial visual and linguistic predictions, a large language model (LLM) is prompted to “brainstorm” a diverse set of plausible text candidates. Finally, a carefully designed visual-linguistic matching module computes similarity scores between the original image and each candidate to select the most accurate transcription.We demonstrate the effectiveness of VLENet across a wide range of Chinese and English benchmarks, achieving new state-of-the-art (SOTA) results. Furthermore, our analysis shows that VLENet performs particularly well on challenging datasets such as COCO and Uber, highlighting its strong ability to reason about and correct text in complex real-world scenarios.&lt;/p&gt;</content:encoded></item><item><title>V2X-VLM: End-to-End V2X cooperative autonomous driving through large vision-Language models</title><link>https://doi.org/10.1016/j.trc.2025.105457</link><guid>10.1016/j.trc.2025.105457</guid><pubDate>Sun, 30 Nov 2025 16:12:40 +0000</pubDate><dc:creator>Junwei You</dc:creator><dc:creator>Zhuoyu Jiang</dc:creator><dc:creator>Zilin Huang</dc:creator><dc:creator>Haotian Shi</dc:creator><dc:creator>Rui Gan</dc:creator><dc:creator>Keshu Wu</dc:creator><dc:creator>Xi Cheng</dc:creator><dc:creator>Xiaopeng Li</dc:creator><dc:creator>Bin Ran</dc:creator><prism:publicationName>Transportation Research Part C: Emerging Technologies</prism:publicationName><prism:doi>10.1016/j.trc.2025.105457</prism:doi><description>Vehicle-to-everything (V2X) cooperation has emerged as a promising paradigm to overcome the perception limitations of classical autonomous driving by leveraging information from both ego-vehicle and infrastructure sensors. However, effectively fusing heterogeneous visual and semantic information while ensuring robust trajectory planning remains a significant challenge. This paper introduces V2X-VLM, a novel end-to-end (E2E) cooperative autonomous driving framework based on vision-language models (VLMs). V2X-VLM integrates multiperspective camera views from vehicles and infrastructure with text-based scene descriptions to enable a more comprehensive understanding of driving environments. Specifically, we propose a contrastive learning-based mechanism to reinforce the alignment of heterogeneous visual and textual characteristics, which enhances the semantic understanding of complex driving scenarios, and employ a knowledge distillation strategy to stabilize training. Experiments on a large real-world dataset demonstrate that V2X-VLM achieves state-of-the-art trajectory planning accuracy, significantly reducing L2 error and collision rate compared to existing cooperative autonomous driving baselines. Ablation studies validate the contributions of each component. Moreover, the evaluation of robustness and efficiency highlights the practicality of V2X-VLM for real-world deployment to enhance overall autonomous driving safety and decision-making.
Published: 2025-11-30T16:12:40+00:00
Venue: Transportation Research Part C: Emerging Technologies
Score: 0.742 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junwei You; Zhuoyu Jiang; Zilin Huang; Haotian Shi; Rui Gan; Keshu Wu; Xi Cheng; Xiaopeng Li; Bin Ran&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Transportation Research Part C: Emerging Technologies&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.trc.2025.105457"&gt;10.1016/j.trc.2025.105457&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.742 (must_read)&lt;/p&gt;
&lt;p&gt;Vehicle-to-everything (V2X) cooperation has emerged as a promising paradigm to overcome the perception limitations of classical autonomous driving by leveraging information from both ego-vehicle and infrastructure sensors. However, effectively fusing heterogeneous visual and semantic information while ensuring robust trajectory planning remains a significant challenge. This paper introduces V2X-VLM, a novel end-to-end (E2E) cooperative autonomous driving framework based on vision-language models (VLMs). V2X-VLM integrates multiperspective camera views from vehicles and infrastructure with text-based scene descriptions to enable a more comprehensive understanding of driving environments. Specifically, we propose a contrastive learning-based mechanism to reinforce the alignment of heterogeneous visual and textual characteristics, which enhances the semantic understanding of complex driving scenarios, and employ a knowledge distillation strategy to stabilize training. Experiments on a large real-world dataset demonstrate that V2X-VLM achieves state-of-the-art trajectory planning accuracy, significantly reducing L2 error and collision rate compared to existing cooperative autonomous driving baselines. Ablation studies validate the contributions of each component. Moreover, the evaluation of robustness and efficiency highlights the practicality of V2X-VLM for real-world deployment to enhance overall autonomous driving safety and decision-making.&lt;/p&gt;</content:encoded></item><item><title>RoadSceneBench: A Lightweight Benchmark for Mid-Level Road Scene Understanding</title><link>https://arxiv.org/abs/2511.22466v1</link><guid>http://arxiv.org/abs/2511.22466v1</guid><pubDate>Thu, 27 Nov 2025 13:57:31 +0000</pubDate><dc:creator>Xiyan Liu</dc:creator><dc:creator>Han Wang</dc:creator><dc:creator>Yuhu Wang</dc:creator><dc:creator>Junjie Cai</dc:creator><dc:creator>Zhe Cao</dc:creator><dc:creator>Jianzhong Yang</dc:creator><dc:creator>Zhen Lu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Understanding mid-level road semantics, which capture the structural and contextual cues that link low-level perception to high-level planning, is essential for reliable autonomous driving and digital map construction. However, existing benchmarks primarily target perception tasks such as detection or segmentation, overlooking the reasoning capabilities required to infer road topology and dynamic scene structure. To address this gap, we present RoadSceneBench, a lightweight yet information-rich benchmark designed to evaluate and advance visual reasoning in complex road environments. Unlike large-scale perception datasets, RoadSceneBench emphasizes relational understanding and structural consistency, encouraging models to capture the underlying logic of real-world road scenes. Furthermore, to enhance reasoning reliability, we propose Hierarchical Relational Reward Propagation with Temporal Consistency (HRRP-T), a training framework for Vision-Language Models (VLMs) in which reward signals adaptively promote spatial coherence and semantic alignment throughout the reasoning process. This paradigm enables models to move beyond static recognition toward geometry-aware and temporally consistent reasoning. Extensive experiments demonstrate that our method achieves state-of-the-art performance across diverse road configurations. RoadSceneBench thus provides a compact yet powerful foundation for studying mid-level road semantics and fostering structure-aware autonomous perception. Our dataset is available at https://github.com/XiyanLiu/RoadSceneBench.
Published: 2025-11-27T13:57:31+00:00
Venue: arXiv
Score: 0.740 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiyan Liu; Han Wang; Yuhu Wang; Junjie Cai; Zhe Cao; Jianzhong Yang; Zhen Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.740 (must_read)&lt;/p&gt;
&lt;p&gt;Understanding mid-level road semantics, which capture the structural and contextual cues that link low-level perception to high-level planning, is essential for reliable autonomous driving and digital map construction. However, existing benchmarks primarily target perception tasks such as detection or segmentation, overlooking the reasoning capabilities required to infer road topology and dynamic scene structure. To address this gap, we present RoadSceneBench, a lightweight yet information-rich benchmark designed to evaluate and advance visual reasoning in complex road environments. Unlike large-scale perception datasets, RoadSceneBench emphasizes relational understanding and structural consistency, encouraging models to capture the underlying logic of real-world road scenes. Furthermore, to enhance reasoning reliability, we propose Hierarchical Relational Reward Propagation with Temporal Consistency (HRRP-T), a training framework for Vision-Language Models (VLMs) in which reward signals adaptively promote spatial coherence and semantic alignment throughout the reasoning process. This paradigm enables models to move beyond static recognition toward geometry-aware and temporally consistent reasoning. Extensive experiments demonstrate that our method achieves state-of-the-art performance across diverse road configurations. RoadSceneBench thus provides a compact yet powerful foundation for studying mid-level road semantics and fostering structure-aware autonomous perception. Our dataset is available at https://github.com/XiyanLiu/RoadSceneBench.&lt;/p&gt;</content:encoded></item><item><title>Unlocking Multilingual Reasoning Capability of LLMs and LVLMs through Representation Engineering</title><link>https://arxiv.org/abs/2511.23231v1</link><guid>http://arxiv.org/abs/2511.23231v1</guid><pubDate>Fri, 28 Nov 2025 14:40:27 +0000</pubDate><dc:creator>Qiming Li</dc:creator><dc:creator>Xiaocheng Feng</dc:creator><dc:creator>Yixuan Ma</dc:creator><dc:creator>Zekai Ye</dc:creator><dc:creator>Ruihan Chen</dc:creator><dc:creator>Xiachong Feng</dc:creator><dc:creator>Bing Qin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) demonstrate strong reasoning capabilities, yet their performance in English significantly outperforms that in low-resource languages, raising fairness concerns in multilingual applications. Existing approaches either rely on costly multilingual training or employ prompting with external translation tools, both of which are resource-intensive and sensitive to translation quality. To address these limitations, we propose a training-free inference-time method to enhance Multilingual Reasoning capabilities via Representation Engineering (MRRE) without using any additional training data or tools. MRRE sequentially injects two precomputed vectors at specific layers during inference processing: cross-lingual reasoning enhancement vectors, which steer non-English reasoning representations toward English space to unlock multilingual reasoning, and target-language output anchoring vectors, which restore the distribution of the target language to preserve input-output language consistency. Comprehensive experiments across six advanced LLMs and LVLMs on four reasoning benchmarks demonstrate that MRRE consistently enhances non-English reasoning by an average gain of 5.48% and up to 7.54% in low-resource languages (Thai and Swahili), while improving input-output language consistency by 3.78%.
Published: 2025-11-28T14:40:27+00:00
Venue: arXiv
Score: 0.739 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qiming Li; Xiaocheng Feng; Yixuan Ma; Zekai Ye; Ruihan Chen; Xiachong Feng; Bing Qin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.739 (must_read)&lt;/p&gt;
&lt;p&gt;Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) demonstrate strong reasoning capabilities, yet their performance in English significantly outperforms that in low-resource languages, raising fairness concerns in multilingual applications. Existing approaches either rely on costly multilingual training or employ prompting with external translation tools, both of which are resource-intensive and sensitive to translation quality. To address these limitations, we propose a training-free inference-time method to enhance Multilingual Reasoning capabilities via Representation Engineering (MRRE) without using any additional training data or tools. MRRE sequentially injects two precomputed vectors at specific layers during inference processing: cross-lingual reasoning enhancement vectors, which steer non-English reasoning representations toward English space to unlock multilingual reasoning, and target-language output anchoring vectors, which restore the distribution of the target language to preserve input-output language consistency. Comprehensive experiments across six advanced LLMs and LVLMs on four reasoning benchmarks demonstrate that MRRE consistently enhances non-English reasoning by an average gain of 5.48% and up to 7.54% in low-resource languages (Thai and Swahili), while improving input-output language consistency by 3.78%.&lt;/p&gt;</content:encoded></item><item><title>DocVAL: Validated Chain-of-Thought Distillation for Grounded Document VQA</title><link>https://arxiv.org/abs/2511.22521v1</link><guid>http://arxiv.org/abs/2511.22521v1</guid><pubDate>Thu, 27 Nov 2025 15:00:58 +0000</pubDate><dc:creator>Ahmad Mohammadshirazi</dc:creator><dc:creator>Pinaki Prasad Guha Neogi</dc:creator><dc:creator>Dheeraj Kulshrestha</dc:creator><dc:creator>Rajiv Ramnath</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Document visual question answering (DocVQA) requires models to jointly reason over textual content and spatial layout, yet current systems exhibit a sharp accuracy--efficiency trade-off: large teacher models achieve strong grounding but are too expensive for deployment, while compact students suffer substantial drops in localization performance. We propose DocVAL, a validated chain-of-thought distillation framework that transfers the spatial reasoning ability of a large teacher into a deployable student VLM through three key components: (1) teacher supervision with validation-time text detection to filter and denoise training signals, (2) a multi-module validator (VAL) that enforces answer correctness and geometric consistency while producing fine-grained, pixel-level error feedback, and (3) a two-stage student training scheme that first learns from validated CoT traces and then undergoes iterative refinement driven by VAL feedback. Our student (Gemma-3 12B) achieves 91.4\% ANLS and 82.4\% mAP on DocVQA as a pure VLM requiring no text detection or OCR at inference. Extensive ablations demonstrate that validated feedback contributes 6.3 mAP gain and iterative refinement accounts for 9.7 mAP improvement. We release 95k high-quality, validator-verified CoT traces to advance spatial reasoning research in document understanding.
Published: 2025-11-27T15:00:58+00:00
Venue: arXiv
Score: 0.739 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ahmad Mohammadshirazi; Pinaki Prasad Guha Neogi; Dheeraj Kulshrestha; Rajiv Ramnath&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.739 (must_read)&lt;/p&gt;
&lt;p&gt;Document visual question answering (DocVQA) requires models to jointly reason over textual content and spatial layout, yet current systems exhibit a sharp accuracy--efficiency trade-off: large teacher models achieve strong grounding but are too expensive for deployment, while compact students suffer substantial drops in localization performance. We propose DocVAL, a validated chain-of-thought distillation framework that transfers the spatial reasoning ability of a large teacher into a deployable student VLM through three key components: (1) teacher supervision with validation-time text detection to filter and denoise training signals, (2) a multi-module validator (VAL) that enforces answer correctness and geometric consistency while producing fine-grained, pixel-level error feedback, and (3) a two-stage student training scheme that first learns from validated CoT traces and then undergoes iterative refinement driven by VAL feedback. Our student (Gemma-3 12B) achieves 91.4\% ANLS and 82.4\% mAP on DocVQA as a pure VLM requiring no text detection or OCR at inference. Extensive ablations demonstrate that validated feedback contributes 6.3 mAP gain and iterative refinement accounts for 9.7 mAP improvement. We release 95k high-quality, validator-verified CoT traces to advance spatial reasoning research in document understanding.&lt;/p&gt;</content:encoded></item><item><title>Diversity Covariance-Aware Prompt Learning for Vision-Language Models</title><link>https://doi.org/10.1016/j.patcog.2025.112806</link><guid>10.1016/j.patcog.2025.112806</guid><pubDate>Sun, 30 Nov 2025 15:08:29 +0000</pubDate><dc:creator>Zhengdong Zhou</dc:creator><dc:creator>Songlin Dong</dc:creator><dc:creator>Chenhao Ding</dc:creator><dc:creator>Xinyuan Gao</dc:creator><dc:creator>Yuhang He</dc:creator><dc:creator>Yihong Gong</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112806</prism:doi><description>Prompt tuning can further enhance the performance of visual-language models across various downstream tasks ( e.g. , few-shot learning), enabling them to better adapt to specific applications and needs. In this paper, we present a D iversity C ovariance- A ware framework that learns distributional information from the data to enhance the few-shot ability of the prompt model. First, we propose a covariance-aware method that models the covariance relationships between visual features and uses anisotropic Mahalanobis distance, instead of the suboptimal cosine distance, to measure the similarity between two modalities. We rigorously derive and prove the validity of this modeling process. Then, we propose the diversity-aware method, which learns multiple diverse soft prompts to capture different attributes of categories and aligns them independently with visual modalities. This method achieves multi-centered covariance modeling, leading to more diverse decision boundaries. Extensive experiments on 11 datasets in various tasks demonstrate the effectiveness of our method.
Published: 2025-11-30T15:08:29+00:00
Venue: Pattern Recognition
Score: 0.739 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhengdong Zhou; Songlin Dong; Chenhao Ding; Xinyuan Gao; Yuhang He; Yihong Gong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112806"&gt;10.1016/j.patcog.2025.112806&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.739 (must_read)&lt;/p&gt;
&lt;p&gt;Prompt tuning can further enhance the performance of visual-language models across various downstream tasks ( e.g. , few-shot learning), enabling them to better adapt to specific applications and needs. In this paper, we present a D iversity C ovariance- A ware framework that learns distributional information from the data to enhance the few-shot ability of the prompt model. First, we propose a covariance-aware method that models the covariance relationships between visual features and uses anisotropic Mahalanobis distance, instead of the suboptimal cosine distance, to measure the similarity between two modalities. We rigorously derive and prove the validity of this modeling process. Then, we propose the diversity-aware method, which learns multiple diverse soft prompts to capture different attributes of categories and aligns them independently with visual modalities. This method achieves multi-centered covariance modeling, leading to more diverse decision boundaries. Extensive experiments on 11 datasets in various tasks demonstrate the effectiveness of our method.&lt;/p&gt;</content:encoded></item><item><title>Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization</title><link>https://arxiv.org/abs/2511.22586v1</link><guid>http://arxiv.org/abs/2511.22586v1</guid><pubDate>Thu, 27 Nov 2025 16:19:34 +0000</pubDate><dc:creator>Yifan Du</dc:creator><dc:creator>Kun Zhou</dc:creator><dc:creator>Yingqian Min</dc:creator><dc:creator>Yue Ling</dc:creator><dc:creator>Wayne Xin Zhao</dc:creator><dc:creator>Youbin Wu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We study how different Chain-of-Thought (CoT) designs affect the acquisition of the generalizable visual reasoning ability in vision-language models (VLMs). While CoT data, especially long or visual CoT such as "think with image", has been widely used to supervise intermediate reasoning, it remains unclear why specific CoT designs help and which ones truly support generalizable reasoning. To systematically evaluate this, we focus on a controlled maze-solving benchmark where reasoning rules are fully visual, difficulty can be tuned by grid size, and all the intermediate steps can be automatically generated. Using Qwen2.5-VL-7B under a standard SFT-then-RL pipeline, we compare three representative CoT formats: Language CoT, Grounding CoT (with spatial coordinate trajectories), and Visual CoT (with image manipulations). Our experiments reveal that visual and longer CoT mainly accelerate convergence but do not lift the final performance ceiling; concise CoT containing only essential grounding steps outperforms longer traces; and, strikingly, CoT retaining only the minimal grounding results generalizes best across different maze sizes. We further validate these insights on other vision-centric tasks. These findings highlight a "short is long" effect and provide practical guidance for constructing more generalizable SFT datasets for visual reasoning.
Published: 2025-11-27T16:19:34+00:00
Venue: arXiv
Score: 0.737 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yifan Du; Kun Zhou; Yingqian Min; Yue Ling; Wayne Xin Zhao; Youbin Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.737 (must_read)&lt;/p&gt;
&lt;p&gt;We study how different Chain-of-Thought (CoT) designs affect the acquisition of the generalizable visual reasoning ability in vision-language models (VLMs). While CoT data, especially long or visual CoT such as &amp;quot;think with image&amp;quot;, has been widely used to supervise intermediate reasoning, it remains unclear why specific CoT designs help and which ones truly support generalizable reasoning. To systematically evaluate this, we focus on a controlled maze-solving benchmark where reasoning rules are fully visual, difficulty can be tuned by grid size, and all the intermediate steps can be automatically generated. Using Qwen2.5-VL-7B under a standard SFT-then-RL pipeline, we compare three representative CoT formats: Language CoT, Grounding CoT (with spatial coordinate trajectories), and Visual CoT (with image manipulations). Our experiments reveal that visual and longer CoT mainly accelerate convergence but do not lift the final performance ceiling; concise CoT containing only essential grounding steps outperforms longer traces; and, strikingly, CoT retaining only the minimal grounding results generalizes best across different maze sizes. We further validate these insights on other vision-centric tasks. These findings highlight a &amp;quot;short is long&amp;quot; effect and provide practical guidance for constructing more generalizable SFT datasets for visual reasoning.&lt;/p&gt;</content:encoded></item><item><title>From Points to Clouds: Learning Robust Semantic Distributions for Multi-modal Prompts</title><link>https://arxiv.org/abs/2511.22897v1</link><guid>http://arxiv.org/abs/2511.22897v1</guid><pubDate>Fri, 28 Nov 2025 06:03:35 +0000</pubDate><dc:creator>Weiran Li</dc:creator><dc:creator>Yeqiang Liu</dc:creator><dc:creator>Yijie Wei</dc:creator><dc:creator>Mina Han</dc:creator><dc:creator>Xin Liu</dc:creator><dc:creator>Zhenbo Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal Prompt Learning (MPL) has emerged as a pivotal technique for adapting large-scale Visual Language Models (VLMs). However, current MPL methods are fundamentally limited by their optimization of a single, static point representation. This paradigm is inherently brittle, leads to overfitting on base classes, and generalizes poorly to novel or ambiguous categories. We challenge this point paradigm, proposing that robust generalization requires learning a semantic cloud (i.e., a distribution over the embedding space). To achieve this, we introduce Points-to-Clouds (P2C), a novel framework inspired by diffusion models that reframes prompt learning as a dynamic denoising task. At the core of P2C is a dual denoising mechanism: a Dynamic Prompt Denoising (DPD) mechanism perturbs text prompts with sophisticated, annealed noise to learn a smoother semantic landscape, while an auxiliary V-L Mapper denoising loss re-tasks the mapper as a denoising autoencoder. This forces the mapper to reconstruct clean visual prompts from noisy text inputs, ensuring robust cross-modal alignment. Extensive experiments across 11 datasets demonstrate that P2C consistently outperforms strong baselines. On the base-to-novel generalization benchmark, our method achieves a Harmonic Mean of 79.7%, representing a relative improvement of 1.4% over the baseline. The code and models are available at https://vranlee.github.io/P2C/.
Published: 2025-11-28T06:03:35+00:00
Venue: arXiv
Score: 0.736 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weiran Li; Yeqiang Liu; Yijie Wei; Mina Han; Xin Liu; Zhenbo Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.736 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal Prompt Learning (MPL) has emerged as a pivotal technique for adapting large-scale Visual Language Models (VLMs). However, current MPL methods are fundamentally limited by their optimization of a single, static point representation. This paradigm is inherently brittle, leads to overfitting on base classes, and generalizes poorly to novel or ambiguous categories. We challenge this point paradigm, proposing that robust generalization requires learning a semantic cloud (i.e., a distribution over the embedding space). To achieve this, we introduce Points-to-Clouds (P2C), a novel framework inspired by diffusion models that reframes prompt learning as a dynamic denoising task. At the core of P2C is a dual denoising mechanism: a Dynamic Prompt Denoising (DPD) mechanism perturbs text prompts with sophisticated, annealed noise to learn a smoother semantic landscape, while an auxiliary V-L Mapper denoising loss re-tasks the mapper as a denoising autoencoder. This forces the mapper to reconstruct clean visual prompts from noisy text inputs, ensuring robust cross-modal alignment. Extensive experiments across 11 datasets demonstrate that P2C consistently outperforms strong baselines. On the base-to-novel generalization benchmark, our method achieves a Harmonic Mean of 79.7%, representing a relative improvement of 1.4% over the baseline. The code and models are available at https://vranlee.github.io/P2C/.&lt;/p&gt;</content:encoded></item><item><title>TempQA: An LLM-based Framework for Temporal Knowledge Graph Question Answering</title><link>https://doi.org/10.1016/j.knosys.2025.114988</link><guid>10.1016/j.knosys.2025.114988</guid><pubDate>Sat, 29 Nov 2025 04:39:32 +0000</pubDate><dc:creator>Qianyi Hu</dc:creator><dc:creator>Xinhui Tu</dc:creator><dc:creator>Ao Li</dc:creator><dc:creator>Biao Yao</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.114988</prism:doi><description>Temporal Knowledge Graph Question Answering (TKGQA) is a challenging task requiring models to reason over time-sensitive facts stored in temporal knowledge graphs (TKGs) as real-world knowledge evolves. Existing TKGQA approaches often fine-tune pretrained language models on labeled data, but these methods require large training sets and struggle to generalize, especially on queries with complex temporal constraints. To address this, we introduce TempQA, an innovative zero-shot TKGQA framework leveraging large language models (LLMs) without the need for task-specific training. TempQA employs a retrieval-augmented approach by first translating TKG facts into natural-language sentences and creating corresponding embeddings. Given a query, it retrieves relevant context by computing semantic similarity between the query and embedded sentences, subsequently providing the query alongside the top-k retrieved facts as context to an LLM (GPT-3.5 or GPT-4) to generate temporally-informed answers. Experimental evaluations on MultiTQ and CronQuestions datasets demonstrate that TempQA significantly outperforms existing baselines, underscoring the capability of LLMs–when combined with effective retrieval strategies–to serve as robust zero-shot temporal reasoners. This work highlights a promising direction toward effective TKGQA solutions without dependency on extensive training data.
Published: 2025-11-29T04:39:32+00:00
Venue: Knowledge-Based Systems
Score: 0.736 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qianyi Hu; Xinhui Tu; Ao Li; Biao Yao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.114988"&gt;10.1016/j.knosys.2025.114988&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.736 (must_read)&lt;/p&gt;
&lt;p&gt;Temporal Knowledge Graph Question Answering (TKGQA) is a challenging task requiring models to reason over time-sensitive facts stored in temporal knowledge graphs (TKGs) as real-world knowledge evolves. Existing TKGQA approaches often fine-tune pretrained language models on labeled data, but these methods require large training sets and struggle to generalize, especially on queries with complex temporal constraints. To address this, we introduce TempQA, an innovative zero-shot TKGQA framework leveraging large language models (LLMs) without the need for task-specific training. TempQA employs a retrieval-augmented approach by first translating TKG facts into natural-language sentences and creating corresponding embeddings. Given a query, it retrieves relevant context by computing semantic similarity between the query and embedded sentences, subsequently providing the query alongside the top-k retrieved facts as context to an LLM (GPT-3.5 or GPT-4) to generate temporally-informed answers. Experimental evaluations on MultiTQ and CronQuestions datasets demonstrate that TempQA significantly outperforms existing baselines, underscoring the capability of LLMs–when combined with effective retrieval strategies–to serve as robust zero-shot temporal reasoners. This work highlights a promising direction toward effective TKGQA solutions without dependency on extensive training data.&lt;/p&gt;</content:encoded></item><item><title>HMR3D: Hierarchical Multimodal Representation for 3D Scene Understanding with Large Vision-Language Model</title><link>https://arxiv.org/abs/2511.22961v1</link><guid>http://arxiv.org/abs/2511.22961v1</guid><pubDate>Fri, 28 Nov 2025 08:06:20 +0000</pubDate><dc:creator>Chen Li</dc:creator><dc:creator>Eric Peh</dc:creator><dc:creator>Basura Fernando</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advances in large vision-language models (VLMs) have shown significant promise for 3D scene understanding. Existing VLM-based approaches typically align 3D scene features with the VLM's embedding space. However, this implicit alignment often yields suboptimal performance due to the scarcity of 3D data and the inherent complexity of spatial relationships in 3D environments. To address these limitations, we propose a novel hierarchical multimodal representation for 3D scene reasoning that explicitly aligns with VLMs at the input space by leveraging both multi-view images and text descriptions. The text descriptions capture spatial relationships by referencing the 3D coordinates of detected objects, while the multi-view images include a top-down perspective and four directional views (forward, left, right, and backward), ensuring comprehensive scene coverage. Additionally, we introduce a hierarchical feature representation that aggregates patch-level image features into view-level and scene-level representations, enabling the model to reason over both local and global scene context. Experimental results on both situated 3D Q&amp;A and general 3D Q&amp;A benchmarks demonstrate the effectiveness of our approach.
Published: 2025-11-28T08:06:20+00:00
Venue: arXiv
Score: 0.734 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chen Li; Eric Peh; Basura Fernando&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.734 (must_read)&lt;/p&gt;
&lt;p&gt;Recent advances in large vision-language models (VLMs) have shown significant promise for 3D scene understanding. Existing VLM-based approaches typically align 3D scene features with the VLM&amp;#x27;s embedding space. However, this implicit alignment often yields suboptimal performance due to the scarcity of 3D data and the inherent complexity of spatial relationships in 3D environments. To address these limitations, we propose a novel hierarchical multimodal representation for 3D scene reasoning that explicitly aligns with VLMs at the input space by leveraging both multi-view images and text descriptions. The text descriptions capture spatial relationships by referencing the 3D coordinates of detected objects, while the multi-view images include a top-down perspective and four directional views (forward, left, right, and backward), ensuring comprehensive scene coverage. Additionally, we introduce a hierarchical feature representation that aggregates patch-level image features into view-level and scene-level representations, enabling the model to reason over both local and global scene context. Experimental results on both situated 3D Q&amp;amp;A and general 3D Q&amp;amp;A benchmarks demonstrate the effectiveness of our approach.&lt;/p&gt;</content:encoded></item><item><title>The Duality of Generative AI and Reinforcement Learning in Robotics: A Review</title><link>https://doi.org/10.1016/j.inffus.2025.104003</link><guid>10.1016/j.inffus.2025.104003</guid><pubDate>Sat, 29 Nov 2025 16:05:06 +0000</pubDate><dc:creator>Angelo Moroncelli</dc:creator><dc:creator>Vishal Soni</dc:creator><dc:creator>Marco Forgione</dc:creator><dc:creator>Dario Piga</dc:creator><dc:creator>Blerina Spahiu</dc:creator><dc:creator>Loris Roveda</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104003</prism:doi><description>Recently, generative AI and reinforcement learning (RL) have been redefining what is possible for AI agents that take information flows as input and produce intelligent behavior. As a result, we are seeing similar advancements in embodied AI and robotics for control policy generation. Our review paper examines the integration of generative AI models with RL to advance robotics. Our primary focus is on the duality between generative AI and RL for robotics downstream tasks. Specifically, we investigate: (1) The role of prominent generative AI tools as modular priors for multi-modal input fusion in RL tasks. (2) How RL can train, fine-tune and distill generative models for policy generation, such as VLA models, similarly to RL applications in large language models. We then propose a new taxonomy based on a considerable amount of selected papers.
Published: 2025-11-29T16:05:06+00:00
Venue: Information Fusion
Score: 0.733 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Angelo Moroncelli; Vishal Soni; Marco Forgione; Dario Piga; Blerina Spahiu; Loris Roveda&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104003"&gt;10.1016/j.inffus.2025.104003&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.733 (consider)&lt;/p&gt;
&lt;p&gt;Recently, generative AI and reinforcement learning (RL) have been redefining what is possible for AI agents that take information flows as input and produce intelligent behavior. As a result, we are seeing similar advancements in embodied AI and robotics for control policy generation. Our review paper examines the integration of generative AI models with RL to advance robotics. Our primary focus is on the duality between generative AI and RL for robotics downstream tasks. Specifically, we investigate: (1) The role of prominent generative AI tools as modular priors for multi-modal input fusion in RL tasks. (2) How RL can train, fine-tune and distill generative models for policy generation, such as VLA models, similarly to RL applications in large language models. We then propose a new taxonomy based on a considerable amount of selected papers.&lt;/p&gt;</content:encoded></item><item><title>MG-Nav: Dual-Scale Visual Navigation via Sparse Spatial Memory</title><link>https://arxiv.org/abs/2511.22609v1</link><guid>http://arxiv.org/abs/2511.22609v1</guid><pubDate>Thu, 27 Nov 2025 16:43:21 +0000</pubDate><dc:creator>Bo Wang</dc:creator><dc:creator>Jiehong Lin</dc:creator><dc:creator>Chenzhi Liu</dc:creator><dc:creator>Xinting Hu</dc:creator><dc:creator>Yifei Yu</dc:creator><dc:creator>Tianjia Liu</dc:creator><dc:creator>Zhongrui Wang</dc:creator><dc:creator>Xiaojuan Qi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present MG-Nav (Memory-Guided Navigation), a dual-scale framework for zero-shot visual navigation that unifies global memory-guided planning with local geometry-enhanced control. At its core is the Sparse Spatial Memory Graph (SMG), a compact, region-centric memory where each node aggregates multi-view keyframe and object semantics, capturing both appearance and spatial structure while preserving viewpoint diversity. At the global level, the agent is localized on SMG and a goal-conditioned node path is planned via an image-to-instance hybrid retrieval, producing a sequence of reachable waypoints for long-horizon guidance. At the local level, a navigation foundation policy executes these waypoints in point-goal mode with obstacle-aware control, and switches to image-goal mode when navigating from the final node towards the visual target. To further enhance viewpoint alignment and goal recognition, we introduce VGGT-adapter, a lightweight geometric module built on the pre-trained VGGT model, which aligns observation and goal features in a shared 3D-aware space. MG-Nav operates global planning and local control at different frequencies, using periodic re-localization to correct errors. Experiments on HM3D Instance-Image-Goal and MP3D Image-Goal benchmarks demonstrate that MG-Nav achieves state-of-the-art zero-shot performance and remains robust under dynamic rearrangements and unseen scene conditions.
Published: 2025-11-27T16:43:21+00:00
Venue: arXiv
Score: 0.732 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bo Wang; Jiehong Lin; Chenzhi Liu; Xinting Hu; Yifei Yu; Tianjia Liu; Zhongrui Wang; Xiaojuan Qi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.732 (consider)&lt;/p&gt;
&lt;p&gt;We present MG-Nav (Memory-Guided Navigation), a dual-scale framework for zero-shot visual navigation that unifies global memory-guided planning with local geometry-enhanced control. At its core is the Sparse Spatial Memory Graph (SMG), a compact, region-centric memory where each node aggregates multi-view keyframe and object semantics, capturing both appearance and spatial structure while preserving viewpoint diversity. At the global level, the agent is localized on SMG and a goal-conditioned node path is planned via an image-to-instance hybrid retrieval, producing a sequence of reachable waypoints for long-horizon guidance. At the local level, a navigation foundation policy executes these waypoints in point-goal mode with obstacle-aware control, and switches to image-goal mode when navigating from the final node towards the visual target. To further enhance viewpoint alignment and goal recognition, we introduce VGGT-adapter, a lightweight geometric module built on the pre-trained VGGT model, which aligns observation and goal features in a shared 3D-aware space. MG-Nav operates global planning and local control at different frequencies, using periodic re-localization to correct errors. Experiments on HM3D Instance-Image-Goal and MP3D Image-Goal benchmarks demonstrate that MG-Nav achieves state-of-the-art zero-shot performance and remains robust under dynamic rearrangements and unseen scene conditions.&lt;/p&gt;</content:encoded></item><item><title>A Graph-Guided LLM Prompting for Supply-Demand Reasoning in Substation Flood Prevention</title><link>https://doi.org/10.1016/j.knosys.2025.115004</link><guid>10.1016/j.knosys.2025.115004</guid><pubDate>Sat, 29 Nov 2025 04:39:43 +0000</pubDate><dc:creator>Lan Lou</dc:creator><dc:creator>Xuanhua Ke</dc:creator><dc:creator>Shijian Liu</dc:creator><dc:creator>Cailong Zhao</dc:creator><dc:creator>Jia Peng</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115004</prism:doi><description>Frequent flood events necessitate supply-demand reasoning for the safe operation of substation. Recent advances demonstrate the potential capabilities of large language models (LLM) in reasoning tasks, yet domain alignment remains challenging in flood prevention scenarios. To address this, we propose a graph-guided LLM prompting framework for supply-demand reasoning tasks. Firstly, a domain-adaptive CasRel model is proposed to extract triples for building a pattern-aware flood Knowledge Graph (KG) with dynamic characteristic embeddings. The embedding is achieved by a well-defined meteorological pattern vector, effectively encoding the flood disaster pattern. A graph attention network is then employed via a two-stage strategy to retrieve the top-K relevant historical cases, which serve as both factual reference and formatted demonstration. A knowledge-guided prompting mechanism incorporates these cases using a chain-of-thought reasoning strategy, aligned with Transformer-predicted flood risk levels. This guides LLMs to generate context-aware, risk-sensitive supply recommendations. Extensive experiments on real-world flood prevention datasets demonstrate that all designed components and proposed improvements make unique contributions to generation quality. Our framework significantly mitigates LLM hallucination, offering better reasoning accuracy and stability across different LLMs. Moreover, the proposed framework that combines domain KGs with tailored prompt engineering is highly generalizable and can be extended for decision support in a wide range of high-stakes, climate-sensitive scenarios.
Published: 2025-11-29T04:39:43+00:00
Venue: Knowledge-Based Systems
Score: 0.731 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lan Lou; Xuanhua Ke; Shijian Liu; Cailong Zhao; Jia Peng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115004"&gt;10.1016/j.knosys.2025.115004&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.731 (consider)&lt;/p&gt;
&lt;p&gt;Frequent flood events necessitate supply-demand reasoning for the safe operation of substation. Recent advances demonstrate the potential capabilities of large language models (LLM) in reasoning tasks, yet domain alignment remains challenging in flood prevention scenarios. To address this, we propose a graph-guided LLM prompting framework for supply-demand reasoning tasks. Firstly, a domain-adaptive CasRel model is proposed to extract triples for building a pattern-aware flood Knowledge Graph (KG) with dynamic characteristic embeddings. The embedding is achieved by a well-defined meteorological pattern vector, effectively encoding the flood disaster pattern. A graph attention network is then employed via a two-stage strategy to retrieve the top-K relevant historical cases, which serve as both factual reference and formatted demonstration. A knowledge-guided prompting mechanism incorporates these cases using a chain-of-thought reasoning strategy, aligned with Transformer-predicted flood risk levels. This guides LLMs to generate context-aware, risk-sensitive supply recommendations. Extensive experiments on real-world flood prevention datasets demonstrate that all designed components and proposed improvements make unique contributions to generation quality. Our framework significantly mitigates LLM hallucination, offering better reasoning accuracy and stability across different LLMs. Moreover, the proposed framework that combines domain KGs with tailored prompt engineering is highly generalizable and can be extended for decision support in a wide range of high-stakes, climate-sensitive scenarios.&lt;/p&gt;</content:encoded></item><item><title>CoT4AD: A Vision-Language-Action Model with Explicit Chain-of-Thought Reasoning for Autonomous Driving</title><link>https://arxiv.org/abs/2511.22532v1</link><guid>http://arxiv.org/abs/2511.22532v1</guid><pubDate>Thu, 27 Nov 2025 15:13:13 +0000</pubDate><dc:creator>Zhaohui Wang</dc:creator><dc:creator>Tengbo Yu</dc:creator><dc:creator>Hao Tang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language-Action (VLA) models have recently attracted growing attention in end-to-end autonomous driving for their strong reasoning capabilities and rich world knowledge. However, existing VLAs often suffer from limited numerical reasoning ability and overly simplified input-output mappings, which hinder their performance in complex driving scenarios requiring step-by-step causal reasoning. To address these challenges, we propose CoT4AD, a novel VLA framework that introduces Chain-of-Thought (CoT) reasoning for autonomous driving to enhance both numerical and causal reasoning in Vision-Language Models (VLMs). CoT4AD integrates visual observations and language instructions to perform semantic reasoning, scene understanding, and trajectory planning. During training, it explicitly models a perception-question-prediction-action CoT to align the reasoning space with the action space across multiple driving tasks. During inference, it performs implicit CoT reasoning to enable consistent numerical reasoning and robust decision-making in dynamic environments. Extensive experiments on both real-world and simulated benchmarks, including nuScenes and Bench2Drive, demonstrate that CoT4AD achieves state-of-the-art performance in both open-loop and closed-loop evaluations. Code will be released upon paper acceptance.
Published: 2025-11-27T15:13:13+00:00
Venue: arXiv
Score: 0.731 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhaohui Wang; Tengbo Yu; Hao Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.731 (consider)&lt;/p&gt;
&lt;p&gt;Vision-Language-Action (VLA) models have recently attracted growing attention in end-to-end autonomous driving for their strong reasoning capabilities and rich world knowledge. However, existing VLAs often suffer from limited numerical reasoning ability and overly simplified input-output mappings, which hinder their performance in complex driving scenarios requiring step-by-step causal reasoning. To address these challenges, we propose CoT4AD, a novel VLA framework that introduces Chain-of-Thought (CoT) reasoning for autonomous driving to enhance both numerical and causal reasoning in Vision-Language Models (VLMs). CoT4AD integrates visual observations and language instructions to perform semantic reasoning, scene understanding, and trajectory planning. During training, it explicitly models a perception-question-prediction-action CoT to align the reasoning space with the action space across multiple driving tasks. During inference, it performs implicit CoT reasoning to enable consistent numerical reasoning and robust decision-making in dynamic environments. Extensive experiments on both real-world and simulated benchmarks, including nuScenes and Bench2Drive, demonstrate that CoT4AD achieves state-of-the-art performance in both open-loop and closed-loop evaluations. Code will be released upon paper acceptance.&lt;/p&gt;</content:encoded></item><item><title>Adversarial Training for Process Reward Models</title><link>https://arxiv.org/abs/2511.22888v1</link><guid>http://arxiv.org/abs/2511.22888v1</guid><pubDate>Fri, 28 Nov 2025 05:32:01 +0000</pubDate><dc:creator>Gurusha Juneja</dc:creator><dc:creator>Deepak Nathani</dc:creator><dc:creator>William Yang Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Process Reward Models (PRMs) enhance reasoning ability of LLMs by providing step-level supervision. However, their widespread adoption is limited due to expensive manual step-level annotation and poor generalization of static training data to novel errors. We introduce Adversarially Trained PRMs (\texttt{APRM}), where a Generator ($G$) learns to produce reasoning errors to deceive a PRM ($R$), while $R$ concurrently learns to detect them. This interaction yields progressively harder negatives for $R$, improving its robustness and generalization to novel errors without requiring manual step-level labels. Averaged across diverse mathematical reasoning benchmarks, \texttt{APRM} improves solver accuracy by $+3.4$ percentage points (pp) over the strongest PRM baseline. \texttt{APRM} achieves gains of $+5.3$ pp on out-of-distribution tasks.
Published: 2025-11-28T05:32:01+00:00
Venue: arXiv
Score: 0.729 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gurusha Juneja; Deepak Nathani; William Yang Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.729 (consider)&lt;/p&gt;
&lt;p&gt;Process Reward Models (PRMs) enhance reasoning ability of LLMs by providing step-level supervision. However, their widespread adoption is limited due to expensive manual step-level annotation and poor generalization of static training data to novel errors. We introduce Adversarially Trained PRMs (\texttt{APRM}), where a Generator ($G$) learns to produce reasoning errors to deceive a PRM ($R$), while $R$ concurrently learns to detect them. This interaction yields progressively harder negatives for $R$, improving its robustness and generalization to novel errors without requiring manual step-level labels. Averaged across diverse mathematical reasoning benchmarks, \texttt{APRM} improves solver accuracy by $+3.4$ percentage points (pp) over the strongest PRM baseline. \texttt{APRM} achieves gains of $+5.3$ pp on out-of-distribution tasks.&lt;/p&gt;</content:encoded></item><item><title>MathSight: A Benchmark Exploring Have Vision-Language Models Really Seen in University-Level Mathematical Reasoning?</title><link>https://arxiv.org/abs/2511.23112v1</link><guid>http://arxiv.org/abs/2511.23112v1</guid><pubDate>Fri, 28 Nov 2025 11:55:05 +0000</pubDate><dc:creator>Yuandong Wang</dc:creator><dc:creator>Yao Cui</dc:creator><dc:creator>Yuxin Zhao</dc:creator><dc:creator>Zhen Yang</dc:creator><dc:creator>Yangfu Zhu</dc:creator><dc:creator>Zhenzhou Shao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advances in Vision-Language Models (VLMs) have achieved impressive progress in multimodal mathematical reasoning. Yet, how much visual information truly contributes to reasoning remains unclear. Existing benchmarks report strong overall performance but seldom isolate the role of the image modality, leaving open whether VLMs genuinely leverage visual understanding or merely depend on linguistic priors. To address this, we present MathSight, a university-level multimodal mathematical reasoning benchmark designed to disentangle and quantify the effect of visual input. Each problem includes multiple visual variants -- original, hand-drawn, photo-captured -- and a text-only condition for controlled comparison. Experiments on state-of-the-art VLMs reveal a consistent trend: the contribution of visual information diminishes with increasing problem difficulty. Remarkably, Qwen3-VL without any image input surpasses both its multimodal variants and GPT-5, underscoring the need for benchmarks like MathSight to advance genuine vision-grounded reasoning in future models.
Published: 2025-11-28T11:55:05+00:00
Venue: arXiv
Score: 0.728 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuandong Wang; Yao Cui; Yuxin Zhao; Zhen Yang; Yangfu Zhu; Zhenzhou Shao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.728 (consider)&lt;/p&gt;
&lt;p&gt;Recent advances in Vision-Language Models (VLMs) have achieved impressive progress in multimodal mathematical reasoning. Yet, how much visual information truly contributes to reasoning remains unclear. Existing benchmarks report strong overall performance but seldom isolate the role of the image modality, leaving open whether VLMs genuinely leverage visual understanding or merely depend on linguistic priors. To address this, we present MathSight, a university-level multimodal mathematical reasoning benchmark designed to disentangle and quantify the effect of visual input. Each problem includes multiple visual variants -- original, hand-drawn, photo-captured -- and a text-only condition for controlled comparison. Experiments on state-of-the-art VLMs reveal a consistent trend: the contribution of visual information diminishes with increasing problem difficulty. Remarkably, Qwen3-VL without any image input surpasses both its multimodal variants and GPT-5, underscoring the need for benchmarks like MathSight to advance genuine vision-grounded reasoning in future models.&lt;/p&gt;</content:encoded></item><item><title>VaMP: Variational Multi-Modal Prompt Learning for Vision-Language Models</title><link>https://arxiv.org/abs/2511.22664v1</link><guid>http://arxiv.org/abs/2511.22664v1</guid><pubDate>Thu, 27 Nov 2025 17:57:39 +0000</pubDate><dc:creator>Silin Cheng</dc:creator><dc:creator>Kai Han</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-language models (VLMs), such as CLIP, have shown strong generalization under zero-shot settings, yet adapting them to downstream tasks with limited supervision remains a significant challenge. Existing multi-modal prompt learning methods typically rely on fixed, shared prompts and deterministic parameters, which limits their ability to capture instance-level variation or model uncertainty across diverse tasks and domains. To tackle this issue, we propose a novel Variational Multi-Modal Prompt Learning (VaMP) framework that enables sample-specific, uncertainty-aware prompt tuning in multi-modal representation learning. VaMP generates instance-conditioned prompts by sampling from a learned posterior distribution, allowing the model to personalize its behavior based on input content. To further enhance the integration of local and global semantics, we introduce a class-aware prior derived from the instance representation and class prototype. Building upon these, we formulate prompt tuning as variational inference over latent prompt representations and train the entire framework end-to-end through reparameterized sampling. Experiments on few-shot and domain generalization benchmarks show that VaMP achieves state-of-the-art performance, highlighting the benefits of modeling both uncertainty and task structure in our method. Project page: https://visual-ai.github.io/vamp
Published: 2025-11-27T17:57:39+00:00
Venue: arXiv
Score: 0.728 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Silin Cheng; Kai Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.728 (consider)&lt;/p&gt;
&lt;p&gt;Vision-language models (VLMs), such as CLIP, have shown strong generalization under zero-shot settings, yet adapting them to downstream tasks with limited supervision remains a significant challenge. Existing multi-modal prompt learning methods typically rely on fixed, shared prompts and deterministic parameters, which limits their ability to capture instance-level variation or model uncertainty across diverse tasks and domains. To tackle this issue, we propose a novel Variational Multi-Modal Prompt Learning (VaMP) framework that enables sample-specific, uncertainty-aware prompt tuning in multi-modal representation learning. VaMP generates instance-conditioned prompts by sampling from a learned posterior distribution, allowing the model to personalize its behavior based on input content. To further enhance the integration of local and global semantics, we introduce a class-aware prior derived from the instance representation and class prototype. Building upon these, we formulate prompt tuning as variational inference over latent prompt representations and train the entire framework end-to-end through reparameterized sampling. Experiments on few-shot and domain generalization benchmarks show that VaMP achieves state-of-the-art performance, highlighting the benefits of modeling both uncertainty and task structure in our method. Project page: https://visual-ai.github.io/vamp&lt;/p&gt;</content:encoded></item><item><title>TIM-PRM: Verifying multimodal reasoning with Tool-Integrated PRM</title><link>https://arxiv.org/abs/2511.22998v1</link><guid>http://arxiv.org/abs/2511.22998v1</guid><pubDate>Fri, 28 Nov 2025 09:01:38 +0000</pubDate><dc:creator>Peng Kuang</dc:creator><dc:creator>Xiangxiang Wang</dc:creator><dc:creator>Wentao Liu</dc:creator><dc:creator>Jian Dong</dc:creator><dc:creator>Kaidi Xu</dc:creator><dc:creator>Haohan Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal Large Language Models (MLLMs) have achieved impressive performances in mathematical reasoning, yet they remain vulnerable to visual hallucinations and logical inconsistencies that standard outcome-based supervision fails to mitigate. While Process Reward Models (PRMs) promise step-by-step verification, current approaches typically operate as scalar scorers or generative critics that suffer from sycophancy, blindly validating the flawed hypotheses rather than grounding them in visual reality. To bridge this gap, we introduce TIM-PRM (Tool-Integrated Multimodal PRM), a novel agentic framework that transforms verification from a passive classification task into an active, tool-augmented investigation. TIM-PRM is trained to explicitly plan verification strategies and utilizes a mechanism of Independent Question Asking to query evidence via external tools, effectively decoupling verification from the reasoning context to eliminate confirmation bias. We instantiate this method by curating a high-quality dataset of tool-integrated verification trajectories. Extensive experiments on VisualProcessBench demonstrate that our 8B parameter model surpasses existing open-source multimodal PRMs, significantly outperforming much larger models like Qwen2.5-72B and InternVL-78B, while offering interpretable insights into the verification process.
Published: 2025-11-28T09:01:38+00:00
Venue: arXiv
Score: 0.718 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Peng Kuang; Xiangxiang Wang; Wentao Liu; Jian Dong; Kaidi Xu; Haohan Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.718 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal Large Language Models (MLLMs) have achieved impressive performances in mathematical reasoning, yet they remain vulnerable to visual hallucinations and logical inconsistencies that standard outcome-based supervision fails to mitigate. While Process Reward Models (PRMs) promise step-by-step verification, current approaches typically operate as scalar scorers or generative critics that suffer from sycophancy, blindly validating the flawed hypotheses rather than grounding them in visual reality. To bridge this gap, we introduce TIM-PRM (Tool-Integrated Multimodal PRM), a novel agentic framework that transforms verification from a passive classification task into an active, tool-augmented investigation. TIM-PRM is trained to explicitly plan verification strategies and utilizes a mechanism of Independent Question Asking to query evidence via external tools, effectively decoupling verification from the reasoning context to eliminate confirmation bias. We instantiate this method by curating a high-quality dataset of tool-integrated verification trajectories. Extensive experiments on VisualProcessBench demonstrate that our 8B parameter model surpasses existing open-source multimodal PRMs, significantly outperforming much larger models like Qwen2.5-72B and InternVL-78B, while offering interpretable insights into the verification process.&lt;/p&gt;</content:encoded></item></channel></rss>