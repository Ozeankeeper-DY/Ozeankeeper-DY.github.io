<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sat, 06 Dec 2025 04:16:20 +0000</lastBuildDate><item><title>AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views</title><link>https://doi.org/10.1145/3763326</link><guid>10.1145/3763326</guid><pubDate>Thu, 04 Dec 2025 17:15:39 +0000</pubDate><dc:creator>Lihan Jiang</dc:creator><dc:creator>Yucheng Mao</dc:creator><dc:creator>Linning Xu</dc:creator><dc:creator>Tao Lu</dc:creator><dc:creator>Kerui Ren</dc:creator><dc:creator>Yichen Jin</dc:creator><dc:creator>Xudong Xu</dc:creator><dc:creator>Mulin Yu</dc:creator><dc:creator>Jiangmiao Pang</dc:creator><dc:creator>Feng Zhao</dc:creator><dc:creator>Dahua Lin</dc:creator><dc:creator>Bo Dai</dc:creator><prism:publicationName>ACM Transactions on Graphics</prism:publicationName><prism:doi>10.1145/3763326</prism:doi><description>We introduce AnySplat, a feed-forward network for novel-view synthesis from uncalibrated image collections. In contrast to traditional neural-rendering pipelines that demand known camera poses and per-scene optimization, or recent feed-forward methods that buckle under the computational weight of dense views—our model predicts everything in one shot. A single forward pass yields a set of 3D Gaussian primitives encoding both scene geometry and appearance, and the corresponding camera intrinsics and extrinsics for each input image. This unified design scales effortlessly to casually captured, multi-view datasets without any pose annotations. In extensive zero-shot evaluations, AnySplat matches the quality of pose-aware baselines in both sparse- and dense-view scenarios while surpassing existing pose-free approaches. Moreover, it greatly reduces rendering latency compared to optimization-based neural fields, bringing real-time novel-view synthesis within reach for unconstrained capture settings. Project page: https://city-super.github.io/anysplat/.
Published: 2025-12-04T17:15:39+00:00
Venue: ACM Transactions on Graphics
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lihan Jiang; Yucheng Mao; Linning Xu; Tao Lu; Kerui Ren; Yichen Jin; Xudong Xu; Mulin Yu; Jiangmiao Pang; Feng Zhao; Dahua Lin; Bo Dai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ACM Transactions on Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1145/3763326"&gt;10.1145/3763326&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;We introduce AnySplat, a feed-forward network for novel-view synthesis from uncalibrated image collections. In contrast to traditional neural-rendering pipelines that demand known camera poses and per-scene optimization, or recent feed-forward methods that buckle under the computational weight of dense views—our model predicts everything in one shot. A single forward pass yields a set of 3D Gaussian primitives encoding both scene geometry and appearance, and the corresponding camera intrinsics and extrinsics for each input image. This unified design scales effortlessly to casually captured, multi-view datasets without any pose annotations. In extensive zero-shot evaluations, AnySplat matches the quality of pose-aware baselines in both sparse- and dense-view scenarios while surpassing existing pose-free approaches. Moreover, it greatly reduces rendering latency compared to optimization-based neural fields, bringing real-time novel-view synthesis within reach for unconstrained capture settings. Project page: https://city-super.github.io/anysplat/.&lt;/p&gt;</content:encoded></item><item><title>Enhanced Spatiotemporal Consistency for Image-to-LiDAR Data Pretraining</title><link>https://doi.org/10.1109/tpami.2025.3640589</link><guid>10.1109/tpami.2025.3640589</guid><pubDate>Fri, 05 Dec 2025 18:37:55 +0000</pubDate><dc:creator>Xiang Xu</dc:creator><dc:creator>Lingdong Kong</dc:creator><dc:creator>Hui Shuai</dc:creator><dc:creator>Wenwei Zhang</dc:creator><dc:creator>Liang Pan</dc:creator><dc:creator>Kai Chen</dc:creator><dc:creator>Ziwei Liu</dc:creator><dc:creator>Qingshan Liu</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3640589</prism:doi><description>LiDAR representation learning has emerged as a promising approach to reducing reliance on costly and labor-intensive human annotations. While existing methods primarily focus on spatial alignment between LiDAR and camera sensors, they often overlook the temporal dynamics critical for capturing motion and scene continuity in driving scenarios. To address this limitation, we propose SuperFlow++, a novel framework that integrates spatiotemporal cues in both pretraining and downstream tasks using consecutive LiDAR-camera pairs. SuperFlow++ introduces four key components: (1) a view consistency alignment module to unify semantic information across camera views, (2) a dense-to-sparse consistency regularization mechanism to enhance feature robustness across varying point cloud densities, (3) a flow-based contrastive learning approach that models temporal relationships for improved scene understanding, and (4) a temporal voting strategy that propagates semantic information across LiDAR scans to improve prediction consistency. Extensive evaluations on 11 heterogeneous LiDAR datasets demonstrate that SuperFlow++ outperforms state-of-the-art methods across diverse tasks and driving conditions. Furthermore, by scaling both 2D and 3D backbones during pretraining, we uncover emergent properties that provide deeper insights into developing scalable 3D foundation models. With strong generalizability and computational efficiency, SuperFlow++ establishes a new benchmark for data-efficient LiDAR-based perception in autonomous driving. The code is publicly available at https://github.com/Xiangxu-0103/SuperFlow.
Published: 2025-12-05T18:37:55+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiang Xu; Lingdong Kong; Hui Shuai; Wenwei Zhang; Liang Pan; Kai Chen; Ziwei Liu; Qingshan Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3640589"&gt;10.1109/tpami.2025.3640589&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;LiDAR representation learning has emerged as a promising approach to reducing reliance on costly and labor-intensive human annotations. While existing methods primarily focus on spatial alignment between LiDAR and camera sensors, they often overlook the temporal dynamics critical for capturing motion and scene continuity in driving scenarios. To address this limitation, we propose SuperFlow++, a novel framework that integrates spatiotemporal cues in both pretraining and downstream tasks using consecutive LiDAR-camera pairs. SuperFlow++ introduces four key components: (1) a view consistency alignment module to unify semantic information across camera views, (2) a dense-to-sparse consistency regularization mechanism to enhance feature robustness across varying point cloud densities, (3) a flow-based contrastive learning approach that models temporal relationships for improved scene understanding, and (4) a temporal voting strategy that propagates semantic information across LiDAR scans to improve prediction consistency. Extensive evaluations on 11 heterogeneous LiDAR datasets demonstrate that SuperFlow++ outperforms state-of-the-art methods across diverse tasks and driving conditions. Furthermore, by scaling both 2D and 3D backbones during pretraining, we uncover emergent properties that provide deeper insights into developing scalable 3D foundation models. With strong generalizability and computational efficiency, SuperFlow++ establishes a new benchmark for data-efficient LiDAR-based perception in autonomous driving. The code is publicly available at https://github.com/Xiangxu-0103/SuperFlow.&lt;/p&gt;</content:encoded></item><item><title>Toward an Advanced Temporal Graph Network in Hyperbolic Space</title><link>https://doi.org/10.1109/tpami.2025.3640172</link><guid>10.1109/tpami.2025.3640172</guid><pubDate>Thu, 04 Dec 2025 18:37:06 +0000</pubDate><dc:creator>Viet Quan Le</dc:creator><dc:creator>Viet Cuong Ta</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3640172</prism:doi><description>Learning over dynamic graphs poses major challenges, including capturing the evolving relationship in the graphs. Inspired by the advantages of hyperbolic embedding in static graphs, the hyperbolic space is expected to capture complex interactions in dynamic graphs. However, due to the distortion errors in the standard tangent space mappings, hyperbolic methods become more sensitive to noise and reduce the learning capacity. To address the distortion in tangent space, we proposed HMPTGN, a temporal graph network that operates directly on the hyperbolic manifold. In this journal paper, we introduce the HMPTGN+ architecture, an extension of the original HMPTGN with major updates to learn better representations of dynamic graphs based on the hyperbolic embedding. Our framework incorporates a high-order graph neural network for extracting spatial dependencies, a dilated causal attention mechanism for modeling temporal patterns while preserving causality, and a curvature-awareness mechanism to capture dynamic structures. Extensive experiments demonstrate the effectiveness of our proposed HMPTGN+ framework over state-of-the-art baselines in both temporal link prediction and temporal new link prediction tasks. Our implementation is available at the GitHub repository https://github.com/quanlv9211/HMPTGN_plus
Published: 2025-12-04T18:37:06+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Viet Quan Le; Viet Cuong Ta&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3640172"&gt;10.1109/tpami.2025.3640172&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Learning over dynamic graphs poses major challenges, including capturing the evolving relationship in the graphs. Inspired by the advantages of hyperbolic embedding in static graphs, the hyperbolic space is expected to capture complex interactions in dynamic graphs. However, due to the distortion errors in the standard tangent space mappings, hyperbolic methods become more sensitive to noise and reduce the learning capacity. To address the distortion in tangent space, we proposed HMPTGN, a temporal graph network that operates directly on the hyperbolic manifold. In this journal paper, we introduce the HMPTGN+ architecture, an extension of the original HMPTGN with major updates to learn better representations of dynamic graphs based on the hyperbolic embedding. Our framework incorporates a high-order graph neural network for extracting spatial dependencies, a dilated causal attention mechanism for modeling temporal patterns while preserving causality, and a curvature-awareness mechanism to capture dynamic structures. Extensive experiments demonstrate the effectiveness of our proposed HMPTGN+ framework over state-of-the-art baselines in both temporal link prediction and temporal new link prediction tasks. Our implementation is available at the GitHub repository https://github.com/quanlv9211/HMPTGN_plus&lt;/p&gt;</content:encoded></item><item><title>Semantic Correspondence: Unified Benchmarking and a Strong Baseline</title><link>https://doi.org/10.1109/tpami.2025.3640429</link><guid>10.1109/tpami.2025.3640429</guid><pubDate>Thu, 04 Dec 2025 18:37:06 +0000</pubDate><dc:creator>Kaiyan Zhang</dc:creator><dc:creator>Xinghui Li</dc:creator><dc:creator>Jingyi Lu</dc:creator><dc:creator>Kai Han</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3640429</prism:doi><description>Establishing semantic correspondence is a challenging task in computer vision, aiming to match keypoints with the same semantic information across different images. Benefiting from the rapid development of deep learning, remarkable progress has been made over the past decade. However, a comprehensive review and analysis of this task remains absent. In this paper, we present the first extensive survey of semantic correspondence methods. We first propose a taxonomy to classify existing methods based on the type of their method designs. These methods are then categorized accordingly, and we provide a detailed analysis of each approach. Furthermore, we aggregate and summarize the results of methods in the literature across various benchmarks into a unified comparative table, with detailed configurations to highlight performance variations. Additionally, to provide a detailed understanding of existing methods for semantic matching, we thoroughly conduct controlled experiments to analyze the effectiveness of the components of different methods. Finally, we propose a simple yet effective baseline that achieves state-of-the-art performance on multiple benchmarks, providing a solid foundation for future research in this field. We hope this survey serves as a comprehensive reference and consolidated baseline for future development. Code is publicly available at: https://github.com/Visual-AI/Semantic-Correspondence.
Published: 2025-12-04T18:37:06+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kaiyan Zhang; Xinghui Li; Jingyi Lu; Kai Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3640429"&gt;10.1109/tpami.2025.3640429&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Establishing semantic correspondence is a challenging task in computer vision, aiming to match keypoints with the same semantic information across different images. Benefiting from the rapid development of deep learning, remarkable progress has been made over the past decade. However, a comprehensive review and analysis of this task remains absent. In this paper, we present the first extensive survey of semantic correspondence methods. We first propose a taxonomy to classify existing methods based on the type of their method designs. These methods are then categorized accordingly, and we provide a detailed analysis of each approach. Furthermore, we aggregate and summarize the results of methods in the literature across various benchmarks into a unified comparative table, with detailed configurations to highlight performance variations. Additionally, to provide a detailed understanding of existing methods for semantic matching, we thoroughly conduct controlled experiments to analyze the effectiveness of the components of different methods. Finally, we propose a simple yet effective baseline that achieves state-of-the-art performance on multiple benchmarks, providing a solid foundation for future research in this field. We hope this survey serves as a comprehensive reference and consolidated baseline for future development. Code is publicly available at: https://github.com/Visual-AI/Semantic-Correspondence.&lt;/p&gt;</content:encoded></item><item><title>Multi-modal Collaborative Learning with Vision Foundation Model Prompt Boosts 3D Semi-supervised Semantic Segmentation</title><link>https://doi.org/10.1016/j.inffus.2025.104019</link><guid>10.1016/j.inffus.2025.104019</guid><pubDate>Fri, 05 Dec 2025 00:44:45 +0000</pubDate><dc:creator>Xiang He</dc:creator><dc:creator>Xu Li</dc:creator><dc:creator>Baidan Li</dc:creator><dc:creator>Zhiyuan Xu</dc:creator><dc:creator>Qimin Xu</dc:creator><dc:creator>Hongwei Lu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104019</prism:doi><description>3D semi-supervised semantic segmentation aims to mitigate heavy reliance on large-scale high-quality annotations to achieve accurate fine-grained and stereoscopic perception, and serves as a promising technique for intelligent industries. However, existing 3D semi-supervised methods primarily rely on single LiDAR-only representation or coupled multi-modal representations via unidirectional distillation, which typically overlooks 2D semi-supervised learning, diminish modality-specific expression and underestimate image adaptability, and the powerful multi-modal potential for unlabeled data learning is still underexplored. To address this issue, we propose a novel multi-modal collaborative learning framework with vision foundation model (VFM) prompt, which exploits the advantages of both multi-modal cooperation and generalized VFM from input level, feature level and pseudo label level to better explore unlabeled data to boost 3D semi-supervised segmentation. Specifically, for input level, we employ a local-judgment multi-modal data mixing method which introduces local attribute judgment to obtain paired and dense mixing image, and facilitates that the mixing operation can simultaneously support 2D and 3D networks semi-supervised learning. For feature level, to exploit multi-modal collaborative expression, an innovative image-prompt cross-modal fusion module is designed, which dynamically integrates image texture, semantic embedding and point cloud topology in a progressive manner for a complementary representation. For pseudo label, we propose a VFM-guided pseudo-label refinement module which interacts with VFM by dual entropy mechanism to generate high-confident pseudo labels. Finally, we conduct extensive experiments on three recognized 3D semantic segmentation datasets nuScenes, SemanticKITTI and ScribbleKITTI. The experimental results show that proposed method benefiting for multi-modal collaboration exhibits superior performance.
Published: 2025-12-05T00:44:45+00:00
Venue: Information Fusion
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiang He; Xu Li; Baidan Li; Zhiyuan Xu; Qimin Xu; Hongwei Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104019"&gt;10.1016/j.inffus.2025.104019&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;3D semi-supervised semantic segmentation aims to mitigate heavy reliance on large-scale high-quality annotations to achieve accurate fine-grained and stereoscopic perception, and serves as a promising technique for intelligent industries. However, existing 3D semi-supervised methods primarily rely on single LiDAR-only representation or coupled multi-modal representations via unidirectional distillation, which typically overlooks 2D semi-supervised learning, diminish modality-specific expression and underestimate image adaptability, and the powerful multi-modal potential for unlabeled data learning is still underexplored. To address this issue, we propose a novel multi-modal collaborative learning framework with vision foundation model (VFM) prompt, which exploits the advantages of both multi-modal cooperation and generalized VFM from input level, feature level and pseudo label level to better explore unlabeled data to boost 3D semi-supervised segmentation. Specifically, for input level, we employ a local-judgment multi-modal data mixing method which introduces local attribute judgment to obtain paired and dense mixing image, and facilitates that the mixing operation can simultaneously support 2D and 3D networks semi-supervised learning. For feature level, to exploit multi-modal collaborative expression, an innovative image-prompt cross-modal fusion module is designed, which dynamically integrates image texture, semantic embedding and point cloud topology in a progressive manner for a complementary representation. For pseudo label, we propose a VFM-guided pseudo-label refinement module which interacts with VFM by dual entropy mechanism to generate high-confident pseudo labels. Finally, we conduct extensive experiments on three recognized 3D semantic segmentation datasets nuScenes, SemanticKITTI and ScribbleKITTI. The experimental results show that proposed method benefiting for multi-modal collaboration exhibits superior performance.&lt;/p&gt;</content:encoded></item><item><title>CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning</title><link>https://doi.org/10.1016/j.inffus.2025.104027</link><guid>10.1016/j.inffus.2025.104027</guid><pubDate>Fri, 05 Dec 2025 00:44:23 +0000</pubDate><dc:creator>Wenjie Li</dc:creator><dc:creator>Yujie Zhang</dc:creator><dc:creator>Haoran Sun</dc:creator><dc:creator>Yueqi Li</dc:creator><dc:creator>Fanrui Zhang</dc:creator><dc:creator>Mengzhe Xu</dc:creator><dc:creator>Victoria Borja Clausich</dc:creator><dc:creator>Sade Mellin</dc:creator><dc:creator>Renhao Yang</dc:creator><dc:creator>Chenrun Wang</dc:creator><dc:creator>Jethro Zih-Shuo Wang</dc:creator><dc:creator>Shiyi Yao</dc:creator><dc:creator>Gen Li</dc:creator><dc:creator>Yidong Xu</dc:creator><dc:creator>Hanyu Wang</dc:creator><dc:creator>Yilin Huang</dc:creator><dc:creator>Angela Lin Wang</dc:creator><dc:creator>Chen Shi</dc:creator><dc:creator>Yin Zhang</dc:creator><dc:creator>Jianan Guo</dc:creator><dc:creator>Luqi Yang</dc:creator><dc:creator>Renxuan Li</dc:creator><dc:creator>Yang Xu</dc:creator><dc:creator>Jiawei Liu</dc:creator><dc:creator>Yao Zhang</dc:creator><dc:creator>Lei Liu</dc:creator><dc:creator>Carlos Gutiérrez Sanromán</dc:creator><dc:creator>Lei Wang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104027</prism:doi><description>Chest X-ray (CXR) imaging is one of the most widely used diagnostic modalities in clinical practice, encompassing a broad spectrum of diagnostic tasks. Recent advancements have seen the extensive application of reasoning-based multimodal large language models (MLLMs) in medical imaging to enhance diagnostic efficiency and interpretability. However, existing multimodal models predominantly rely on ”one-time” diagnostic approaches, lacking verifiable supervision of the reasoning process. This leads to challenges in multi-task CXR diagnosis, including lengthy reasoning, sparse rewards, and frequent hallucinations. To address these issues, we propose CX-Mind , the first generative model to achieve interleaved ”think-answer” reasoning for CXR tasks, driven by curriculum-based reinforcement learning and verifiable process rewards (CuRL-VPR). Specifically, we constructed an instruction-tuning dataset, CX-Set, comprising 708,473 images and 2,619,148 samples, and generated 42,828 high-quality interleaved reasoning data points supervised by clinical reports. Optimization was conducted in two stages under the Group Relative Policy Optimization framework: initially stabilizing basic reasoning with closed-domain tasks, followed by transfer to open-domain diagnostics, incorporating rule-based conditional process rewards to bypass the need for pretrained reward models. Extensive experimental results demonstrate that CX-Mind significantly outperforms existing medical and general-domain MLLMs in visual understanding, text generation, and spatiotemporal alignment, achieving an average performance improvement of 25.1% over comparable CXR-specific models. On a real-world clinical dataset (Rui-CXR), CX-Mind achieves a mean recall@1 across 14 diseases that substantially surpasses the second-best results, with multi-center expert evaluations further confirming its clinical utility across multiple dimensions. CX-Mind establishes a new paradigm for constructing interpretable, and high-performing medical MLLMs.
Published: 2025-12-05T00:44:23+00:00
Venue: Information Fusion
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenjie Li; Yujie Zhang; Haoran Sun; Yueqi Li; Fanrui Zhang; Mengzhe Xu; Victoria Borja Clausich; Sade Mellin; Renhao Yang; Chenrun Wang; Jethro Zih-Shuo Wang; Shiyi Yao; Gen Li; Yidong Xu; Hanyu Wang; Yilin Huang; Angela Lin Wang; Chen Shi; Yin Zhang; Jianan Guo; Luqi Yang; Renxuan Li; Yang Xu; Jiawei Liu; Yao Zhang; Lei Liu; Carlos Gutiérrez Sanromán; Lei Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104027"&gt;10.1016/j.inffus.2025.104027&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Chest X-ray (CXR) imaging is one of the most widely used diagnostic modalities in clinical practice, encompassing a broad spectrum of diagnostic tasks. Recent advancements have seen the extensive application of reasoning-based multimodal large language models (MLLMs) in medical imaging to enhance diagnostic efficiency and interpretability. However, existing multimodal models predominantly rely on ”one-time” diagnostic approaches, lacking verifiable supervision of the reasoning process. This leads to challenges in multi-task CXR diagnosis, including lengthy reasoning, sparse rewards, and frequent hallucinations. To address these issues, we propose CX-Mind , the first generative model to achieve interleaved ”think-answer” reasoning for CXR tasks, driven by curriculum-based reinforcement learning and verifiable process rewards (CuRL-VPR). Specifically, we constructed an instruction-tuning dataset, CX-Set, comprising 708,473 images and 2,619,148 samples, and generated 42,828 high-quality interleaved reasoning data points supervised by clinical reports. Optimization was conducted in two stages under the Group Relative Policy Optimization framework: initially stabilizing basic reasoning with closed-domain tasks, followed by transfer to open-domain diagnostics, incorporating rule-based conditional process rewards to bypass the need for pretrained reward models. Extensive experimental results demonstrate that CX-Mind significantly outperforms existing medical and general-domain MLLMs in visual understanding, text generation, and spatiotemporal alignment, achieving an average performance improvement of 25.1% over comparable CXR-specific models. On a real-world clinical dataset (Rui-CXR), CX-Mind achieves a mean recall@1 across 14 diseases that substantially surpasses the second-best results, with multi-center expert evaluations further confirming its clinical utility across multiple dimensions. CX-Mind establishes a new paradigm for constructing interpretable, and high-performing medical MLLMs.&lt;/p&gt;</content:encoded></item><item><title>OneThinker: All-in-one Reasoning Model for Image and Video</title><link>https://arxiv.org/abs/2512.03043v2</link><guid>http://arxiv.org/abs/2512.03043v2</guid><pubDate>Tue, 02 Dec 2025 18:59:52 +0000</pubDate><dc:creator>Kaituo Feng</dc:creator><dc:creator>Manyuan Zhang</dc:creator><dc:creator>Hongyu Li</dc:creator><dc:creator>Kaixuan Fan</dc:creator><dc:creator>Shuang Chen</dc:creator><dc:creator>Yilei Jiang</dc:creator><dc:creator>Dian Zheng</dc:creator><dc:creator>Peiwen Sun</dc:creator><dc:creator>Yiyuan Zhang</dc:creator><dc:creator>Haoze Sun</dc:creator><dc:creator>Yan Feng</dc:creator><dc:creator>Peng Pei</dc:creator><dc:creator>Xunliang Cai</dc:creator><dc:creator>Xiangyu Yue</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.
Published: 2025-12-02T18:59:52+00:00
Venue: arXiv
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kaituo Feng; Manyuan Zhang; Hongyu Li; Kaixuan Fan; Shuang Chen; Yilei Jiang; Dian Zheng; Peiwen Sun; Yiyuan Zhang; Haoze Sun; Yan Feng; Peng Pei; Xunliang Cai; Xiangyu Yue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.&lt;/p&gt;</content:encoded></item><item><title>COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence</title><link>https://arxiv.org/abs/2512.04563v1</link><guid>http://arxiv.org/abs/2512.04563v1</guid><pubDate>Thu, 04 Dec 2025 08:26:04 +0000</pubDate><dc:creator>Zefeng Zhang</dc:creator><dc:creator>Xiangzhao Hao</dc:creator><dc:creator>Hengzhu Tang</dc:creator><dc:creator>Zhenyu Zhang</dc:creator><dc:creator>Jiawei Sheng</dc:creator><dc:creator>Xiaodong Li</dc:creator><dc:creator>Zhenyang Li</dc:creator><dc:creator>Li Gao</dc:creator><dc:creator>Daiting Shi</dc:creator><dc:creator>Dawei Yin</dc:creator><dc:creator>Tingwen Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose \textbf{COOPER}, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average \textbf{6.91\%} improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a \textbf{7.92\%} gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.
Published: 2025-12-04T08:26:04+00:00
Venue: arXiv
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zefeng Zhang; Xiangzhao Hao; Hengzhu Tang; Zhenyu Zhang; Jiawei Sheng; Xiaodong Li; Zhenyang Li; Li Gao; Daiting Shi; Dawei Yin; Tingwen Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose \textbf{COOPER}, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average \textbf{6.91\%} improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a \textbf{7.92\%} gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.&lt;/p&gt;</content:encoded></item><item><title>CrossSeg-GvT: multi-view graph vision transformers with context-aware memory and meta prompting for cross-domain few-shot semantic segmentation</title><link>https://doi.org/10.1016/j.neucom.2025.132337</link><guid>10.1016/j.neucom.2025.132337</guid><pubDate>Fri, 05 Dec 2025 08:07:58 +0000</pubDate><dc:creator>Anil Ahmed</dc:creator><dc:creator>Degen Huang</dc:creator><dc:creator>Salahuddin Unar</dc:creator><dc:creator>Mobeen Nazar</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132337</prism:doi><description>Generalizing few-shot semantic segmentation to unseen domains with minimal annotations remains challenging, especially under significant domain shifts. This paper presents CrossSeg-GvT, a novel framework that integrates graph-based reasoning with meta-learning to enhance cross-domain adaptability in few-shot segmentation. Our approach introduces three core innovations: (1) an Enhanced Graph Vision Transformer employing adaptive graph smoothing mitigation to dynamically refine node relationships through multi-view attention, (2) a Context-Aware Memory Module that preserves domain-invariant features via learnable memory banks, and (3) a Cross-Domain Fusion Module enabling adaptive feature composition through domain-specific prompting. Leveraging a meta-learning paradigm with a ViT backbone pre-trained on multiple source domains (PASCAL VOC, ADE20K, and SBD), our approach is evaluated on five diverse target domains, including medical imaging (ISIC2018), satellite imagery (DeepGlobe), and autonomous driving datasets (Cityscapes). Comprehensive experiments demonstrate that CrossSeg-GvT achieves state-of-the-art performance, with average mIoU scores of 71.1 % in the 1-shot setting and 75.5 % in the 5-shot setting across four challenging benchmarks. Notably, on the Cityscapes dataset, our model attains mIoU scores of 74.5 % and 77.3 % in the 1-shot and 5-shot settings, respectively, while on the ISIC2018 dataset it achieves 67.9 % and 70.6 % mIoU. These results reflect an improvement of approximately 7.2 %–12.1 % percentage points over the existing methods. Ablation studies further reveal the relative contributions of each component, with the adaptive graph smoothing mechanism accounting for a significant portion of the error reduction in domain-shift scenarios. The proposed framework advances the practical applications of few-shot segmentation in real-world scenarios with domain discrepancies.
Published: 2025-12-05T08:07:58+00:00
Venue: Neurocomputing
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Anil Ahmed; Degen Huang; Salahuddin Unar; Mobeen Nazar&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132337"&gt;10.1016/j.neucom.2025.132337&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;Generalizing few-shot semantic segmentation to unseen domains with minimal annotations remains challenging, especially under significant domain shifts. This paper presents CrossSeg-GvT, a novel framework that integrates graph-based reasoning with meta-learning to enhance cross-domain adaptability in few-shot segmentation. Our approach introduces three core innovations: (1) an Enhanced Graph Vision Transformer employing adaptive graph smoothing mitigation to dynamically refine node relationships through multi-view attention, (2) a Context-Aware Memory Module that preserves domain-invariant features via learnable memory banks, and (3) a Cross-Domain Fusion Module enabling adaptive feature composition through domain-specific prompting. Leveraging a meta-learning paradigm with a ViT backbone pre-trained on multiple source domains (PASCAL VOC, ADE20K, and SBD), our approach is evaluated on five diverse target domains, including medical imaging (ISIC2018), satellite imagery (DeepGlobe), and autonomous driving datasets (Cityscapes). Comprehensive experiments demonstrate that CrossSeg-GvT achieves state-of-the-art performance, with average mIoU scores of 71.1 % in the 1-shot setting and 75.5 % in the 5-shot setting across four challenging benchmarks. Notably, on the Cityscapes dataset, our model attains mIoU scores of 74.5 % and 77.3 % in the 1-shot and 5-shot settings, respectively, while on the ISIC2018 dataset it achieves 67.9 % and 70.6 % mIoU. These results reflect an improvement of approximately 7.2 %–12.1 % percentage points over the existing methods. Ablation studies further reveal the relative contributions of each component, with the adaptive graph smoothing mechanism accounting for a significant portion of the error reduction in domain-shift scenarios. The proposed framework advances the practical applications of few-shot segmentation in real-world scenarios with domain discrepancies.&lt;/p&gt;</content:encoded></item><item><title>Homophily Edge Augment Graph Neural Network for High-Class Homophily Variance Learning</title><link>https://doi.org/10.1109/tpami.2025.3640635</link><guid>10.1109/tpami.2025.3640635</guid><pubDate>Fri, 05 Dec 2025 18:37:55 +0000</pubDate><dc:creator>Minjian Guang</dc:creator><dc:creator>Rui Zhang</dc:creator><dc:creator>Dawei Cheng</dc:creator><dc:creator>Xiaoyang Wang</dc:creator><dc:creator>Xin Liu</dc:creator><dc:creator>Jie Yang</dc:creator><dc:creator>Yi Ouyang</dc:creator><dc:creator>Xian Wu</dc:creator><dc:creator>Yefeng Zheng</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3640635</prism:doi><description>Graph Neural Networks (GNNs) have achieved remarkable success in machine learning tasks by learning the features of graph data. However, experiments show that vanilla GNNs fail to achieve good classification performance in the field of graph anomaly detection. To address this issue, we propose and theoretically prove that the high-Class Homophily Variance (CHV) characteristic is the reason behind the suboptimal performance of GNN models in anomaly detection tasks. Statistical analysis shows that in most standard node classification datasets, homophily levels are similar across all classes, so CHV is low. In contrast, graph anomaly detection datasets have high CHV, as benign nodes are highly homophilic while anomalies are not, leading to a clear separation. To mitigate its impact, we propose a novel GNN model named Homophily Edge Augment Graph Neural Network (HEAug). Different from previous work, our method emphasizes generating new edges with low CHV value, using the original edges as an auxiliary. HEAug samples homophily adjacency matrices from scratch using a self-attention mechanism, and leverages nodes that are relevant in the feature space but not directly connected in the original graph. Additionally, we modify the loss function to punish the generation of unnecessary heterophilic edges by the model. Extensive comparison experiments demonstrate that HEAug achieved the best performance across eight benchmark datasets, including anomaly detection, edgeless node classification and adversarial attack. We also defined a heterophily attack to increase the CHV value in other graphs, demonstrating the effectiveness of our theory and model in various scenarios.
Published: 2025-12-05T18:37:55+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Minjian Guang; Rui Zhang; Dawei Cheng; Xiaoyang Wang; Xin Liu; Jie Yang; Yi Ouyang; Xian Wu; Yefeng Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3640635"&gt;10.1109/tpami.2025.3640635&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;Graph Neural Networks (GNNs) have achieved remarkable success in machine learning tasks by learning the features of graph data. However, experiments show that vanilla GNNs fail to achieve good classification performance in the field of graph anomaly detection. To address this issue, we propose and theoretically prove that the high-Class Homophily Variance (CHV) characteristic is the reason behind the suboptimal performance of GNN models in anomaly detection tasks. Statistical analysis shows that in most standard node classification datasets, homophily levels are similar across all classes, so CHV is low. In contrast, graph anomaly detection datasets have high CHV, as benign nodes are highly homophilic while anomalies are not, leading to a clear separation. To mitigate its impact, we propose a novel GNN model named Homophily Edge Augment Graph Neural Network (HEAug). Different from previous work, our method emphasizes generating new edges with low CHV value, using the original edges as an auxiliary. HEAug samples homophily adjacency matrices from scratch using a self-attention mechanism, and leverages nodes that are relevant in the feature space but not directly connected in the original graph. Additionally, we modify the loss function to punish the generation of unnecessary heterophilic edges by the model. Extensive comparison experiments demonstrate that HEAug achieved the best performance across eight benchmark datasets, including anomaly detection, edgeless node classification and adversarial attack. We also defined a heterophily attack to increase the CHV value in other graphs, demonstrating the effectiveness of our theory and model in various scenarios.&lt;/p&gt;</content:encoded></item><item><title>Generalizing Vision-Language Models with Dedicated Prompt Guidance</title><link>https://arxiv.org/abs/2512.02421v1</link><guid>http://arxiv.org/abs/2512.02421v1</guid><pubDate>Tue, 02 Dec 2025 05:06:17 +0000</pubDate><dc:creator>Xinyao Li</dc:creator><dc:creator>Yinjie Min</dc:creator><dc:creator>Hongbo Chen</dc:creator><dc:creator>Zhekai Du</dc:creator><dc:creator>Fengling Li</dc:creator><dc:creator>Jingjing Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Fine-tuning large pretrained vision-language models (VLMs) has emerged as a prevalent paradigm for downstream adaptation, yet it faces a critical trade-off between domain specificity and domain generalization (DG) ability. Current methods typically fine-tune a universal model on the entire dataset, which potentially compromises the ability to generalize to unseen domains. To fill this gap, we provide a theoretical understanding of the generalization ability for VLM fine-tuning, which reveals that training multiple parameter-efficient expert models on partitioned source domains leads to better generalization than fine-tuning a universal model. Inspired by this finding, we propose a two-step domain-expert-Guided DG (GuiDG) framework. GuiDG first employs prompt tuning to obtain source domain experts, then introduces a Cross-Modal Attention module to guide the fine-tuning of the vision encoder via adaptive expert integration. To better evaluate few-shot DG, we construct ImageNet-DG from ImageNet and its variants. Extensive experiments on standard DG benchmarks and ImageNet-DG demonstrate that GuiDG improves upon state-of-the-art fine-tuning methods while maintaining efficiency.
Published: 2025-12-02T05:06:17+00:00
Venue: arXiv
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinyao Li; Yinjie Min; Hongbo Chen; Zhekai Du; Fengling Li; Jingjing Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;Fine-tuning large pretrained vision-language models (VLMs) has emerged as a prevalent paradigm for downstream adaptation, yet it faces a critical trade-off between domain specificity and domain generalization (DG) ability. Current methods typically fine-tune a universal model on the entire dataset, which potentially compromises the ability to generalize to unseen domains. To fill this gap, we provide a theoretical understanding of the generalization ability for VLM fine-tuning, which reveals that training multiple parameter-efficient expert models on partitioned source domains leads to better generalization than fine-tuning a universal model. Inspired by this finding, we propose a two-step domain-expert-Guided DG (GuiDG) framework. GuiDG first employs prompt tuning to obtain source domain experts, then introduces a Cross-Modal Attention module to guide the fine-tuning of the vision encoder via adaptive expert integration. To better evaluate few-shot DG, we construct ImageNet-DG from ImageNet and its variants. Extensive experiments on standard DG benchmarks and ImageNet-DG demonstrate that GuiDG improves upon state-of-the-art fine-tuning methods while maintaining efficiency.&lt;/p&gt;</content:encoded></item><item><title>Annotation-free cloud masking for PlanetScope images in the Arctic via cross-platform ability transfer using deep learning and foundation models</title><link>https://doi.org/10.1016/j.rse.2025.115138</link><guid>10.1016/j.rse.2025.115138</guid><pubDate>Fri, 05 Dec 2025 15:10:56 +0000</pubDate><dc:creator>Zhili Li</dc:creator><dc:creator>Yiqun Xie</dc:creator><dc:creator>Sergii Skakun</dc:creator><dc:creator>Xiaowei Jia</dc:creator><dc:creator>Gengchen Mai</dc:creator><dc:creator>William Lu</dc:creator><dc:creator>Matthew Tong</dc:creator><dc:creator>Zhihao Wang</dc:creator><prism:publicationName>Remote Sensing of Environment</prism:publicationName><prism:doi>10.1016/j.rse.2025.115138</prism:doi><description>Cloud masking is an essential task for satellite-based Earth monitoring, and the quality of cloud masks can directly impact the solutions of the downstream Earth monitoring tasks. While significant progress has been made especially for data with desired bands (e.g., thermal bands in Landsat-8), the masking quality on small satellites with higher resolution but fewer spectral bands is still unreliable at high latitudes, where confusion with snow and ice makes the task significantly more challenging. We propose a novel learning-enabled cross-platform ability transfer paradigm that offers a scalable and effective solution to tackle this challenge through a case study using PlanetScope images in the Arctic. A unique characteristic of the new paradigm is that it does not require manual annotations to be collected for PlanetScope images, which is often the bottleneck and the most time-consuming part of machine learning-based cloud masking, especially given the similarity between clouds and snow/ice. To realize this, our approach first designs and creates a new training dataset, Co-Clouds, which contains around 45,000 coincident pairs of PlanetScope and Landsat-8 image patches collected within a nearly simultaneous temporal window. This coincident dataset offers a way to generate large volumes of training data and builds a bridge to transfer Landsat-8’s stronger cloud masking skills in the Arctic to PlanetScope images via data-driven learning. We also show the feasibility of the ability transfer from spectral signatures (e.g., thermal bands) to spatial signatures (e.g., textures). Using our Co-Clouds dataset, we train several deep learning models including both regular-size deep learning models and large foundation models. To validate the quality of the masks, we further create a manually labeled cloud mask dataset for PlanetScope images in the Arctic. Both the quantitative and qualitative results show significant improvements over the current operational cloud masks by PlanetScope. For example, the large foundation models such as SegFormer achieve approximately 20 % higher overall accuracy and 28 % higher producer’s accuracy than the operational cloud masks, while maintaining comparable or better user’s accuracy exceeding 90 %. The new approach is also very easy to implement and extend to other platforms, opening new opportunities for broadcasting advanced skills from one platform to others.
Published: 2025-12-05T15:10:56+00:00
Venue: Remote Sensing of Environment
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhili Li; Yiqun Xie; Sergii Skakun; Xiaowei Jia; Gengchen Mai; William Lu; Matthew Tong; Zhihao Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing of Environment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.rse.2025.115138"&gt;10.1016/j.rse.2025.115138&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;Cloud masking is an essential task for satellite-based Earth monitoring, and the quality of cloud masks can directly impact the solutions of the downstream Earth monitoring tasks. While significant progress has been made especially for data with desired bands (e.g., thermal bands in Landsat-8), the masking quality on small satellites with higher resolution but fewer spectral bands is still unreliable at high latitudes, where confusion with snow and ice makes the task significantly more challenging. We propose a novel learning-enabled cross-platform ability transfer paradigm that offers a scalable and effective solution to tackle this challenge through a case study using PlanetScope images in the Arctic. A unique characteristic of the new paradigm is that it does not require manual annotations to be collected for PlanetScope images, which is often the bottleneck and the most time-consuming part of machine learning-based cloud masking, especially given the similarity between clouds and snow/ice. To realize this, our approach first designs and creates a new training dataset, Co-Clouds, which contains around 45,000 coincident pairs of PlanetScope and Landsat-8 image patches collected within a nearly simultaneous temporal window. This coincident dataset offers a way to generate large volumes of training data and builds a bridge to transfer Landsat-8’s stronger cloud masking skills in the Arctic to PlanetScope images via data-driven learning. We also show the feasibility of the ability transfer from spectral signatures (e.g., thermal bands) to spatial signatures (e.g., textures). Using our Co-Clouds dataset, we train several deep learning models including both regular-size deep learning models and large foundation models. To validate the quality of the masks, we further create a manually labeled cloud mask dataset for PlanetScope images in the Arctic. Both the quantitative and qualitative results show significant improvements over the current operational cloud masks by PlanetScope. For example, the large foundation models such as SegFormer achieve approximately 20 % higher overall accuracy and 28 % higher producer’s accuracy than the operational cloud masks, while maintaining comparable or better user’s accuracy exceeding 90 %. The new approach is also very easy to implement and extend to other platforms, opening new opportunities for broadcasting advanced skills from one platform to others.&lt;/p&gt;</content:encoded></item><item><title>GeoBridge: A Semantic-Anchored Multi-View Foundation Model Bridging Images and Text for Geo-Localization</title><link>https://arxiv.org/abs/2512.02697v1</link><guid>http://arxiv.org/abs/2512.02697v1</guid><pubDate>Tue, 02 Dec 2025 12:28:22 +0000</pubDate><dc:creator>Zixuan Song</dc:creator><dc:creator>Jing Zhang</dc:creator><dc:creator>Di Wang</dc:creator><dc:creator>Zidie Zhou</dc:creator><dc:creator>Wenbin Liu</dc:creator><dc:creator>Haonan Guo</dc:creator><dc:creator>En Wang</dc:creator><dc:creator>Bo Du</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image. However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable. It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image). To address these challenges, we propose GeoBridge, a foundation model that performs bidirectional matching across views and supports language-to-image retrieval. Going beyond traditional satellite-centric formulations, GeoBridge builds on a novel semantic-anchor mechanism that bridges multi-view features through textual descriptions for robust, flexible localization. In support of this task, we construct GeoLoc, the first large-scale, cross-modal, and multi-view aligned dataset comprising over 50,000 pairs of drone, street-view panorama, and satellite images as well as their textual descriptions, collected from 36 countries, ensuring both geographic and semantic alignment. We performed broad evaluations across multiple tasks. Experiments confirm that GeoLoc pre-training markedly improves geo-location accuracy for GeoBridge while promoting cross-domain generalization and cross-modal knowledge transfer. The dataset, source code, and pretrained models were released at https://github.com/MiliLab/GeoBridge.
Published: 2025-12-02T12:28:22+00:00
Venue: arXiv
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zixuan Song; Jing Zhang; Di Wang; Zidie Zhou; Wenbin Liu; Haonan Guo; En Wang; Bo Du&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image. However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable. It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image). To address these challenges, we propose GeoBridge, a foundation model that performs bidirectional matching across views and supports language-to-image retrieval. Going beyond traditional satellite-centric formulations, GeoBridge builds on a novel semantic-anchor mechanism that bridges multi-view features through textual descriptions for robust, flexible localization. In support of this task, we construct GeoLoc, the first large-scale, cross-modal, and multi-view aligned dataset comprising over 50,000 pairs of drone, street-view panorama, and satellite images as well as their textual descriptions, collected from 36 countries, ensuring both geographic and semantic alignment. We performed broad evaluations across multiple tasks. Experiments confirm that GeoLoc pre-training markedly improves geo-location accuracy for GeoBridge while promoting cross-domain generalization and cross-modal knowledge transfer. The dataset, source code, and pretrained models were released at https://github.com/MiliLab/GeoBridge.&lt;/p&gt;</content:encoded></item><item><title>Structure as an inductive bias for brain–model alignment</title><link>https://doi.org/10.1038/s42256-025-01155-y</link><guid>10.1038/s42256-025-01155-y</guid><pubDate>Thu, 04 Dec 2025 16:01:22 +0000</pubDate><dc:creator>Binxu Wang</dc:creator><dc:creator>Carlos R. Ponce</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01155-y</prism:doi><description>Even before training, convolutional neural networks may reflect the brain’s visual processing principles. A study now shows how structure alone can help to explain the alignment between brains and models.
Published: 2025-12-04T16:01:22+00:00
Venue: Nature Machine Intelligence
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Binxu Wang; Carlos R. Ponce&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01155-y"&gt;10.1038/s42256-025-01155-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;Even before training, convolutional neural networks may reflect the brain’s visual processing principles. A study now shows how structure alone can help to explain the alignment between brains and models.&lt;/p&gt;</content:encoded></item><item><title>GeoDiT: A Diffusion-based Vision-Language Model for Geospatial Understanding</title><link>https://arxiv.org/abs/2512.02505v1</link><guid>http://arxiv.org/abs/2512.02505v1</guid><pubDate>Tue, 02 Dec 2025 07:59:46 +0000</pubDate><dc:creator>Jiaqi Liu</dc:creator><dc:creator>Ronghao Fu</dc:creator><dc:creator>Haoran Liu</dc:creator><dc:creator>Lang Sun</dc:creator><dc:creator>Bo Yang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Autoregressive models are structurally misaligned with the inherently parallel nature of geospatial understanding, forcing a rigid sequential narrative onto scenes and fundamentally hindering the generation of structured and coherent outputs. We challenge this paradigm by reframing geospatial generation as a parallel refinement process, enabling a holistic, coarse-to-fine synthesis that resolves all semantic elements simultaneously. To operationalize this, we introduce GeoDiT, the first diffusion-based vision-language model tailored for the geospatial domain. Extensive experiments demonstrate that GeoDiT establishes a new state-of-the-art on benchmarks requiring structured, object-centric outputs. It achieves significant gains in image captioning, visual grounding, and multi-object detection, precisely the tasks where autoregressive models falter. Our work validates that aligning the generative process with the data's intrinsic structure is key to unlocking superior performance in complex geospatial analysis.
Published: 2025-12-02T07:59:46+00:00
Venue: arXiv
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaqi Liu; Ronghao Fu; Haoran Liu; Lang Sun; Bo Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;Autoregressive models are structurally misaligned with the inherently parallel nature of geospatial understanding, forcing a rigid sequential narrative onto scenes and fundamentally hindering the generation of structured and coherent outputs. We challenge this paradigm by reframing geospatial generation as a parallel refinement process, enabling a holistic, coarse-to-fine synthesis that resolves all semantic elements simultaneously. To operationalize this, we introduce GeoDiT, the first diffusion-based vision-language model tailored for the geospatial domain. Extensive experiments demonstrate that GeoDiT establishes a new state-of-the-art on benchmarks requiring structured, object-centric outputs. It achieves significant gains in image captioning, visual grounding, and multi-object detection, precisely the tasks where autoregressive models falter. Our work validates that aligning the generative process with the data&amp;#x27;s intrinsic structure is key to unlocking superior performance in complex geospatial analysis.&lt;/p&gt;</content:encoded></item><item><title>CartoMapQA: A Fundamental Benchmark Dataset Evaluating Vision-Language Models on Cartographic Map Understanding</title><link>https://arxiv.org/abs/2512.03558v1</link><guid>http://arxiv.org/abs/2512.03558v1</guid><pubDate>Wed, 03 Dec 2025 08:25:22 +0000</pubDate><dc:creator>Huy Quang Ung</dc:creator><dc:creator>Guillaume Habault</dc:creator><dc:creator>Yasutaka Nishimura</dc:creator><dc:creator>Hao Niu</dc:creator><dc:creator>Roberto Legaspi</dc:creator><dc:creator>Tomoki Oya</dc:creator><dc:creator>Ryoichi Kojima</dc:creator><dc:creator>Masato Taya</dc:creator><dc:creator>Chihiro Ono</dc:creator><dc:creator>Atsunori Minamikawa</dc:creator><dc:creator>Yan Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The rise of Visual-Language Models (LVLMs) has unlocked new possibilities for seamlessly integrating visual and textual information. However, their ability to interpret cartographic maps remains largely unexplored. In this paper, we introduce CartoMapQA, a benchmark specifically designed to evaluate LVLMs' understanding of cartographic maps through question-answering tasks. The dataset includes over 2000 samples, each composed of a cartographic map, a question (with open-ended or multiple-choice answers), and a ground-truth answer. These tasks span key low-, mid- and high-level map interpretation skills, including symbol recognition, embedded information extraction, scale interpretation, and route-based reasoning. Our evaluation of both open-source and proprietary LVLMs reveals persistent challenges: models frequently struggle with map-specific semantics, exhibit limited geospatial reasoning, and are prone to Optical Character Recognition (OCR)-related errors. By isolating these weaknesses, CartoMapQA offers a valuable tool for guiding future improvements in LVLM architectures. Ultimately, it supports the development of models better equipped for real-world applications that depend on robust and reliable map understanding, such as navigation, geographic search, and urban planning. Our source code and data are openly available to the research community at: https://github.com/ungquanghuy-kddi/CartoMapQA.git
Published: 2025-12-03T08:25:22+00:00
Venue: arXiv
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Huy Quang Ung; Guillaume Habault; Yasutaka Nishimura; Hao Niu; Roberto Legaspi; Tomoki Oya; Ryoichi Kojima; Masato Taya; Chihiro Ono; Atsunori Minamikawa; Yan Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;The rise of Visual-Language Models (LVLMs) has unlocked new possibilities for seamlessly integrating visual and textual information. However, their ability to interpret cartographic maps remains largely unexplored. In this paper, we introduce CartoMapQA, a benchmark specifically designed to evaluate LVLMs&amp;#x27; understanding of cartographic maps through question-answering tasks. The dataset includes over 2000 samples, each composed of a cartographic map, a question (with open-ended or multiple-choice answers), and a ground-truth answer. These tasks span key low-, mid- and high-level map interpretation skills, including symbol recognition, embedded information extraction, scale interpretation, and route-based reasoning. Our evaluation of both open-source and proprietary LVLMs reveals persistent challenges: models frequently struggle with map-specific semantics, exhibit limited geospatial reasoning, and are prone to Optical Character Recognition (OCR)-related errors. By isolating these weaknesses, CartoMapQA offers a valuable tool for guiding future improvements in LVLM architectures. Ultimately, it supports the development of models better equipped for real-world applications that depend on robust and reliable map understanding, such as navigation, geographic search, and urban planning. Our source code and data are openly available to the research community at: https://github.com/ungquanghuy-kddi/CartoMapQA.git&lt;/p&gt;</content:encoded></item><item><title>CombAlign: Enhancing Model Expressiveness in Unsupervised Graph Alignment</title><link>https://doi.org/10.1109/tkde.2025.3640287</link><guid>10.1109/tkde.2025.3640287</guid><pubDate>Thu, 04 Dec 2025 18:38:39 +0000</pubDate><dc:creator>Songyang Chen</dc:creator><dc:creator>Yu Liu</dc:creator><dc:creator>Lei Zou</dc:creator><dc:creator>Zexuan Wang</dc:creator><dc:creator>Youfang Lin</dc:creator><prism:publicationName>IEEE Transactions on Knowledge and Data Engineering</prism:publicationName><prism:doi>10.1109/tkde.2025.3640287</prism:doi><description>Unsupervised graph alignment finds the node correspondence between a pair of attributed graphs by only exploiting graph structure and node features. One category of recent studies first computes the node representation and then matches nodes with the largest embedding-based similarity, while the other category reduces the problem to optimal transport (OT) via Gromov-Wasserstein learning. However, it remains largely unexplored in the model expressiveness, as well as how theoretical expressivity impacts prediction accuracy. We investigate the model expressiveness from two aspects. First, we characterize the model's discriminative power in distinguishing matched and unmatched node pairs across two graphs. Second, we study the model's capability of guaranteeing node matching properties such as one-to-one matching and mutual alignment. Motivated by our theoretical analysis, we put forward a hybrid approach named CombAlign with stronger expressive power. Specifically, we enable cross-dimensional feature interaction for OT-based learning and propose an embedding-based method inspired by the Weisfeiler-Lehman test. We also apply non-uniform marginals obtained from the embedding-based modules to OT as priors for more expressiveness. Based on that, we propose a traditional algorithm-based refinement, which combines our OT and embedding-based predictions using the ensemble learning strategy and reduces the problem to maximum weight matching. With carefully designed edge weights, we ensure these matching properties and further enhance prediction accuracy. By extensive experiments, we demonstrate a significant improvement of 14.5% in alignment accuracy compared to state-of-the-art approaches and confirm the soundness of our theoretical analysis.
Published: 2025-12-04T18:38:39+00:00
Venue: IEEE Transactions on Knowledge and Data Engineering
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Songyang Chen; Yu Liu; Lei Zou; Zexuan Wang; Youfang Lin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Knowledge and Data Engineering&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tkde.2025.3640287"&gt;10.1109/tkde.2025.3640287&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Unsupervised graph alignment finds the node correspondence between a pair of attributed graphs by only exploiting graph structure and node features. One category of recent studies first computes the node representation and then matches nodes with the largest embedding-based similarity, while the other category reduces the problem to optimal transport (OT) via Gromov-Wasserstein learning. However, it remains largely unexplored in the model expressiveness, as well as how theoretical expressivity impacts prediction accuracy. We investigate the model expressiveness from two aspects. First, we characterize the model&amp;#x27;s discriminative power in distinguishing matched and unmatched node pairs across two graphs. Second, we study the model&amp;#x27;s capability of guaranteeing node matching properties such as one-to-one matching and mutual alignment. Motivated by our theoretical analysis, we put forward a hybrid approach named CombAlign with stronger expressive power. Specifically, we enable cross-dimensional feature interaction for OT-based learning and propose an embedding-based method inspired by the Weisfeiler-Lehman test. We also apply non-uniform marginals obtained from the embedding-based modules to OT as priors for more expressiveness. Based on that, we propose a traditional algorithm-based refinement, which combines our OT and embedding-based predictions using the ensemble learning strategy and reduces the problem to maximum weight matching. With carefully designed edge weights, we ensure these matching properties and further enhance prediction accuracy. By extensive experiments, we demonstrate a significant improvement of 14.5% in alignment accuracy compared to state-of-the-art approaches and confirm the soundness of our theoretical analysis.&lt;/p&gt;</content:encoded></item><item><title>Graph Node Embedding by Neighborhood Prediction Based on Multiview Contrastive Learning</title><link>https://doi.org/10.1016/j.knosys.2025.115026</link><guid>10.1016/j.knosys.2025.115026</guid><pubDate>Fri, 05 Dec 2025 08:08:38 +0000</pubDate><dc:creator>Chen Liu</dc:creator><dc:creator>Xuan Yao</dc:creator><dc:creator>Lixin Zhou</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115026</prism:doi><description>The connectivity between nodes and their neighbors (neighborhood structure) represents the most fundamental and intrinsic information in graphs. Graph representation learning deeply extracts useful information in graphs using graph neural networks, showing superior performance in various graph-based tasks. This study introduces a novel graph representation learning method, namely Graph contrastive learning by neighborhood prediction (GraphNP). Specifically, we design a new graph neural network architecture that incorporates neighborhood structural information to predict node embeddings. Beyond direct neighbors, the architecture hierarchically processes and aggregates multihop neighbor information to construct node embeddings. Furthermore, we incorporate multiview contrastive learning objectives that contrast multiple predicted embeddings with target embeddings in original view, to avoid reliance on manually crafted negative samples while avoiding complex momentum encoders. Extensive experiments on eight benchmark datasets reveal that GraphNP outperforms competing models, achieving an average performance gain of 12.9% and a maximum improvement of 25.7%, thereby confirming its effectiveness.
Published: 2025-12-05T08:08:38+00:00
Venue: Knowledge-Based Systems
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chen Liu; Xuan Yao; Lixin Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115026"&gt;10.1016/j.knosys.2025.115026&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;The connectivity between nodes and their neighbors (neighborhood structure) represents the most fundamental and intrinsic information in graphs. Graph representation learning deeply extracts useful information in graphs using graph neural networks, showing superior performance in various graph-based tasks. This study introduces a novel graph representation learning method, namely Graph contrastive learning by neighborhood prediction (GraphNP). Specifically, we design a new graph neural network architecture that incorporates neighborhood structural information to predict node embeddings. Beyond direct neighbors, the architecture hierarchically processes and aggregates multihop neighbor information to construct node embeddings. Furthermore, we incorporate multiview contrastive learning objectives that contrast multiple predicted embeddings with target embeddings in original view, to avoid reliance on manually crafted negative samples while avoiding complex momentum encoders. Extensive experiments on eight benchmark datasets reveal that GraphNP outperforms competing models, achieving an average performance gain of 12.9% and a maximum improvement of 25.7%, thereby confirming its effectiveness.&lt;/p&gt;</content:encoded></item><item><title>DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images</title><link>https://arxiv.org/abs/2512.03004v1</link><guid>http://arxiv.org/abs/2512.03004v1</guid><pubDate>Tue, 02 Dec 2025 18:29:18 +0000</pubDate><dc:creator>Xiaoxue Chen</dc:creator><dc:creator>Ziyi Xiong</dc:creator><dc:creator>Yuantao Chen</dc:creator><dc:creator>Gen Li</dc:creator><dc:creator>Nan Wang</dc:creator><dc:creator>Hongcheng Luo</dc:creator><dc:creator>Long Chen</dc:creator><dc:creator>Haiyang Sun</dc:creator><dc:creator>Bing Wang</dc:creator><dc:creator>Guang Chen</dc:creator><dc:creator>Hangjun Ye</dc:creator><dc:creator>Hongyang Li</dc:creator><dc:creator>Ya-Qin Zhang</dc:creator><dc:creator>Hao Zhao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.
Published: 2025-12-02T18:29:18+00:00
Venue: arXiv
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoxue Chen; Ziyi Xiong; Yuantao Chen; Gen Li; Nan Wang; Hongcheng Luo; Long Chen; Haiyang Sun; Bing Wang; Guang Chen; Hangjun Ye; Hongyang Li; Ya-Qin Zhang; Hao Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.&lt;/p&gt;</content:encoded></item><item><title>See, Think, Learn: A Self-Taught Multimodal Reasoner</title><link>https://arxiv.org/abs/2512.02456v1</link><guid>http://arxiv.org/abs/2512.02456v1</guid><pubDate>Tue, 02 Dec 2025 06:30:10 +0000</pubDate><dc:creator>Sourabh Sharma</dc:creator><dc:creator>Sonam Gupta</dc:creator><dc:creator>Sadbhawna</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language Models (VLMs) have achieved remarkable progress in integrating visual perception with language understanding. However, effective multimodal reasoning requires both accurate perception and robust reasoning, and weakness in either limits the performance of VLMs. Prior efforts to enhance reasoning often depend on high-quality chain-of-thought (CoT) data, obtained via labor-intensive human annotations, costly proprietary models, or self-training methods that overlook perception. To address these limitations, we propose a simple yet effective self-training framework called See-Think-Learn (STL). At its core, STL introduces a structured reasoning template that encourages the model to see before thinking, first extracting visual attributes in textual form, then using them to guide reasoning. The framework jointly improves perception and reasoning by having the model generate and learn from its own structured rationales in a self-training loop. Furthermore, we augment the training data with negative rationales, i.e. explanations that justify why certain answer choices are incorrect, to enhance the model's ability to distinguish between correct and misleading responses. This fosters more discriminative and robust learning. Experiments across diverse domains show that STL consistently outperforms baselines trained directly only on answers or self-generated reasoning, while qualitative analysis confirms the high quality of its rationales. STL thus provides a cost-effective solution to enhance multimodal reasoning ability of VLMs.
Published: 2025-12-02T06:30:10+00:00
Venue: arXiv
Score: 0.773 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sourabh Sharma; Sonam Gupta; Sadbhawna&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (must_read)&lt;/p&gt;
&lt;p&gt;Vision-Language Models (VLMs) have achieved remarkable progress in integrating visual perception with language understanding. However, effective multimodal reasoning requires both accurate perception and robust reasoning, and weakness in either limits the performance of VLMs. Prior efforts to enhance reasoning often depend on high-quality chain-of-thought (CoT) data, obtained via labor-intensive human annotations, costly proprietary models, or self-training methods that overlook perception. To address these limitations, we propose a simple yet effective self-training framework called See-Think-Learn (STL). At its core, STL introduces a structured reasoning template that encourages the model to see before thinking, first extracting visual attributes in textual form, then using them to guide reasoning. The framework jointly improves perception and reasoning by having the model generate and learn from its own structured rationales in a self-training loop. Furthermore, we augment the training data with negative rationales, i.e. explanations that justify why certain answer choices are incorrect, to enhance the model&amp;#x27;s ability to distinguish between correct and misleading responses. This fosters more discriminative and robust learning. Experiments across diverse domains show that STL consistently outperforms baselines trained directly only on answers or self-generated reasoning, while qualitative analysis confirms the high quality of its rationales. STL thus provides a cost-effective solution to enhance multimodal reasoning ability of VLMs.&lt;/p&gt;</content:encoded></item><item><title>MT-Depth: Multi-task Instance feature analysis for the Depth Completion</title><link>https://arxiv.org/abs/2512.04734v1</link><guid>http://arxiv.org/abs/2512.04734v1</guid><pubDate>Thu, 04 Dec 2025 12:17:33 +0000</pubDate><dc:creator>Abdul Haseeb Nizamani</dc:creator><dc:creator>Dandi Zhou</dc:creator><dc:creator>Xinhai Sun</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Depth completion plays a vital role in 3D perception systems, especially in scenarios where sparse depth data must be densified for tasks such as autonomous driving, robotics, and augmented reality. While many existing approaches rely on semantic segmentation to guide depth completion, they often overlook the benefits of object-level understanding. In this work, we introduce an instance-aware depth completion framework that explicitly integrates binary instance masks as spatial priors to refine depth predictions. Our model combines four main components: a frozen YOLO V11 instance segmentation branch, a U-Net-based depth completion backbone, a cross-attention fusion module, and an attention-guided prediction head. The instance segmentation branch generates per-image foreground masks that guide the depth branch via cross-attention, allowing the network to focus on object-centric regions during refinement. We validate our method on the Virtual KITTI 2 dataset, showing that it achieves lower RMSE compared to both a U-Net-only baseline and previous semantic-guided methods, while maintaining competitive MAE. Qualitative and quantitative results demonstrate that the proposed model effectively enhances depth accuracy near object boundaries, occlusions, and thin structures. Our findings suggest that incorporating instance-aware cues offers a promising direction for improving depth completion without relying on dense semantic labels.
Published: 2025-12-04T12:17:33+00:00
Venue: arXiv
Score: 0.773 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Abdul Haseeb Nizamani; Dandi Zhou; Xinhai Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (must_read)&lt;/p&gt;
&lt;p&gt;Depth completion plays a vital role in 3D perception systems, especially in scenarios where sparse depth data must be densified for tasks such as autonomous driving, robotics, and augmented reality. While many existing approaches rely on semantic segmentation to guide depth completion, they often overlook the benefits of object-level understanding. In this work, we introduce an instance-aware depth completion framework that explicitly integrates binary instance masks as spatial priors to refine depth predictions. Our model combines four main components: a frozen YOLO V11 instance segmentation branch, a U-Net-based depth completion backbone, a cross-attention fusion module, and an attention-guided prediction head. The instance segmentation branch generates per-image foreground masks that guide the depth branch via cross-attention, allowing the network to focus on object-centric regions during refinement. We validate our method on the Virtual KITTI 2 dataset, showing that it achieves lower RMSE compared to both a U-Net-only baseline and previous semantic-guided methods, while maintaining competitive MAE. Qualitative and quantitative results demonstrate that the proposed model effectively enhances depth accuracy near object boundaries, occlusions, and thin structures. Our findings suggest that incorporating instance-aware cues offers a promising direction for improving depth completion without relying on dense semantic labels.&lt;/p&gt;</content:encoded></item><item><title>SS4D: Native 4D Generative Model via Structured Spacetime Latents</title><link>https://doi.org/10.1145/3763302</link><guid>10.1145/3763302</guid><pubDate>Thu, 04 Dec 2025 17:15:39 +0000</pubDate><dc:creator>Zhibing Li</dc:creator><dc:creator>Mengchen Zhang</dc:creator><dc:creator>Tong Wu</dc:creator><dc:creator>Jing Tan</dc:creator><dc:creator>Jiaqi Wang</dc:creator><dc:creator>Dahua Lin</dc:creator><prism:publicationName>ACM Transactions on Graphics</prism:publicationName><prism:doi>10.1145/3763302</prism:doi><description>We present SS4D, a native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that construct 4D representations by optimizing over 3D or video generative models, we train a generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is a compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on a pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To support efficient training and inference over long video sequences, we compress the latent sequence along the temporal axis using factorized 4D convolutions and temporal downsampling blocks. In addition, we employ a carefully designed training strategy to enhance robustness against occlusion and motion blur, leading to high-quality generation. Extensive experiments show that SS4D produces spatio-temporally consistent 4D objects with superior quality and efficiency, significantly outperforming state-of-the-art methods on both synthetic and real-world datasets.
Published: 2025-12-04T17:15:39+00:00
Venue: ACM Transactions on Graphics
Score: 0.772 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhibing Li; Mengchen Zhang; Tong Wu; Jing Tan; Jiaqi Wang; Dahua Lin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ACM Transactions on Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1145/3763302"&gt;10.1145/3763302&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (must_read)&lt;/p&gt;
&lt;p&gt;We present SS4D, a native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that construct 4D representations by optimizing over 3D or video generative models, we train a generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is a compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on a pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To support efficient training and inference over long video sequences, we compress the latent sequence along the temporal axis using factorized 4D convolutions and temporal downsampling blocks. In addition, we employ a carefully designed training strategy to enhance robustness against occlusion and motion blur, leading to high-quality generation. Extensive experiments show that SS4D produces spatio-temporally consistent 4D objects with superior quality and efficiency, significantly outperforming state-of-the-art methods on both synthetic and real-world datasets.&lt;/p&gt;</content:encoded></item><item><title>Efficient and Scalable Point Cloud Generation With Sparse Point-Voxel Diffusion Models</title><link>https://doi.org/10.1109/tnnls.2025.3636409</link><guid>10.1109/tnnls.2025.3636409</guid><pubDate>Thu, 04 Dec 2025 18:38:05 +0000</pubDate><dc:creator>Ioannis Romanelis</dc:creator><dc:creator>Vlassis Fotis</dc:creator><dc:creator>Athanasios Kalogeras</dc:creator><dc:creator>Christos Alexakos</dc:creator><dc:creator>Adrian Munteanu</dc:creator><dc:creator>Konstantinos Moustakas</dc:creator><prism:publicationName>IEEE Transactions on Neural Networks and Learning Systems</prism:publicationName><prism:doi>10.1109/tnnls.2025.3636409</prism:doi><description>We propose a novel point cloud U-Net diffusion architecture for 3-D generative modeling capable of generating high-quality and diverse 3-D shapes while maintaining fast generation times. Our network employs a dual-branch architecture, combining the high-resolution representations of points with the computational efficiency of sparse voxels. Our fastest variant outperforms all nondiffusion generative approaches on unconditional shape generation, the most popular benchmark for evaluating point cloud generative models, while our largest model achieves state-of-the-art results among diffusion methods, with a runtime approximately 70% of the previously state-of-the-art point-voxel diffusion (PVD), measured on the same hardware setting. Beyond unconditional generation, we perform extensive evaluations, including conditional generation on all categories of ShapeNet, demonstrating the scalability of our model to larger datasets, and implicit generation, which allows our network to produce high-quality point clouds on fewer timesteps, further decreasing the generation time. Finally, we evaluate the architecture’s performance in point cloud completion and super-resolution. Our model excels in all tasks, establishing it as a state-of-the-art diffusion U-Net for point cloud generative modeling. The code is publicly available at https://github.com/JohnRomanelis/SPVD
Published: 2025-12-04T18:38:05+00:00
Venue: IEEE Transactions on Neural Networks and Learning Systems
Score: 0.770 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ioannis Romanelis; Vlassis Fotis; Athanasios Kalogeras; Christos Alexakos; Adrian Munteanu; Konstantinos Moustakas&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Neural Networks and Learning Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tnnls.2025.3636409"&gt;10.1109/tnnls.2025.3636409&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (must_read)&lt;/p&gt;
&lt;p&gt;We propose a novel point cloud U-Net diffusion architecture for 3-D generative modeling capable of generating high-quality and diverse 3-D shapes while maintaining fast generation times. Our network employs a dual-branch architecture, combining the high-resolution representations of points with the computational efficiency of sparse voxels. Our fastest variant outperforms all nondiffusion generative approaches on unconditional shape generation, the most popular benchmark for evaluating point cloud generative models, while our largest model achieves state-of-the-art results among diffusion methods, with a runtime approximately 70% of the previously state-of-the-art point-voxel diffusion (PVD), measured on the same hardware setting. Beyond unconditional generation, we perform extensive evaluations, including conditional generation on all categories of ShapeNet, demonstrating the scalability of our model to larger datasets, and implicit generation, which allows our network to produce high-quality point clouds on fewer timesteps, further decreasing the generation time. Finally, we evaluate the architecture’s performance in point cloud completion and super-resolution. Our model excels in all tasks, establishing it as a state-of-the-art diffusion U-Net for point cloud generative modeling. The code is publicly available at https://github.com/JohnRomanelis/SPVD&lt;/p&gt;</content:encoded></item><item><title>Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles</title><link>https://arxiv.org/abs/2512.03454v1</link><guid>http://arxiv.org/abs/2512.03454v1</guid><pubDate>Wed, 03 Dec 2025 05:14:16 +0000</pubDate><dc:creator>Haicheng Liao</dc:creator><dc:creator>Huanming Shen</dc:creator><dc:creator>Bonan Wang</dc:creator><dc:creator>Yongkang Li</dc:creator><dc:creator>Yihong Tang</dc:creator><dc:creator>Chengyue Wang</dc:creator><dc:creator>Dingyi Zhuang</dc:creator><dc:creator>Kehua Chen</dc:creator><dc:creator>Hai Yang</dc:creator><dc:creator>Chengzhong Xu</dc:creator><dc:creator>Zhenning Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.
Published: 2025-12-03T05:14:16+00:00
Venue: arXiv
Score: 0.770 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haicheng Liao; Huanming Shen; Bonan Wang; Yongkang Li; Yihong Tang; Chengyue Wang; Dingyi Zhuang; Kehua Chen; Hai Yang; Chengzhong Xu; Zhenning Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (must_read)&lt;/p&gt;
&lt;p&gt;Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.&lt;/p&gt;</content:encoded></item><item><title>SAM3-I: Segment Anything with Instructions</title><link>https://arxiv.org/abs/2512.04585v1</link><guid>http://arxiv.org/abs/2512.04585v1</guid><pubDate>Thu, 04 Dec 2025 09:00:25 +0000</pubDate><dc:creator>Jingjing Li</dc:creator><dc:creator>Yue Feng</dc:creator><dc:creator>Yuchen Guo</dc:creator><dc:creator>Jincai Huang</dc:creator><dc:creator>Yongri Piao</dc:creator><dc:creator>Qi Bi</dc:creator><dc:creator>Miao Zhang</dc:creator><dc:creator>Xiaoqi Zhao</dc:creator><dc:creator>Qiang Chen</dc:creator><dc:creator>Shihao Zou</dc:creator><dc:creator>Wei Ji</dc:creator><dc:creator>Huchuan Lu</dc:creator><dc:creator>Li Cheng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Segment Anything Model 3 (SAM3) has advanced open-vocabulary segmentation through promptable concept segmentation, allowing users to segment all instances corresponding to a given concept, typically specified with short noun-phrase (NP) prompts. While this marks the first integration of language-level concepts within the SAM family, real-world usage typically requires far richer expressions that include attributes, spatial relations, functionalities, actions, states, and even implicit reasoning over instances. Currently, SAM3 relies on external multi-modal agents to convert complex instructions into NPs and then conduct iterative mask filtering. However, these NP-level concepts remain overly coarse, often failing to precisely represent a specific instance. In this work, we present SAM3-I, an enhanced framework that unifies concept-level understanding and instruction-level reasoning within the SAM family. SAM3-I introduces an instruction-aware cascaded adaptation mechanism that progressively aligns expressive instruction semantics with SAM3's existing vision-language representations, enabling direct instruction-following segmentation without sacrificing its original concept-driven capabilities. Furthermore, we design a structured instruction taxonomy spanning concept, simple, and complex levels, and develop a scalable data engine to construct a dataset with diverse instruction-mask pairs. Experiments show that SAM3-I delivers appealing performance, demonstrating that SAM3 can be effectively extended to follow natural-language instructions while preserving its strong concept grounding. We open-source SAM3-I and provide practical fine-tuning workflows, enabling researchers to adapt it to domain-specific applications. The source code is available here.
Published: 2025-12-04T09:00:25+00:00
Venue: arXiv
Score: 0.770 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jingjing Li; Yue Feng; Yuchen Guo; Jincai Huang; Yongri Piao; Qi Bi; Miao Zhang; Xiaoqi Zhao; Qiang Chen; Shihao Zou; Wei Ji; Huchuan Lu; Li Cheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (must_read)&lt;/p&gt;
&lt;p&gt;Segment Anything Model 3 (SAM3) has advanced open-vocabulary segmentation through promptable concept segmentation, allowing users to segment all instances corresponding to a given concept, typically specified with short noun-phrase (NP) prompts. While this marks the first integration of language-level concepts within the SAM family, real-world usage typically requires far richer expressions that include attributes, spatial relations, functionalities, actions, states, and even implicit reasoning over instances. Currently, SAM3 relies on external multi-modal agents to convert complex instructions into NPs and then conduct iterative mask filtering. However, these NP-level concepts remain overly coarse, often failing to precisely represent a specific instance. In this work, we present SAM3-I, an enhanced framework that unifies concept-level understanding and instruction-level reasoning within the SAM family. SAM3-I introduces an instruction-aware cascaded adaptation mechanism that progressively aligns expressive instruction semantics with SAM3&amp;#x27;s existing vision-language representations, enabling direct instruction-following segmentation without sacrificing its original concept-driven capabilities. Furthermore, we design a structured instruction taxonomy spanning concept, simple, and complex levels, and develop a scalable data engine to construct a dataset with diverse instruction-mask pairs. Experiments show that SAM3-I delivers appealing performance, demonstrating that SAM3 can be effectively extended to follow natural-language instructions while preserving its strong concept grounding. We open-source SAM3-I and provide practical fine-tuning workflows, enabling researchers to adapt it to domain-specific applications. The source code is available here.&lt;/p&gt;</content:encoded></item><item><title>RAMEN: Resolution-Adjustable Multimodal Encoder for Earth Observation</title><link>https://arxiv.org/abs/2512.05025v1</link><guid>http://arxiv.org/abs/2512.05025v1</guid><pubDate>Thu, 04 Dec 2025 17:40:17 +0000</pubDate><dc:creator>Nicolas Houdré</dc:creator><dc:creator>Diego Marcos</dc:creator><dc:creator>Hugo Riffaud de Turckheim</dc:creator><dc:creator>Dino Ienco</dc:creator><dc:creator>Laurent Wendling</dc:creator><dc:creator>Camille Kurtz</dc:creator><dc:creator>Sylvain Lobry</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Earth observation (EO) data spans a wide range of spatial, spectral, and temporal resolutions, from high-resolution optical imagery to low resolution multispectral products or radar time series. While recent foundation models have improved multimodal integration for learning meaningful representations, they often expect fixed input resolutions or are based on sensor-specific encoders limiting generalization across heterogeneous EO modalities. To overcome these limitations we introduce RAMEN, a resolution-adjustable multimodal encoder that learns a shared visual representation across EO data in a fully sensor-agnostic manner. RAMEN treats the modality and spatial and temporal resolutions as key input data features, enabling coherent analysis across modalities within a unified latent space. Its main methodological contribution is to define spatial resolution as a controllable output parameter, giving users direct control over the desired level of detail at inference and allowing explicit trade-offs between spatial precision and computational cost. We train a single, unified transformer encoder reconstructing masked multimodal EO data drawn from diverse sources, ensuring generalization across sensors and resolutions. Once pretrained, RAMEN transfers effectively to both known and unseen sensor configurations and outperforms larger state-of-the-art models on the community-standard PANGAEA benchmark, containing various multi-sensor and multi-resolution downstream tasks. Our code and pretrained model are available at https://github.com/nicolashoudre/RAMEN.
Published: 2025-12-04T17:40:17+00:00
Venue: arXiv
Score: 0.769 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Nicolas Houdré; Diego Marcos; Hugo Riffaud de Turckheim; Dino Ienco; Laurent Wendling; Camille Kurtz; Sylvain Lobry&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.769 (must_read)&lt;/p&gt;
&lt;p&gt;Earth observation (EO) data spans a wide range of spatial, spectral, and temporal resolutions, from high-resolution optical imagery to low resolution multispectral products or radar time series. While recent foundation models have improved multimodal integration for learning meaningful representations, they often expect fixed input resolutions or are based on sensor-specific encoders limiting generalization across heterogeneous EO modalities. To overcome these limitations we introduce RAMEN, a resolution-adjustable multimodal encoder that learns a shared visual representation across EO data in a fully sensor-agnostic manner. RAMEN treats the modality and spatial and temporal resolutions as key input data features, enabling coherent analysis across modalities within a unified latent space. Its main methodological contribution is to define spatial resolution as a controllable output parameter, giving users direct control over the desired level of detail at inference and allowing explicit trade-offs between spatial precision and computational cost. We train a single, unified transformer encoder reconstructing masked multimodal EO data drawn from diverse sources, ensuring generalization across sensors and resolutions. Once pretrained, RAMEN transfers effectively to both known and unseen sensor configurations and outperforms larger state-of-the-art models on the community-standard PANGAEA benchmark, containing various multi-sensor and multi-resolution downstream tasks. Our code and pretrained model are available at https://github.com/nicolashoudre/RAMEN.&lt;/p&gt;</content:encoded></item><item><title>Generative models for SAR–optical image translation: A systematic review</title><link>https://doi.org/10.1016/j.jag.2025.105009</link><guid>10.1016/j.jag.2025.105009</guid><pubDate>Thu, 04 Dec 2025 22:45:43 +0000</pubDate><dc:creator>Zhao Wang</dc:creator><dc:creator>Zheng Zhang</dc:creator><dc:creator>Xiaojun Shan</dc:creator><dc:creator>Hong-an Wei</dc:creator><dc:creator>Ping Tang</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2025.105009</prism:doi><description>Growing demands in sustainable development and resource management are driving increasing reliance on remote sensing-based Earth observation and image interpretation. In parallel, multimodal collaborative processing is attracting research attention. Synthetic aperture radar (SAR) and optical images offer complementary advantages but pose challenges for simultaneous use due to platform constraints and environmental conditions, often leaving only one modality available and impeding joint analysis. Generative models, particularly generative adversarial networks (GANs) and diffusion models (DMs), address this by learning cross-modal mappings. Translated images preserve structure and semantics while adopting target characteristics, thereby facilitating collaborative use. This review systematically categorizes translation frameworks spanning GANs, DMs, and other generative models. It then details downstream tasks supported by SAR–optical translation, including cloud removal, change detection, semantic segmentation, registration, and object detection, highlighting how translation bridges data gaps and enhances interpretation robustness. Furthermore, we provide open-source code and public datasets, discuss current challenges, and outline future research directions.
Published: 2025-12-04T22:45:43+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.769 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhao Wang; Zheng Zhang; Xiaojun Shan; Hong-an Wei; Ping Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2025.105009"&gt;10.1016/j.jag.2025.105009&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.769 (must_read)&lt;/p&gt;
&lt;p&gt;Growing demands in sustainable development and resource management are driving increasing reliance on remote sensing-based Earth observation and image interpretation. In parallel, multimodal collaborative processing is attracting research attention. Synthetic aperture radar (SAR) and optical images offer complementary advantages but pose challenges for simultaneous use due to platform constraints and environmental conditions, often leaving only one modality available and impeding joint analysis. Generative models, particularly generative adversarial networks (GANs) and diffusion models (DMs), address this by learning cross-modal mappings. Translated images preserve structure and semantics while adopting target characteristics, thereby facilitating collaborative use. This review systematically categorizes translation frameworks spanning GANs, DMs, and other generative models. It then details downstream tasks supported by SAR–optical translation, including cloud removal, change detection, semantic segmentation, registration, and object detection, highlighting how translation bridges data gaps and enhances interpretation robustness. Furthermore, we provide open-source code and public datasets, discuss current challenges, and outline future research directions.&lt;/p&gt;</content:encoded></item><item><title>TransDiff: Unsupervised Non-line-of-sight Imaging with Aperture-limited Relay Surfaces</title><link>https://doi.org/10.1109/tip.2025.3637694</link><guid>10.1109/tip.2025.3637694</guid><pubDate>Thu, 04 Dec 2025 18:39:34 +0000</pubDate><dc:creator>Xingyu Cui</dc:creator><dc:creator>Huanjing Yue</dc:creator><dc:creator>Shida Sun</dc:creator><dc:creator>Yue Li</dc:creator><dc:creator>Yusen Hou</dc:creator><dc:creator>Zhiwei Xiong</dc:creator><dc:creator>Jingyu Yang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3637694</prism:doi><description>Non-line-of-sight (NLOS) imaging aims to reconstruct scenes hidden from direct view and has broad applications in robotic vision, rescue operations, autonomous driving, and remote sensing. However, most existing methods rely on densely sampled transients from large, continuous relay surfaces, which limits their practicality in real-world scenarios with aperture constraints. To address this limitation, we propose an unsupervised zero-shot framework tailored for confocal NLOS imaging with aperture-limited relay surfaces. Our method leverages latent diffusion models to recover fully-sampled transients from undersampled versions by enforcing measurement consistency during the sampling process. To further improve recovered transient quality, we introduce a progressive recovery strategy that incrementally recovers missing transient values, effectively mitigating the impact of severe aperture limitations. In addition, to suppress error propagation during recovery, we develop a backpropagation-based error correction reconstruction algorithm that refines intermediate recovered transients by enforcing sparsity regularization in the voxel domain, enabling high-fidelity final reconstructions. Extensive experiments on both simulated and real-world datasets validate the robustness and generalization capability of our method across diverse aperture-limited relay surfaces. Notably, our method follows a zero-shot paradigm, requiring only a single pretraining stage without paired data or pattern-specific retraining, making it a more practical and generalizable framework for NLOS imaging.
Published: 2025-12-04T18:39:34+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.769 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xingyu Cui; Huanjing Yue; Shida Sun; Yue Li; Yusen Hou; Zhiwei Xiong; Jingyu Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3637694"&gt;10.1109/tip.2025.3637694&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.769 (must_read)&lt;/p&gt;
&lt;p&gt;Non-line-of-sight (NLOS) imaging aims to reconstruct scenes hidden from direct view and has broad applications in robotic vision, rescue operations, autonomous driving, and remote sensing. However, most existing methods rely on densely sampled transients from large, continuous relay surfaces, which limits their practicality in real-world scenarios with aperture constraints. To address this limitation, we propose an unsupervised zero-shot framework tailored for confocal NLOS imaging with aperture-limited relay surfaces. Our method leverages latent diffusion models to recover fully-sampled transients from undersampled versions by enforcing measurement consistency during the sampling process. To further improve recovered transient quality, we introduce a progressive recovery strategy that incrementally recovers missing transient values, effectively mitigating the impact of severe aperture limitations. In addition, to suppress error propagation during recovery, we develop a backpropagation-based error correction reconstruction algorithm that refines intermediate recovered transients by enforcing sparsity regularization in the voxel domain, enabling high-fidelity final reconstructions. Extensive experiments on both simulated and real-world datasets validate the robustness and generalization capability of our method across diverse aperture-limited relay surfaces. Notably, our method follows a zero-shot paradigm, requiring only a single pretraining stage without paired data or pattern-specific retraining, making it a more practical and generalizable framework for NLOS imaging.&lt;/p&gt;</content:encoded></item><item><title>PCF-LLM: Scaling LLMs for Multimodal Understanding of Structured Scientific Data in Photonic Crystal Fiber Sensors</title><link>https://doi.org/10.1016/j.inffus.2025.104022</link><guid>10.1016/j.inffus.2025.104022</guid><pubDate>Thu, 04 Dec 2025 08:08:52 +0000</pubDate><dc:creator>Shengchao Chen</dc:creator><dc:creator>Geyao Hu</dc:creator><dc:creator>Sufen Ren</dc:creator><dc:creator>Ting Shu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104022</prism:doi><description>Photonic crystal fibers (PCFs) exhibit complex and highly tunable structure–property relationships, making them promising for diverse photonic applications but challenging for accurate modeling and inverse design. Traditional numerical solvers offer high fidelity but are computationally expensive, while existing learning-based approaches are typically limited to narrow, single-task objectives and generalize poorly to unseen structures. We define PCF understanding as the capability to jointly reason over numerical PCF geometry–property mappings and textual descriptions, enabling four core tasks: optical property prediction, inverse design suggestion, structural description generation, and property interpretation. To address these, we propose PCF-LLM, a scalable multimodal framework that adapts pretrained large language models (LLMs) for unified PCF understanding. PCF-LLM incorporates a cross-modality alignment mechanism to fuse structured PCF geometry and optical properties with language prompts, and employs parameter-efficient fine-tuning via Low-Rank Adaptation. To enable such modeling, we curate PCF-MM-170K, the first large-scale multimodal PCF dataset comprising 170,000 samples across four representative structures, each annotated with high-fidelity optical simulations and fine-grained textual descriptions. Extensive experiments across multiple LLMs demonstrate that PCF-LLM achieves high accuracy, strong physical consistency, and robust cross-task generalization, advancing the use of LLMs for scientific discovery in photonics.
Published: 2025-12-04T08:08:52+00:00
Venue: Information Fusion
Score: 0.767 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shengchao Chen; Geyao Hu; Sufen Ren; Ting Shu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104022"&gt;10.1016/j.inffus.2025.104022&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.767 (must_read)&lt;/p&gt;
&lt;p&gt;Photonic crystal fibers (PCFs) exhibit complex and highly tunable structure–property relationships, making them promising for diverse photonic applications but challenging for accurate modeling and inverse design. Traditional numerical solvers offer high fidelity but are computationally expensive, while existing learning-based approaches are typically limited to narrow, single-task objectives and generalize poorly to unseen structures. We define PCF understanding as the capability to jointly reason over numerical PCF geometry–property mappings and textual descriptions, enabling four core tasks: optical property prediction, inverse design suggestion, structural description generation, and property interpretation. To address these, we propose PCF-LLM, a scalable multimodal framework that adapts pretrained large language models (LLMs) for unified PCF understanding. PCF-LLM incorporates a cross-modality alignment mechanism to fuse structured PCF geometry and optical properties with language prompts, and employs parameter-efficient fine-tuning via Low-Rank Adaptation. To enable such modeling, we curate PCF-MM-170K, the first large-scale multimodal PCF dataset comprising 170,000 samples across four representative structures, each annotated with high-fidelity optical simulations and fine-grained textual descriptions. Extensive experiments across multiple LLMs demonstrate that PCF-LLM achieves high accuracy, strong physical consistency, and robust cross-task generalization, advancing the use of LLMs for scientific discovery in photonics.&lt;/p&gt;</content:encoded></item><item><title>GeoVideo: Introducing Geometric Regularization into Video Generation Model</title><link>https://arxiv.org/abs/2512.03453v1</link><guid>http://arxiv.org/abs/2512.03453v1</guid><pubDate>Wed, 03 Dec 2025 05:11:57 +0000</pubDate><dc:creator>Yunpeng Bai</dc:creator><dc:creator>Shaoheng Fang</dc:creator><dc:creator>Chaohui Yu</dc:creator><dc:creator>Fan Wang</dc:creator><dc:creator>Qixing Huang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advances in video generation have enabled the synthesis of high-quality and visually realistic clips using diffusion transformer models. However, most existing approaches operate purely in the 2D pixel space and lack explicit mechanisms for modeling 3D structures, often resulting in temporally inconsistent geometries, implausible motions, and structural artifacts. In this work, we introduce geometric regularization losses into video generation by augmenting latent diffusion models with per-frame depth prediction. We adopted depth as the geometric representation because of the great progress in depth prediction and its compatibility with image-based latent encoders. Specifically, to enforce structural consistency over time, we propose a multi-view geometric loss that aligns the predicted depth maps across frames within a shared 3D coordinate system. Our method bridges the gap between appearance generation and 3D structure modeling, leading to improved spatio-temporal coherence, shape consistency, and physical plausibility. Experiments across multiple datasets show that our approach produces significantly more stable and geometrically consistent results than existing baselines.
Published: 2025-12-03T05:11:57+00:00
Venue: arXiv
Score: 0.765 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yunpeng Bai; Shaoheng Fang; Chaohui Yu; Fan Wang; Qixing Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (must_read)&lt;/p&gt;
&lt;p&gt;Recent advances in video generation have enabled the synthesis of high-quality and visually realistic clips using diffusion transformer models. However, most existing approaches operate purely in the 2D pixel space and lack explicit mechanisms for modeling 3D structures, often resulting in temporally inconsistent geometries, implausible motions, and structural artifacts. In this work, we introduce geometric regularization losses into video generation by augmenting latent diffusion models with per-frame depth prediction. We adopted depth as the geometric representation because of the great progress in depth prediction and its compatibility with image-based latent encoders. Specifically, to enforce structural consistency over time, we propose a multi-view geometric loss that aligns the predicted depth maps across frames within a shared 3D coordinate system. Our method bridges the gap between appearance generation and 3D structure modeling, leading to improved spatio-temporal coherence, shape consistency, and physical plausibility. Experiments across multiple datasets show that our approach produces significantly more stable and geometrically consistent results than existing baselines.&lt;/p&gt;</content:encoded></item></channel></rss>