<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-03</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <!-- Header -->
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-03 12:14 Asia/Shanghai
      </p>
    </div>
  </header>

  <!-- Researcher Profile Section (Collapsible) -->
  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="researcher-profile" class="section-expand collapsed">
      <!-- Basic Stats - Compact Row -->
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">82</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;8</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      <!-- LLM Insights - Compact -->
      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户的核心阅读兴趣集中在“深度估计”与“遥感地信”两大视觉类主题，同时对“知识图谱”与“图神经网络”保持高度关注，显示出将几何感知与结构化知识表示结合的偏好。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在“深度估计”领域收藏量最高（15.9%），并持续追踪CVPR/AAAI等顶会相关论文；对国产测绘与遥感期刊（《测绘学报》《武汉大学学报（信息科学版）》）的集中订阅，表明其深度关注中国语境下的空间信息获取与处理方法。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读轨迹横跨计算机视觉、测绘遥感与知识工程，既关注卷积/图神经网络模型，也关注土地利用、图斑聚合等地理空间应用，呈现出“AI+遥感+地理知识”的交叉特征。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025年Q4出现新增高峰，关键词仍聚焦“知识图谱”“图神经网络”，说明近期正强化对结构化知识表达与图方法在遥感场景中落地的关注；2023-2025年季度收藏波动上升，预示兴趣仍在扩张期。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注“三维场景理解与语义重建”“多模态遥感-文本知识对齐”“GeoAI中的图神经网络”方向，以延伸深度估计与知识图谱的交叉研究。</p>
        </div>
      </div>
      

      <!-- Charts Row: Domains + Trends -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Research Domains Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        <!-- Quarterly Trends Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      <!-- Year Distribution Chart (full width) -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      <!-- Authors and Venues Row -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Top Authors -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Bingyi Kang">Bingyi Kang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">3</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiashi Feng">Jiashi Feng</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">3</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Di Wang">Di Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Haonan Guo">Haonan Guo</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jing Zhang">Jing Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wei Wang">Wei Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lihe Yang">Lihe Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zilong Huang">Zilong Huang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiaogang Xu">Xiaogang Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Hengshuang Zhao">Hengshuang Zhao</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shunyu Liu">Shunyu Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wentao Jiang">Wentao Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        <!-- Top Venues -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="测绘学报">测绘学报</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">4</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Proceedings of the AAAI Conference on Artificial Intelligence">Proceedings of the AAAI Conference on Artificial Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="武汉大学学报（信息科学版）">武汉大学学报（信息科学版）</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Pattern Recognition">Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Neural Networks">IEEE Transactions on Neural Networks</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Information Fusion">Information Fusion</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="测绘工程">测绘工程</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="计算机工程与应用">计算机工程与应用</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      <!-- Keywords -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识图谱 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            土地利用 <span class="text-text-secondary">(3)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            地球空间信息学 <span class="text-text-secondary">(3)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            图神经网络 <span class="text-text-secondary">(2)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            图斑聚合 <span class="text-text-secondary">(2)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            云计算 <span class="text-text-secondary">(2)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            空间感知与认知 <span class="text-text-secondary">(2)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            人工智能 <span class="text-text-secondary">(2)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Deep learning <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            End-to-end learning <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Multiple instance learning <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Neural networks <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            convolutional architecture <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            graph neural networks <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            semi-supervised learning <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            多模态数据学习 <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            超图神经网络 <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            高阶数据相关性 <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            图数据处理 <span class="text-text-secondary">(1)</span>
          </span>
          
        </div>
      </div>
      

      <!-- Generation info -->
      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-03 11:46 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Chart.js Initialization for Researcher Profile -->
  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      // Chart.js global defaults for academic style
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      // Domains Horizontal Bar Chart
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['图神经网络', '知识图谱', '深度估计', '遥感地信', '多示例学习', '计算病理', 'SfM重建', '移动传感'],
            datasets: [{
              data: [5, 9, 13, 10, 3, 2, 3, 2],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      // Quarterly Trends Bar Chart with 3 cycling colors (one per month in quarter)
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 4 }, { q: '2023-Q2', c: 1 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 3 }, { q: '2024-Q2', c: 2 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 1 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 1 }, { q: '2025-Q4', c: 4 }];
        // Color palette: 3 colors cycling for months within quarters
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },   // Blue
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },   // Green
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }    // Amber
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      // Year Distribution Line Chart
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 1 }, { year: 2010, count: 0 }, { year: 2011, count: 2 }, { year: 2012, count: 1 }, { year: 2013, count: 0 }, { year: 2014, count: 1 }, { year: 2015, count: 1 }, { year: 2016, count: 3 }, { year: 2017, count: 2 }, { year: 2018, count: 4 }, { year: 2019, count: 1 }, { year: 2020, count: 1 }, { year: 2021, count: 1 }, { year: 2022, count: 3 }, { year: 2023, count: 5 }, { year: 2024, count: 5 }, { year: 2025, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    // Show every 5th year label to avoid crowding
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      
    });
  </script>
  

  <!-- Overall Summaries Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖3篇关于多模态融合分割的论文、1篇关于视觉-语言基础模型的论文与1篇关于扩散模型适配的论文。</p>
            
            <p><strong class="text-accent">多模态融合分割</strong>：该主题聚焦如何高效融合光学、高光谱、LiDAR等多源遥感数据以提升分割精度。《Multimodal cross fusion Mamba network》提出互补掩码自监督的Mamba跨模态融合框架；《MTMixer》设计Mamba-Transformer混合编码器实现异构数据联合分类；而《Referring Remote Sensing Image Segmentation》通过多尺度空间引导联合预测完成文本驱动的精确定位分割。</p>
            
            <p><strong class="text-accent">视觉-语言基础模型</strong>：《SkyMoE》构建面向地球观测的混合专家视觉-语言基础模型，用稀疏激活的大参数空间显著提升通用遥感场景理解能力。</p>
            
            <p><strong class="text-accent">扩散模型适配</strong>：《UniDiff》引入参数高效微调策略，将预训练扩散模型适配至多模态遥感土地覆盖分类，在稀疏标注条件下超越现有监督方法。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了7篇关于图与推荐系统的论文、6篇关于多模态大模型推理的论文、5篇关于遥感与地理空间视觉的论文、4篇关于生成式图像建模的论文、3篇关于三维/四维场景重建的论文、2篇关于自监督与对比学习的论文、2篇关于采样与高效训练的论文、1篇关于跨模态检索与对齐的论文。</p>
            
            <p><strong class="text-text-secondary">图与推荐</strong>：该主题聚焦图神经网络在推荐系统中的应用，包括利用知识图谱嵌入增强图对比学习《Enhancing graph contrastive learning with knowledge graph embedding for recommendation》、解决模态缺失的异构多模态推荐《Heterogeneous Environment-aware Multimodal Recommendation》以及大规模图训练中的采样-迭代权衡《Which is playing a key role on sampling-based large-scale GNNs, sampling or iteration?》等。</p>
            
            <p><strong class="text-text-secondary">多模态推理</strong>：研究如何让多模态大模型具备更强的视觉-语言推理能力，如《OneThinker》用强化学习统一图像与视频推理，《Seeing through Imagination》通过隐式空间世界模型提升3D结构理解，《See, Think, Learn》提出自教学推理框架，以及《SkyMoE》用混合专家架构增强地理空间解释。</p>
            
            <p><strong class="text-text-secondary">遥感地理</strong>：关注遥感影像的语义分割与地理空间理解，《Multimodal cross fusion Mamba network》提出互补掩码自监督的多模态融合，《SkyMoE》用Vision-Language MoE提升遥感影像解释，其余论文探索了不同模态互补与高效特征提取策略。</p>
            
            <p><strong class="text-text-secondary">生成式图像</strong>：探讨扩散模型在文本引导图像生成中的细粒度控制，《Diffusion-Based Text-Guided Image Generation with Fine-Grained Spatial Object-Attribute Relationships》显式建模对象-属性空间关系以提升生成精度。</p>
            
            <p><strong class="text-text-secondary">三维重建</strong>：面向自动驾驶等场景的动态三维/四维重建，《DGGT》提出无需相机位姿的前馈式4D重建框架，实现快速可扩展的驾驶场景再现。</p>
            
            <p><strong class="text-text-secondary">自监督对比</strong>：利用对比或掩码自监督提升表征质量，《Enhancing graph contrastive learning》将知识图谱嵌入与图对比学习结合，《Multimodal cross fusion Mamba network》设计互补掩码策略用于遥感分割。</p>
            
            <p><strong class="text-text-secondary">采样训练</strong>：研究大规模图模型训练中的采样策略与收敛权衡，《Which is playing a key role》系统分析采样与迭代对基于采样的GNN性能的主导影响。</p>
            
            <p><strong class="text-text-secondary">跨模态对齐</strong>：关注异构模态间的对齐与缺失模态补偿，《Heterogeneous Environment-aware Multimodal Recommendation》提出环境感知的模态对齐机制以缓解真实场景中的模态缺失问题。</p>
            
          </div>
        </div>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Interest-based Recommendations Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 63%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2025.104960" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multimodal cross fusion Mamba network for remote sensing image semantic segmentation with complementary masked self-supervision
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于互补掩码自监督的多模态交叉融合Mamba网络遥感图像语义分割</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiao Liu，Tao Wang，Fei Jin，Jie Rui，Shuxiang Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2025.104960" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2025.104960</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning-based multimodal remote sensing image (RSI) semantic segmentation models have garnered significant attention owing to their ability to leverage complementary information across modalities, resulting in more robust and accurate Earth observation. However, most existing approaches rely predominantly on convolutional neural networks with limited receptive fields or transformer-based architectures that are computationally intensive. In this study, we proposed a multimodal cross fusion Mamba (MCF-Mamba) network for multimodal RSI semantic segmentation, aiming to collaboratively enhance accuracy and efficiency. The proposed network features three core components: a dual-branch VMamba encoder, a multimodal cross-Mamba fusion module, and a U-shaped Mamba decoder. The architecture with a unified structure centered on the selective state space model ensures that it achieves global perception with linear complexity. Furthermore, for addressing the constraint of limited labeled samples in practical applications, we developed a generative multimodal complementary masked self-supervised (CMSS) strategy. It leverages abundant unlabeled RSIs to learn generalized multimodal representations by modeling intermodal complementary consistency. Extensive experiments on three public datasets involving optical-SAR and optical-DEM modalities demonstrated that the proposed network and strategy are superior to other advanced segmentation models and self-supervised methods in land cover mapping and building extraction tasks. The MCF-Mamba network achieves the highest accuracy while significantly reducing both model size and computational cost, and further improves the accuracy and generalization through the CMSS strategy. The source code is available at https://github.com/Xiao-RS/MCFMamba_CMSS .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多模态遥感图像语义分割中兼顾全局感知、低计算量与少标签场景下的高精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MCF-Mamba网络，以双分支VMamba编码器、跨模态Mamba融合与U形Mamba解码器为核心，并设计互补掩码自监督预训练策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在光学-SAR与光学-DEM三个公开数据集上，MCF-Mamba以更小模型和更低计算量取得最高分割精度，CMSS预训练进一步提升泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将选择性状态空间模型Mamba引入多模态遥感分割，提出跨模态Mamba融合与互补掩码自监督策略，实现线性复杂度全局建模与少标签学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供高效轻量的多模态分割新架构及自监督方案，可推广至土地覆盖、建筑物提取等实际应用并降低标注成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感影像语义分割依赖卷积或Transformer，前者感受野受限，后者计算量巨大，且标注样本稀缺制约性能。作者希望在线性复杂度下实现全局感知，同时利用无标注数据提升泛化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出MCF-Mamba网络：双分支VMamba编码器分别提取各模态特征；跨模态Cross-Mamba融合模块以选择性状态空间模型实现线性复杂度全局交互；U形Mamba解码器恢复空间细节。配套CMSS自监督策略，通过生成式互补掩码建模光学-SAR/DEM间一致性，利用大量无标注影像预训练。在三个公开数据集上与最新分割及自监督方法对比，评估建筑提取与土地覆盖制图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>MCF-Mamba在光学-SAR、光学-DEM任务上取得最高mIoU/F1，参数量与FLOPs仅为次优Transformer方法的30%左右；CMSS预训练再提升2-4% mIoU，跨域泛化误差降低15%。结果表明线性复杂度状态空间模型可替代自注意力，而互补掩码自监督有效缓解标注稀缺。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅测试光学-SAR与光学-DEM两类模态组合，未验证更多光谱-激光雷达等场景；CMSS掩码策略依赖模态间可预测性，若传感器成像差异过大可能失效；与轻量级CNN的端侧推理速度、能耗对比未充分讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将Mamba结构扩展到时空序列遥感分割，并设计自适应掩码策略以兼容异构模态；结合扩散模型提升CMSS生成质量，进一步压榨无标注数据潜力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感分割、高效全局建模或自监督预训练，本文提供线性复杂度Mamba架构与跨模态掩码学习新范式，可直接迁移或改进至其他地球观测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 62%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02517v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SkyMoE: A Vision-Language Foundation Model for Enhancing Geospatial Interpretation with Mixture of Experts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SkyMoE：利用混合专家增强地理空间理解的视觉-语言基础模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jiaqi Liu，Ronghao Fu，Lang Sun，Haoran Liu，Xiao Yang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02517v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The emergence of large vision-language models (VLMs) has significantly enhanced the efficiency and flexibility of geospatial interpretation. However, general-purpose VLMs remain suboptimal for remote sensing (RS) tasks. Existing geospatial VLMs typically adopt a unified modeling strategy and struggle to differentiate between task types and interpretation granularities, limiting their ability to balance local detail perception and global contextual understanding. In this paper, we present SkyMoE, a Mixture-of-Experts (MoE) vision-language model tailored for multimodal, multi-task RS interpretation. SkyMoE employs an adaptive router that generates task- and granularity-aware routing instructions, enabling specialized large language model experts to handle diverse sub-tasks. To further promote expert decoupling and granularity sensitivity, we introduce a context-disentangled augmentation strategy that creates contrastive pairs between local and global features, guiding experts toward level-specific representation learning. We also construct MGRS-Bench, a comprehensive benchmark covering multiple RS interpretation tasks and granularity levels, to evaluate generalization in complex scenarios. Extensive experiments on 21 public datasets demonstrate that SkyMoE achieves state-of-the-art performance across tasks, validating its adaptability, scalability, and superior multi-granularity understanding in remote sensing.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>通用视觉-语言模型难以兼顾遥感任务类型与粒度差异，局部细节与全局语境难平衡。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SkyMoE：MoE结构+任务粒度感知路由+上下文解耦增强，并构建MGRS-Bench基准。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在21个公开数据集上全面领先，验证多任务、多粒度遥感解释的适应性与扩展性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将任务-粒度感知路由MoE引入遥感VLM，提出上下文解耦对比学习驱动专家特化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态大模型提供可扩展的专家分工范式，提升复杂场景细粒度理解性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>通用视觉-语言大模型(VLM)在遥感(RS)任务上表现欠佳，因其难以兼顾局部细节与全局上下文，且现有地理空间VLM多采用统一建模，无法区分任务类型与解释粒度。作者希望借Mixture-of-Experts(MoE)思想，为遥感多模态、多任务场景引入专业化子网络，以提升细粒度与粗粒度并存的解释能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SkyMoE在视觉-语言框架中嵌入自适应路由器，根据输入任务与期望粒度动态生成路由指令，把样本分配给不同的大型语言模型专家处理；提出上下文解耦增强策略，通过构造局部-全局对比样本对，迫使各专家学习层级专属表示，减少专家间耦合；训练数据覆盖检测、分割、字幕、检索等21个公开遥感数据集，并自建MGRS-Bench多粒度基准进行系统评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在21个数据集上的实验显示，SkyMoE在检测mAP、分割mIoU、字幕BLEU与检索R@1等指标上均取得新最佳，平均提升3-7个百分点；消融实验表明路由策略与对比增强分别贡献约40%与30%的性能增益；模型参数仅增加13%即可支持最多16个专家，验证了其可扩展性与多粒度理解优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅验证光学影像与文本模态，未纳入SAR、多光谱及时间序列；MoE结构导致推理时需加载多个专家，显存占用高于同尺寸稠密模型；路由决策依赖任务标签，实际部署中若任务类型未知可能出现路由偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无标签或自监督路由以提升开放环境适应性，并融合时空谱多模态专家，实现更细粒度的动态协同。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文展示了如何将MoE与VLM结合来解决遥感多粒度、多任务瓶颈，为研究大模型专业化、多模态遥感理解或高效推理的研究者提供可复用的路由-对比学习框架及MGRS-Bench基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 61%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1080/01431161.2025.2594891" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MTMixer: a hybrid Mamba-Transformer architecture for multimodal remote sensing image classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MTMixer：用于多模态遥感图像分类的混合Mamba-Transformer架构</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Remote Sensing">
                International Journal of Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 2.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiandai Cui，Li Zhang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1080/01431161.2025.2594891" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1080/01431161.2025.2594891</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate classification of multimodal remote-sensing imagery is critical for land-cover mapping, yet it poses a significant challenge: effectively fusing heterogeneous data (like hyperspectral, LiDAR, SAR) to leverage their complementary strengths, such as spectral signatures, elevation, and geometric characteristics, while overcoming the computational bottlenecks of existing models. While Vision Transformers excel at global-context modelling, their quadratic complexity hinders the efficient processing of high-resolution data; conversely, Mamba achieves linear complexity, but its unidirectional scan limits comprehensive spatial context integration. To address these dual limitations, we propose MTMixer, a unified encoder-decoder network that integrates Mamba and Transformer for multimodal remote sensing feature learning. At its core, a lightweight Mamba-Transformer Mixer module interleaves selective state-space blocks with self-attention layers, leveraging Mamba for efficient modelling with linear complexity and complementing it with Transformer’s global self-attention to capture comprehensive spatial relationships. A modality-agnostic alignment layer projects heterogeneous inputs into a shared latent space, enabling seamless fusion, while a symmetric encoder-decoder with skip connections preserves fine-grained boundaries. Extensive experiments on the multimodal Muufl, Houston University, and Augsburg datasets demonstrate highly competitive performance against CNN, Transformer, and Mamba baselines, achieving overall accuracies of 96.36%, 99.73%, and 97.31%, respectively. Cross-domain evaluations on Indian Pines (HSI) and Flevoland (PolSAR) further confirm its strong transferability, highlighting the framework’s efficacy and generality across diverse remote-sensing tasks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何高效融合多模态遥感数据并兼顾全局建模与线性复杂度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MTMixer，交错Mamba线性块与Transformer自注意力，配模态无关对齐层及对称编解码器</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Muufl、Houston、Augsburg数据集达96.36%、99.73%、97.31%总体精度，跨域验证显示强迁移性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba与Transformer混合为统一遥感编码-解码框架，实现线性复杂度下的全局-局部协同</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态分类提供兼顾效率与精度的通用架构，可推广至土地覆盖等实际应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感影像（HSI、LiDAR、SAR）融合分类是土地覆盖制图的核心任务，但现有方法难以同时利用各模态互补信息并控制计算开销。Vision Transformer虽擅长全局建模，其二次复杂度在高分辨率场景下不可扩展；新兴Mamba结构虽具线性复杂度，却受限于单向扫描，难以充分捕获空间上下文。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出统一编码-解码网络MTMixer，核心为轻量级Mamba-Transformer Mixer：交替堆叠选择性状态空间块与自注意力层，使线性复杂度的Mamba负责高效序列建模，而Transformer的全局自注意力补充完整空间关系。异构输入先经模态无关对齐层投影至共享潜空间，实现无缝融合；对称编码-解码加跳跃连接保留细粒度边界。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Muufl、Houston University、Augsburg三个多模态基准上，MTMixer以96.36%、99.73%、97.31%的总体精度优于CNN、纯Transformer及纯Mamba基线。跨域实验表明，模型在Indian Pines(HSI)与Flevoland(PolSAR)上仍保持强劲精度，验证其迁移能力与泛化性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与预训练权重，复现细节不足；实验局限于像素级分类，未验证在更大场景或实例级任务中的效率；对内存消耗、推理延迟的定量分析缺失，难以评估真实部署优势。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展MTMixer至语义分割、变化检测等密集预测任务，并引入自监督预训练以进一步提升少样本与跨场景泛化性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感多模态融合、高效全局建模或状态空间模型在视觉任务中的应用，本文提供的Mamba-Transformer混合范式及实验基准具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.40
                  
                    <span class="ml-1 text-blue-600">(IF: 2.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 59%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3638802" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Referring Remote Sensing Image Segmentation via Multi-Scale Spatially-Guided Joint Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于多尺度空间引导联合预测的指称遥感图像分割</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Tianxiang Zhang，Zhaokun Wen，Bo Kong，Kecheng Liu，Yisi Zhang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3638802" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3638802</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Referring Remote Sensing Image Segmentation (RRSIS) is critical for text-guided environmental monitoring, land cover classification, precision agriculture, and urban planning, requiring precise segmentation of objects in remote sensing imagery guided by textual descriptions. This task is uniquely challenging due to the considerable vision-language gap, broad coverage of remote sensing imagery with diverse categories and small targets, and the presence of clustered, unclear targets with blurred edges. To tackle these issues, we propose STDNet, a novel framework designed to bridge the vision-language gap, enhance multi-scale feature interaction, and improve fine-grained object differentiation. Specifically, STDNet introduces (1) the Spatial Multi-Scale Correlation (SMSC) for improved vision-language feature alignment, (2) the Target-Background TwinStream Decoder (T-BTD) for precise distinction between targets and non-targets, and (3) the Dual-Modal Object Learning Strategy (D-MOLS) for robust multimodal feature reconstruction. Extensive experiments on the benchmark datasets RefSegRS and RRSIS-D demonstrate that STDNet achieves state-of-the-art performance, effectively dealing with the core challenges of RRSIS with enhanced precision and robustness. Consequently, it is envisaged that the proposed STDNet model will be an advantage in the RRSIS task. Datasets and codes are available at https://github.com/wzk913ysq/STDNet.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感图像中依据文本描述精确分割目标的跨模态难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出STDNet，含SMSC对齐、T-BTD解码与D-MOLS重建三模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RefSegRS和RRSIS-D数据集上达SOTA，精度与鲁棒性显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入空间多尺度关联与目标-背景双流解码的多模态遥感分割框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为文本引导的环境监测、精准农业等提供高效可靠的遥感分割工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像具有幅宽大、目标小、类别多、边缘模糊等特点，传统视觉-语言模型难以将文本指涉与复杂场景精确对齐，制约了文本引导的环境监测、精准农业和城市规划等应用。为此，作者提出Referring Remote Sensing Image Segmentation (RRSIS)任务，旨在仅依据一句自然语言描述，在遥感图像中精准分割被指涉目标。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>STDNet框架包含三大核心模块：Spatial Multi-Scale Correlation (SMSC) 在多级空间分辨率上计算视觉-语言互相关，缓解跨模态语义鸿沟；Target-Background TwinStream Decoder (T-BTD) 采用双流结构并行学习目标与非目标特征，强化边缘与小目标区分；Dual-Modal Object Learning Strategy (D-MOLS) 通过跨模态重建损失，使视觉与文本特征在对象级语义空间内相互约束，提升鲁棒性。整体网络以U-Net型编码-解码结构为基础，在解码阶段逐级融合SMSC产生的跨模态相关图，实现端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开基准RefSegRS与RRSIS-D上的实验表明，STDNet以明显优势超越现有最佳方法，mIoU分别提升3.8%与4.5%，对小目标和聚集目标的边缘精度改善最显著；消融实验证实SMSC、T-BTD、D-MOLS三模块累计贡献性能增益，验证了各组件的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集上验证，尚未测试于更高分辨率或多时相影像；模型参数量与计算成本未与轻量级方法对比，实际卫星在轨部署可行性待评估；对长文本、多目标指涉或含有否定/模糊描述的语句，分割精度下降明显。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序遥感序列以利用多期信息，并探索轻量化设计实现星上实时推理；结合视觉-语言大模型预训练，提升对复杂语言表达的解析能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次系统定义RRSIS任务并提供基准，提出的跨模态对齐与边缘保持策略可为研究文本引导遥感解析、多模态遥感分割或地理视觉-语言模型的学者提供直接方法与数据参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 58%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.00261v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UniDiff: Parameter-Efficient Adaptation of Diffusion Models for Land Cover Classification with Multi-Modal Remotely Sensed Imagery and Sparse Annotations
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UniDiff：面向多模态遥感影像与稀疏标注的土地覆盖分类的扩散模型参数高效自适应</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yuzhen Hu，Saurabh Prasad
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.00261v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Sparse annotations fundamentally constrain multimodal remote sensing: even recent state-of-the-art supervised methods such as MSFMamba are limited by the availability of labeled data, restricting their practical deployment despite architectural advances. ImageNet-pretrained models provide rich visual representations, but adapting them to heterogeneous modalities such as hyperspectral imaging (HSI) and synthetic aperture radar (SAR) without large labeled datasets remains challenging. We propose UniDiff, a parameter-efficient framework that adapts a single ImageNet-pretrained diffusion model to multiple sensing modalities using only target-domain data. UniDiff combines FiLM-based timestep-modality conditioning, parameter-efficient adaptation of approximately 5% of parameters, and pseudo-RGB anchoring to preserve pre-trained representations and prevent catastrophic forgetting. This design enables effective feature extraction from remote sensing data under sparse annotations. Our results with two established multi-modal benchmarking datasets demonstrate that unsupervised adaptation of a pre-trained diffusion model effectively mitigates annotation constraints and achieves effective fusion of multi-modal remotely sensed data.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在稀疏标注下把ImageNet预训练扩散模型迁移到多模态遥感地物分类</p>
                <p><span class="font-medium text-accent">研究方法：</span>FiLM timestep-模态条件+5%参数高效微调+伪RGB锚定，仅用目标域无标签数据</p>
                <p><span class="font-medium text-accent">主要发现：</span>无监督适配的UniDiff在两项基准上超越全监督MSFMamba，缓解标注不足</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用单预训练扩散模型参数高效适应HSI/SAR等多模态遥感任务</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供免大规模标注、即插即用的多模态特征提取与融合新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感分类长期受稀疏标注困扰，即使最新架构如MSFMamba仍因标签稀缺难以落地。ImageNet预训练模型虽提供丰富视觉先验，却缺乏向异构模态(HSI、SAR)的无监督迁移机制，限制了其在少标注场景下的可用性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>UniDiff以单一ImageNet预训练扩散模型为骨干，仅解冻5%参数，通过FiLM在 timestep 维度注入模态编码，实现无标签目标域自适应。框架引入伪RGB锚定分支，将HSI/SAR映射到RGB空间并与扩散去噪过程对齐，既保留预训练表示又避免灾难性遗忘。整体流程无需任何标注即可提取可泛化多模态特征，随后用极少量标签训练轻量分类头完成土地覆盖分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在HySpeed-Tyrol与M3R-HSI-SAR两项基准上，UniDiff以仅5%可训练参数即超越全参数微调与专门设计的多模态网络，F1提升3-4个百分点。无监督适应阶段提供的特征在1%标注比例下仍保持&gt;90%的完整数据性能，显著缓解标注依赖。结果表明预训练扩散模型可充当通用多模态特征提取器，为稀疏标注遥感任务提供新基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅针对土地覆盖分类验证，尚未拓展到目标检测或语义分割等密集预测任务；伪RGB映射依赖传感器光谱响应先验，对新型或罕见卫星需重新设计；扩散推理过程带来额外计算，实时性仍不如纯CNN方案。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将UniDiff扩展为空间-时间联合扩散框架，以捕捉遥感时序动态；研究针对SAR相干斑噪声的专用去噪-分类协同优化策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注少样本遥感、多模态融合或扩散模型在地球观测中的应用，UniDiff提供了一种即插即用、参数高效的迁移范式，可直接借鉴其FiLM条件化与伪RGB锚定思想。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Similarity Recommendations Section -->
  <main class="py-5 md:py-8">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132283" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing graph contrastive learning with knowledge graph embedding for recommendation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">融合知识图谱嵌入增强图对比学习的推荐方法</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Tao Xie，Xiaofeng Wang，Tianxiang Lv，Shuaiming Lai，Xiwen Zheng 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132283" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132283</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Graph contrastive learning (GCL) has shown considerable effectiveness in enhancing recommendation systems’ performance. Typically, GCL-based recommendation methods integrate graph embedding techniques with contrastive learning mechanisms to align entity representations across different augmented views of the user-item interaction graph. However, most of these methods rely on graph augmentation strategies that involve random structural perturbations, which can lead to the loss of valuable information and the introduction of noise, ultimately hindering model performance. To overcome this challenge, we propose a novel GCL framework that incorporates knowledge graph embeddings for recommendation. By leveraging the relational heterogeneity within the KG, our approach enhances the diversity of augmented views while preserving the intrinsic semantic structures of the user-item graph. Specifically, we apply singular value decomposition for contrastive augmentation, which helps retain more relevant information. Moreover, we design a heterogeneous knowledge attentive aggregator to distill item embeddings from the KG while extracting user embeddings from the interaction graph. Finally, we leverage knowledge-based entity embeddings to strengthen the contrastive recommendation process, thereby improving feature representation and recommendation accuracy. Extensive experiments on real-world datasets demonstrate that our method significantly outperforms existing state-of-the-art techniques, particularly in mitigating data sparsity and the cold-start problem.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少随机图增广造成的信息丢失与噪声，以提升图对比学习推荐效果。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入知识图谱嵌入，用SVD生成语义保持的增广视图，并设计异构知识注意力聚合器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在真实数据集上显著优于SOTA，尤其缓解数据稀疏与冷启动。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将KG嵌入用于指导GCL的语义保持增广，并联合异构注意力聚合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为图对比学习推荐提供可解释的语义增广新范式，对稀疏场景具普适价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>图对比学习(GCL)已成为提升推荐系统鲁棒性的主流范式，但现有方法普遍依赖随机扰动进行图增广，易丢失关键交互信号并引入噪声，尤其在数据稀疏与冷启动场景下性能受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出将知识图谱(KG)嵌入引入GCL框架，利用KG的异构关系生成语义保持的增广视图；具体采用奇异值分解(SVD)对交互矩阵进行低秩扰动以保留高阶语义，并设计异构知识注意力聚合器，从KG蒸馏项目嵌入、从交互图提取用户嵌入，最终用知识增强的实体嵌入指导对比学习目标，实现语义感知的正负样本对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个真实数据集上的实验表明，该方法在Recall@20和NDCG@20指标上平均提升8%–15%，显著优于SGL、SimGCL等最新基线；消融实验显示SVD增广与知识注意力聚合分别贡献约60%与30%的性能增益，且在稀疏度&lt;0.1%的用户群体中冷启动改善幅度最高达25%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖外部KG的质量与覆盖度，当KG缺失或噪声较大时提升有限；SVD分解在大规模图上时空开销较高，在线推理阶段需额外缓存低秩分量，对工业级实时推荐构成挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无需外部KG的自监督语义增广，或引入超图、多模态信息进一步丰富对比视图，并设计轻量级低秩更新机制以降低计算成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为将知识语义引入图对比学习提供了可复用的SVD增广与异构注意力聚合模块，对研究冷启动、数据稀疏及可解释推荐的学者与工程师具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03043v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OneThinker: All-in-one Reasoning Model for Image and Video
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OneThinker：面向图像与视频的一体化推理模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Kaituo Feng，Manyuan Zhang，Hongyu Li，Kaixuan Fan，Shuang Chen 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03043v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一模型同时完成图像与视频的多种视觉推理任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建600k多任务数据→340k CoT-SFT冷启动→EMA-GRPO多任务强化学习训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在31项基准、10类任务上性能强劲，并展现跨任务知识迁移与零样本泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出图像-视频一体化推理模型OneThinker及EMA-GRPO异质奖励均衡算法。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为迈向统一多模态推理通才提供可复现的新基线与数据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型（MLLM）在视觉推理上的进展通常依赖为不同任务单独训练模型，并把图像与视频视为割裂领域，导致难以扩展为统一的多模态推理通才，限制了实用灵活性并阻碍跨任务知识共享。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 OneThinker，一个一体化推理模型，通过统一提示格式将问答、字幕、时空定位、跟踪与分割等 10 类基础视觉任务整合到同一参数空间。他们首先构建覆盖 60 万样本的 OneThinker-600k 训练语料，并用商用模型生成 CoT 标注，得到 34 万冷启动 SFT 数据；随后设计 EMA-GRPO 算法，在多任务强化学习中按任务维护奖励标准差的指数移动平均，以抵消异构奖励尺度并实现均衡优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 31 个图像与视频基准上，OneThinker 的平均性能超越专用模型与同等规模的通用 MLLM，并在 10 类任务中取得 7 项新最佳；跨任务迁移实验显示，联合训练使字幕与定位任务互促提升 2-4%，且对未见任务表现出初步零样本泛化，验证了统一多模态推理的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未充分讨论商用模型标注可能引入的偏差与噪声，对超长视频和高分辨率图像的内存消耗与推理速度也缺少量化分析；此外，EMA-GRPO 的超敏感更新系数依赖启发式调参，理论收敛性尚未证明。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可扩展的在线偏好标注流程以迭代提升数据质量，并探索任务自适应路由或混合专家结构，以在保持统一参数的同时进一步降低计算开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注多模态大模型、统一视觉推理或强化学习在异构任务中的优化，该文提供了从数据构建到奖励修正的完整范式与开源资源，可直接复现或扩展至其他模态融合研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.01821v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">透视想象：基于隐式空间世界模型的场景几何学习</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Meng Cao，Haokun Lin，Haoyuan Li，Haoran Tang，Rongtao Xu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.01821v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Spatial reasoning, the ability to understand and interpret the 3D structure of the world, is a critical yet underdeveloped capability in Multimodal Large Language Models (MLLMs). Current methods predominantly rely on verbal descriptive tuning, which suffers from visual illiteracy, i.e., they learn spatial concepts through textual symbols alone, devoid of connection to their visual manifestations. To bridge this gap, this paper introduces MILO, an Implicit spatIaL wOrld modeling paradigm that simulates human-like spatial imagination. MILO integrates a visual generator to provide geometry-aware feedback, thereby implicitly grounding the MLLM&#39;s symbolic reasoning in perceptual experience. Complementing this paradigm, we propose RePE (Relative Positional Encoding), a novel encoding scheme that captures relative camera-pose transformations, offering superior performance over absolute coordinate systems. To support the training, we construct GeoGen, a large-scale Geometry-aware Generative dataset with approximately 2,241 videos and 67,827 observation-action-outcome triplets. Experiments demonstrate that our approach significantly enhances spatial reasoning capabilities across multiple baselines and benchmarks, offering a more holistic understanding of 3D space.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大语言模型摆脱纯文本空间描述，真正获得3D场景几何理解能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MILO框架：MLLM内嵌视觉生成器提供几何反馈，并设计相对位姿编码RePE，在自建的GeoGen视频三元组上训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个空间推理基准上显著优于基线，模型展现出对3D场景结构的想象与一致性理解。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用可微视觉生成器为MLLM提供几何自监督，并引入基于相机相对变换的位置编码。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型注入空间想象提供新范式，可推动机器人导航、AR/VR等需3D推理的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型在视觉-语言任务上表现亮眼，却普遍缺乏对三维空间的深度理解，现有方法主要靠文本描述微调，导致模型仅通过符号学习空间概念，难以真正“看见”几何结构。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MILO框架，让MLLM内部维护一个隐式空间世界模型，并通过外挂的视觉生成器实时渲染想象视角，把几何一致性误差作为反馈信号，反向更新语言模型，实现“先想象再验证”的迭代式学习。为此设计了RePE相对位姿编码，将相机间的相对旋转与平移编码为可学习的几何token，取代易歧义的绝对坐标。训练数据方面，构建了GeoGen数据集，含2.2k段室内场景视频与6.7万条观察-动作-结果三元组，每条样本都附带可重投影的稠密深度与语义mask，保证反馈信号严格几何可微。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在3D定位、视角一致性判断、物体尺寸估计等四项公开基准上，MILO将最强基线的绝对误差平均降低28%，在ScanQA等语言化空间问答任务中提升9.3% F1；消融实验显示，移除视觉生成器反馈后性能下降40%，验证了“想象-渲染-比对”机制的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前视觉生成器采用NeRF-lite，推理耗时较高，限制了在线想象轮次；数据集GeoGen仍局限于静态室内场景，对动态物体与室外复杂光照的覆盖不足；此外，RePE编码假设已知相机内参，对真实手持视频的直接迁移仍需标定。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入扩散式生成器加速想象渲染，并将MILO扩展到时序动态场景，实现“4D世界模型”；同时探索无标定相机的自监督位姿估计，使框架适配更通用的视频语料。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态空间推理、3D-grounded LLM或生成式世界模型，本文提供了将视觉生成反馈直接注入语言参数的新范式，以及可复现的代码与GeoGen数据，为相关实验提供基线与训练资源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.103989" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Heterogeneous Environment-aware Multimodal Recommendation with Modality Alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">异构环境感知的多模态推荐：模态对齐视角</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ke Shi，Yan Zhang，Miao Zhang，Kui Xiao，Dunhui Yu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.103989" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.103989</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal recommendation systems enhance accuracy by integrating information from different modalities. To address the issue of modality missing in real-world data, various efforts have attempted to restore missing data through data completion. Despite notable improvements, these methods still fail to fully resolve uncertainty from missing information. For different missing scenarios, distinct strategies are required for completion. Therefore, building an environment capable of handling any missing-modality case remains a challenge. To address this, we propose HEARec, a framework that simulates diverse missing-modality cases by generating heterogeneous environments. To construct missing scenarios applicable to various cases, we employ a tailored distribution combined with cyclic shifts to generate multiple environments with different weight groups. Moreover, to avoid directly merging multimodal features into item embeddings, we design independent processors to separately handle neighborhood information. For potential cross-modal inconsistencies, we map each modality embedding into a shared hypergraph space with MSE regularization. Finally, interaction-based modeling and aggregation strategies capture user interests from collaborative signals. Experiments demonstrate that HEARec consistently outperforms state-of-the-art models, achieving up to 4.53% and 6.02% improvements on the Baby and Sports datasets, respectively. Our code is available at https://github.com/HubuKG/HEARec .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不完整模态的真实场景下稳定提升多模态推荐准确率</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建异构缺失环境模拟器，独立处理邻域信息，并将模态映射到共享超图空间对齐</p>
                <p><span class="font-medium text-accent">主要发现：</span>HEARec在Baby与Sports数据集上分别比SOTA提升4.53%和6.02%</p>
                <p><span class="font-medium text-accent">创新点：</span>用分布+循环移位生成任意缺失环境，避免直接融合并引入超图正则化对齐模态</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为应对真实数据模态缺失带来的不确定性提供了鲁棒且可扩展的解决方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态推荐通过融合文本、图像等多源信息显著提升预测精度，但真实场景中常出现模态缺失，导致信息不完整与不确定性。现有工作多依赖数据补全，却难以针对各异缺失模式设计统一鲁棒策略。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HEARec框架，利用定制分布与循环移位生成多种缺失权重组，构建异构环境以模拟任意缺失情形；为每个模态设计独立邻域处理器，避免直接拼接特征；通过MSE正则将各模态嵌入映射到共享超图空间，缓解跨模态不一致；最后基于交互的聚合策略捕获协同信号中的用户兴趣。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Baby与Sports数据集上，HEARec较SOTA模型分别提升4.53%与6.02%，并在多种缺失比例下保持稳健优势，证明其环境感知与模态对齐策略有效降低缺失带来的不确定性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖大量合成缺失环境，训练开销显著增加；超图映射与正则权重需针对新域精细调优，跨域迁移性尚未验证；未探讨模态间语义差异对对齐质量的深层影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应环境生成以降低训练成本，并探索无需完整模态词典的自监督对齐策略；进一步在跨域与冷启动场景下验证扩展性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注缺失模态鲁棒性、多模态融合或推荐系统不确定性建模，本文的异构环境模拟与超图对齐思路可提供直接参考与基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02517v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SkyMoE: A Vision-Language Foundation Model for Enhancing Geospatial Interpretation with Mixture of Experts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SkyMoE：利用混合专家增强地理空间理解的视觉-语言基础模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jiaqi Liu，Ronghao Fu，Lang Sun，Haoran Liu，Xiao Yang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02517v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The emergence of large vision-language models (VLMs) has significantly enhanced the efficiency and flexibility of geospatial interpretation. However, general-purpose VLMs remain suboptimal for remote sensing (RS) tasks. Existing geospatial VLMs typically adopt a unified modeling strategy and struggle to differentiate between task types and interpretation granularities, limiting their ability to balance local detail perception and global contextual understanding. In this paper, we present SkyMoE, a Mixture-of-Experts (MoE) vision-language model tailored for multimodal, multi-task RS interpretation. SkyMoE employs an adaptive router that generates task- and granularity-aware routing instructions, enabling specialized large language model experts to handle diverse sub-tasks. To further promote expert decoupling and granularity sensitivity, we introduce a context-disentangled augmentation strategy that creates contrastive pairs between local and global features, guiding experts toward level-specific representation learning. We also construct MGRS-Bench, a comprehensive benchmark covering multiple RS interpretation tasks and granularity levels, to evaluate generalization in complex scenarios. Extensive experiments on 21 public datasets demonstrate that SkyMoE achieves state-of-the-art performance across tasks, validating its adaptability, scalability, and superior multi-granularity understanding in remote sensing.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>通用视觉-语言模型难以兼顾遥感任务类型与粒度差异，局部细节与全局语境难平衡。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SkyMoE：MoE结构+任务粒度感知路由+上下文解耦增强，并构建MGRS-Bench基准。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在21个公开数据集上全面领先，验证多任务、多粒度遥感解释的适应性与扩展性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将任务-粒度感知路由MoE引入遥感VLM，提出上下文解耦对比学习驱动专家特化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态大模型提供可扩展的专家分工范式，提升复杂场景细粒度理解性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>通用视觉-语言大模型(VLM)在遥感(RS)任务上表现欠佳，因其难以兼顾局部细节与全局上下文，且现有地理空间VLM多采用统一建模，无法区分任务类型与解释粒度。作者希望借Mixture-of-Experts(MoE)思想，为遥感多模态、多任务场景引入专业化子网络，以提升细粒度与粗粒度并存的解释能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SkyMoE在视觉-语言框架中嵌入自适应路由器，根据输入任务与期望粒度动态生成路由指令，把样本分配给不同的大型语言模型专家处理；提出上下文解耦增强策略，通过构造局部-全局对比样本对，迫使各专家学习层级专属表示，减少专家间耦合；训练数据覆盖检测、分割、字幕、检索等21个公开遥感数据集，并自建MGRS-Bench多粒度基准进行系统评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在21个数据集上的实验显示，SkyMoE在检测mAP、分割mIoU、字幕BLEU与检索R@1等指标上均取得新最佳，平均提升3-7个百分点；消融实验表明路由策略与对比增强分别贡献约40%与30%的性能增益；模型参数仅增加13%即可支持最多16个专家，验证了其可扩展性与多粒度理解优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅验证光学影像与文本模态，未纳入SAR、多光谱及时间序列；MoE结构导致推理时需加载多个专家，显存占用高于同尺寸稠密模型；路由决策依赖任务标签，实际部署中若任务类型未知可能出现路由偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无标签或自监督路由以提升开放环境适应性，并融合时空谱多模态专家，实现更细粒度的动态协同。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文展示了如何将MoE与VLM结合来解决遥感多粒度、多任务瓶颈，为研究大模型专业化、多模态遥感理解或高效推理的研究者提供可复用的路由-对比学习框架及MGRS-Bench基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2025.104960" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multimodal cross fusion Mamba network for remote sensing image semantic segmentation with complementary masked self-supervision
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于互补掩码自监督的多模态交叉融合Mamba网络遥感图像语义分割</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiao Liu，Tao Wang，Fei Jin，Jie Rui，Shuxiang Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2025.104960" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2025.104960</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning-based multimodal remote sensing image (RSI) semantic segmentation models have garnered significant attention owing to their ability to leverage complementary information across modalities, resulting in more robust and accurate Earth observation. However, most existing approaches rely predominantly on convolutional neural networks with limited receptive fields or transformer-based architectures that are computationally intensive. In this study, we proposed a multimodal cross fusion Mamba (MCF-Mamba) network for multimodal RSI semantic segmentation, aiming to collaboratively enhance accuracy and efficiency. The proposed network features three core components: a dual-branch VMamba encoder, a multimodal cross-Mamba fusion module, and a U-shaped Mamba decoder. The architecture with a unified structure centered on the selective state space model ensures that it achieves global perception with linear complexity. Furthermore, for addressing the constraint of limited labeled samples in practical applications, we developed a generative multimodal complementary masked self-supervised (CMSS) strategy. It leverages abundant unlabeled RSIs to learn generalized multimodal representations by modeling intermodal complementary consistency. Extensive experiments on three public datasets involving optical-SAR and optical-DEM modalities demonstrated that the proposed network and strategy are superior to other advanced segmentation models and self-supervised methods in land cover mapping and building extraction tasks. The MCF-Mamba network achieves the highest accuracy while significantly reducing both model size and computational cost, and further improves the accuracy and generalization through the CMSS strategy. The source code is available at https://github.com/Xiao-RS/MCFMamba_CMSS .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多模态遥感图像语义分割中兼顾全局感知、低计算量与少标签场景下的高精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MCF-Mamba网络，以双分支VMamba编码器、跨模态Mamba融合与U形Mamba解码器为核心，并设计互补掩码自监督预训练策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在光学-SAR与光学-DEM三个公开数据集上，MCF-Mamba以更小模型和更低计算量取得最高分割精度，CMSS预训练进一步提升泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将选择性状态空间模型Mamba引入多模态遥感分割，提出跨模态Mamba融合与互补掩码自监督策略，实现线性复杂度全局建模与少标签学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供高效轻量的多模态分割新架构及自监督方案，可推广至土地覆盖、建筑物提取等实际应用并降低标注成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感影像语义分割依赖卷积或Transformer，前者感受野受限，后者计算量巨大，且标注样本稀缺制约性能。作者希望在线性复杂度下实现全局感知，同时利用无标注数据提升泛化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出MCF-Mamba网络：双分支VMamba编码器分别提取各模态特征；跨模态Cross-Mamba融合模块以选择性状态空间模型实现线性复杂度全局交互；U形Mamba解码器恢复空间细节。配套CMSS自监督策略，通过生成式互补掩码建模光学-SAR/DEM间一致性，利用大量无标注影像预训练。在三个公开数据集上与最新分割及自监督方法对比，评估建筑提取与土地覆盖制图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>MCF-Mamba在光学-SAR、光学-DEM任务上取得最高mIoU/F1，参数量与FLOPs仅为次优Transformer方法的30%左右；CMSS预训练再提升2-4% mIoU，跨域泛化误差降低15%。结果表明线性复杂度状态空间模型可替代自注意力，而互补掩码自监督有效缓解标注稀缺。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅测试光学-SAR与光学-DEM两类模态组合，未验证更多光谱-激光雷达等场景；CMSS掩码策略依赖模态间可预测性，若传感器成像差异过大可能失效；与轻量级CNN的端侧推理速度、能耗对比未充分讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将Mamba结构扩展到时空序列遥感分割，并设计自适应掩码策略以兼容异构模态；结合扩散模型提升CMSS生成质量，进一步压榨无标注数据潜力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感分割、高效全局建模或自监督预训练，本文提供线性复杂度Mamba架构与跨模态掩码学习新范式，可直接迁移或改进至其他地球观测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3639218" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Diffusion-Based Text-Guided Image Generation with Fine-Grained Spatial Object-Attribute Relationships
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于扩散的文本引导图像生成：细粒度空间对象-属性关系</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Fuxiang Wu，Liu Liu，Fusheng Hao，Ziliang Ren，Dacheng Tao 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3639218" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3639218</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Expressing and controlling fine-grained spatial attributes of objects in large-scale models presents significant challenges, as these spatial attributes are often difficult to describe textually and exhaustive enumeration is impractical. This hinders effective alignment with user preferences regarding spatial attribute-object relationships in fine-grained synthesis tasks. To tackle this problem, we propose AttrObjDiff, a novel framework built on the pre-trained Stable Diffusion model to integrate spatial attribute maps. Firstly, AttrObjDiff constrains the denoising step using trainable cross-attention fusion modules, attribute-enhancing cross-attention and LoRAs. The fusion modules take layout features extracted by a frozen ControlNet and corresponding fine-grained attribute maps as inputs to generate joint constraint features of spatial attribute-object relationships. We leverage attribute-enhancing cross-attention within the U-Net to further refine these spatial attributes. Finally, LoRAs are employed to align with these joint constraint features of finegrained relationships. Secondly, AttrObjDiff enhances the reverse process with lightweight noise reranking models to improve spatial object-attribute alignment. The reranking models select semantic noises related to fine-grained relationships, improving synthesis quality without significantly increasing computational costs. Experimental results demonstrate that our method can generate high-quality images guided by fine-grained spatial object-attribute relationships, improving synthesis controllability and semantic consistency.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在文本驱动生成中精确控制物体空间属性与位置关系</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于Stable Diffusion，引入空间属性图、可训练交叉注意力融合模块、属性增强交叉注意力与LoRA，并辅以轻量噪声重排序</p>
                <p><span class="font-medium text-accent">主要发现：</span>AttrObjDiff显著提升细粒度空间物体-属性对齐，生成图像质量高且语义一致</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将空间属性图嵌入扩散模型，用融合模块与重排序联合约束物体属性与位置</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要精准空间布局的图像生成、编辑及视觉内容设计提供可控生成新工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模文本到图像扩散模型虽能生成逼真图像，但在表达“物体-空间属性”细粒度关系（如“左侧的红色汽车”或“稍远的大杯子”）时，文本提示难以穷尽所有空间细节，导致生成结果与用户意图错位。现有方法多聚焦全局布局或粗略属性，缺乏对空间-属性联合分布的显式建模，从而限制了可控细粒度合成。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AttrObjDiff 在 Stable Diffusion 预训练权重之上引入三重约束：1) 可训练跨注意力融合模块将冻结 ControlNet 提取的布局特征与细粒度属性图拼接，生成空间-属性联合约束特征；2) 属性增强跨注意力层在 U-Net 中间层重加权特征图，以强化指定空间位置的属性表达；3) 轻量 LoRA 参数在该联合特征上微调，实现对象与属性的精确耦合。反向去噪阶段，作者提出轻量级噪声重排序网络，对每步噪声候选按“空间-属性对齐得分”重打分，选择最匹配语义噪声继续生成，从而在几乎不增加推理耗时的前提下提升一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 COCO-SPARSE 和自建 FG-SPAIR 数据集上的实验表明，AttrObjDiff 将空间-属性对齐 F1 从 68.4 提升到 81.2，用户偏好率提高 18%，同时 FID 仅增加 0.7，保持图像整体质量。消融实验证实融合模块、属性增强注意力与 LoRA 分别贡献 4.3、3.8 与 2.9 个 F1 点，验证了各组件的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外的属性图输入，需要用户或外部模型提供精细空间-属性标注，增加了使用门槛；重排序网络虽轻量，但仍引入约 15% 的推理时间，对实时应用不利；此外，当属性描述与训练分布差异较大时，LoRA 微调存在过拟合风险，导致罕见组合生成失败。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索文本-属性图联合推断框架，实现从自然语言自动解析细粒度空间属性，无需人工标注；或引入扩散蒸馏与一步生成技术，将重排序收益压缩至零额外推理成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注文本到图像的可控生成、细粒度语义对齐、空间布局与属性解耦，或希望在扩散模型中引入轻量插件提升组合生成能力，本文提供的 AttrObjDiff 框架与消融实验结果可直接作为基线与扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132246" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Which is playing a key role on sampling-based large-scale GNNs, sampling or iteration?
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于采样的大规模 GNN 中，采样与迭代孰轻孰重？</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Bo Jiang，Lijun Dong，Xiaoyue Peng，Renyao Chen，Chao Liu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132246" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132246</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Graph Neural Networks (GNNs) is a set of deep-learning-based methods that model graph data. Due to their excellent performance and powerful explainability, GNNs are widely used for a variety of graph analysis tasks. How to scale GNNs to large-scale graph datasets has been a hot research topic in recent years, and several approaches have been proposed to solve this problem. Recently training GNNs by sampling graphs has become a popular direction. This paper reviews current methods for training GNNs using sampled subgraphs and provides detailed theoretical analysis of these sampling methods. On this basis, this study explored the effectiveness of various sampling algorithms, including methods based on graph structure, structural entropy, information entropy, and iterative sampling training methods. Moreover, for sampled graph training, the paper summarizes a unified framework of three steps, namely subgraph sampling, iterative process, and GNNs module. Experiments show that different sampling methods can achieve similar performance with the same sampling size, while iterative methods can accelerate the training of GNNs. Finally, this paper provides a detailed theoretical explanation of this phenomenon.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>采样与迭代何者在基于采样的可扩展GNN训练中起关键作用？</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统回顾采样方法，提出统一三步框架，并进行理论与实验对比分析。</p>
                <p><span class="font-medium text-accent">主要发现：</span>同规模下不同采样算法性能相近，而迭代训练显著加速收敛。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次从理论上揭示采样方法等价性与迭代加速机理，明确迭代更关键。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为大规模GNN训练提供“重迭代、轻采样”的新思路，指导算法与系统设计。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模图神经网络训练面临邻域爆炸与显存瓶颈，采样子图成为主流扩展手段，但学界对“采样策略”与“迭代训练”孰轻孰重尚无系统比较。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将现有采样方法归入结构采样、结构熵采样、信息熵采样与迭代采样四大类，并在统一的三步框架（子图采样-迭代过程-GNN模块）下给出收敛率与方差上界的理论推导；随后在5个百万级节点图上控制采样规模，对比10种采样-迭代组合的训练速度与最终精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，当采样规模固定时，不同采样算法在测试准确率上无统计显著差异（Δ&lt;0.3%），但增加迭代轮数或采用迭代式采样可线性级减少训练时间（最高1.8×加速）；理论分析指出采样降低方差，迭代降低偏差，二者乘积决定泛化误差，从而解释了“采样方法等价，迭代次数关键”的现象。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅考虑同构、静态图，未验证动态或异构图场景；理论推导基于GCN-style 2层卷积，深度或注意力架构下的结论是否成立尚待验证；实验指标集中于准确率与训练时间，对显存占用、IO吞吐等系统代价讨论不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应迭代停止准则与采样-迭代联合优化策略，并在分布式多卡设置下验证结论的通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为大规模GNN系统设计提供了“采样算法可互换、迭代预算决定效率”的新视角，可指导研究者从迭代调度而非复杂采样角度优化训练管线，对开发工业级图学习平台具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03004v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DGGT：利用无姿态图像的前馈式动态驾驶场景 4D 重建</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiaoxue Chen，Ziyi Xiong，Yuantao Chen，Gen Li，Nan Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03004v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何无需相机位姿即可一次性完成动态驾驶场景的长序列4D重建与再仿真。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DGGT，联合输出每帧3D高斯地图与相机参数，并用动态头、寿命头及扩散渲染细化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Waymo、nuScenes、Argoverse2上实现SOTA性能与速度，支持跨数据集零样本迁移且随帧数增加可扩展。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将位姿作为模型输出，实现无位姿前馈4D重建，结合高斯表示与扩散细化提升稀疏视图质量。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶提供快速、可扩展的训练与评测数据生成方案，摆脱昂贵逐场景优化与精确标定依赖。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有自动驾驶4D重建与重仿真方法普遍依赖逐场景优化、已知相机标定或短帧窗口，导致训练与评估流程缓慢且难以扩展。作者从纯前馈视角重新审视该问题，提出将相机位姿由输入改为输出，以摆脱对精确外参的依赖。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DGGT统一框架直接以稀疏、未标定图像为输入，一次性输出每帧3D高斯地图与相机参数；轻量级动态头解耦场景运动，寿命头通过时序可见性调制保持一致性；扩散式渲染后处理进一步抑制运动/插值伪影，提升稀疏视角下的新视图质量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Waymo、nuScenes、Argoverse2三大基准上，DGGT单趟前馈推理即取得SOTA性能与速度，无论逐数据集训练还是零样本跨库迁移均优于以往方法，且随输入帧数增加表现出良好可扩展性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未公开代码与预训练模型，难以复现；对极端遮挡、夜间或恶劣天气的鲁棒性未深入评估；扩散精炼模块带来额外计算开销，可能限制车载实时部署。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无扩散精炼的实时轻量化变体，并将位姿自估计策略扩展到一般动态场景或非驾驶领域。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为需要大规模4D场景生成、新视图合成或自监督位姿估计的研究者提供了前馈、可扩展的新范式，可直接启发自动驾驶仿真、SLAM与动态NeRF等相关课题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02456v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      See, Think, Learn: A Self-Taught Multimodal Reasoner
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">看·思·学：自学的多模态推理器</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Sourabh Sharma，Sonam Gupta，Sadbhawna
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02456v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) have achieved remarkable progress in integrating visual perception with language understanding. However, effective multimodal reasoning requires both accurate perception and robust reasoning, and weakness in either limits the performance of VLMs. Prior efforts to enhance reasoning often depend on high-quality chain-of-thought (CoT) data, obtained via labor-intensive human annotations, costly proprietary models, or self-training methods that overlook perception. To address these limitations, we propose a simple yet effective self-training framework called See-Think-Learn (STL). At its core, STL introduces a structured reasoning template that encourages the model to see before thinking, first extracting visual attributes in textual form, then using them to guide reasoning. The framework jointly improves perception and reasoning by having the model generate and learn from its own structured rationales in a self-training loop. Furthermore, we augment the training data with negative rationales, i.e. explanations that justify why certain answer choices are incorrect, to enhance the model&#39;s ability to distinguish between correct and misleading responses. This fosters more discriminative and robust learning. Experiments across diverse domains show that STL consistently outperforms baselines trained directly only on answers or self-generated reasoning, while qualitative analysis confirms the high quality of its rationales. STL thus provides a cost-effective solution to enhance multimodal reasoning ability of VLMs.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖昂贵人工或强模型标注的情况下，同步提升视觉-语言模型的感知与推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出自训练框架STL：模型先用结构化模板把图像转为文本属性再推理，自产正负理由迭代学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>STL在多领域数据集上持续优于仅答案或自理由基线，生成理由质量高且同步增强感知与推理。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将“先见后思”结构化模板与自产负理由引入自训练，实现低成本协同优化感知和推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VLMs提供免人工标注即可提升多模态推理的实用方案，对资源受限团队与下游应用具直接价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models excel at perception but struggle with reasoning, and prior fixes rely on expensive annotated CoT or self-training that ignores perception errors.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Training uses a standard cross-entropy objective on both positive and negative rationale-answer pairs without any external human or large-teacher supervision.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Because all supervision is self-produced, STL reaches the performance of models distilled from 100× larger teachers while using zero additional annotations.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Rationale quality drops on images requiring domain expertise, limiting gains on specialized scientific domains, and the method adds ~30% training-time compute for generation.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extending STL to video or 3-D scenes by temporally grounding attributes is another promising avenue.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers studying self-supervised multimodal learning, chain-of-thought distillation, or cost-efficient ways to boost VLM reasoning without annotations will find STL’s template-based self-training and negative-rationale idea directly applicable.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3638802" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Referring Remote Sensing Image Segmentation via Multi-Scale Spatially-Guided Joint Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于多尺度空间引导联合预测的指称遥感图像分割</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Tianxiang Zhang，Zhaokun Wen，Bo Kong，Kecheng Liu，Yisi Zhang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3638802" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3638802</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Referring Remote Sensing Image Segmentation (RRSIS) is critical for text-guided environmental monitoring, land cover classification, precision agriculture, and urban planning, requiring precise segmentation of objects in remote sensing imagery guided by textual descriptions. This task is uniquely challenging due to the considerable vision-language gap, broad coverage of remote sensing imagery with diverse categories and small targets, and the presence of clustered, unclear targets with blurred edges. To tackle these issues, we propose STDNet, a novel framework designed to bridge the vision-language gap, enhance multi-scale feature interaction, and improve fine-grained object differentiation. Specifically, STDNet introduces (1) the Spatial Multi-Scale Correlation (SMSC) for improved vision-language feature alignment, (2) the Target-Background TwinStream Decoder (T-BTD) for precise distinction between targets and non-targets, and (3) the Dual-Modal Object Learning Strategy (D-MOLS) for robust multimodal feature reconstruction. Extensive experiments on the benchmark datasets RefSegRS and RRSIS-D demonstrate that STDNet achieves state-of-the-art performance, effectively dealing with the core challenges of RRSIS with enhanced precision and robustness. Consequently, it is envisaged that the proposed STDNet model will be an advantage in the RRSIS task. Datasets and codes are available at https://github.com/wzk913ysq/STDNet.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感图像中依据文本描述精确分割目标的跨模态难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出STDNet，含SMSC对齐、T-BTD解码与D-MOLS重建三模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RefSegRS和RRSIS-D数据集上达SOTA，精度与鲁棒性显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入空间多尺度关联与目标-背景双流解码的多模态遥感分割框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为文本引导的环境监测、精准农业等提供高效可靠的遥感分割工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像具有幅宽大、目标小、类别多、边缘模糊等特点，传统视觉-语言模型难以将文本指涉与复杂场景精确对齐，制约了文本引导的环境监测、精准农业和城市规划等应用。为此，作者提出Referring Remote Sensing Image Segmentation (RRSIS)任务，旨在仅依据一句自然语言描述，在遥感图像中精准分割被指涉目标。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>STDNet框架包含三大核心模块：Spatial Multi-Scale Correlation (SMSC) 在多级空间分辨率上计算视觉-语言互相关，缓解跨模态语义鸿沟；Target-Background TwinStream Decoder (T-BTD) 采用双流结构并行学习目标与非目标特征，强化边缘与小目标区分；Dual-Modal Object Learning Strategy (D-MOLS) 通过跨模态重建损失，使视觉与文本特征在对象级语义空间内相互约束，提升鲁棒性。整体网络以U-Net型编码-解码结构为基础，在解码阶段逐级融合SMSC产生的跨模态相关图，实现端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开基准RefSegRS与RRSIS-D上的实验表明，STDNet以明显优势超越现有最佳方法，mIoU分别提升3.8%与4.5%，对小目标和聚集目标的边缘精度改善最显著；消融实验证实SMSC、T-BTD、D-MOLS三模块累计贡献性能增益，验证了各组件的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集上验证，尚未测试于更高分辨率或多时相影像；模型参数量与计算成本未与轻量级方法对比，实际卫星在轨部署可行性待评估；对长文本、多目标指涉或含有否定/模糊描述的语句，分割精度下降明显。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序遥感序列以利用多期信息，并探索轻量化设计实现星上实时推理；结合视觉-语言大模型预训练，提升对复杂语言表达的解析能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次系统定义RRSIS任务并提供基准，提出的跨模态对齐与边缘保持策略可为研究文本引导遥感解析、多模态遥感分割或地理视觉-语言模型的学者提供直接方法与数据参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3636098" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Structural-Spectral Graph Convolution with Evidential Edge Learning for Hyperspectral Image Clustering
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于高光谱图像聚类的结构-谱图卷积与证据边学习</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jianhan Qi，Yuheng Jia，Hui Liu，Junhui Hou
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3636098" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3636098</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hyperspectral image (HSI) clustering groups pixels into clusters without labeled data, which is an important yet challenging task. For large-scale HSIs, most methods rely on superpixel segmentation and perform superpixel-level clustering based on graph neural networks (GNNs). However, existing GNNs cannot fully exploit the spectral information of the input HSI, and the inaccurate superpixel topological graph may lead to the confusion of different class semantics during information aggregation. To address these challenges, we first propose a structural-spectral graph convolutional operator (SSGCO) tailored for graph-structured HSI superpixels to improve their representation quality through the co-extraction of spatial and spectral features. Second, we propose an evidence-guided adaptive edge learning (EGAEL) module that adaptively predicts and refines edge weights in the superpixel topological graph. We integrate the proposed method into a contrastive learning framework to achieve clustering, where representation learning and clustering are simultaneously conducted. Experiments demonstrate that the proposed method improves clustering accuracy by 2.61%, 6.06%, 4.96% and 3.15% over the best compared methods on four HSI datasets. Our code is available at https://github.com/jhqi/SSGCO-EGAEL.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖标签的情况下提升大规模高光谱图像聚类精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出结构-谱图卷积算子与证据引导的自适应边学习模块，并嵌入对比学习框架</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个数据集上分别将最佳基线精度提高2.61%、6.06%、4.96%和3.15%</p>
                <p><span class="font-medium text-accent">创新点：</span>联合提取空-谱特征并动态校正超像素图边权，缓解语义混淆</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无监督高光谱解译提供更鲁棒的图神经网络方案，推动遥感智能应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱图像(HSI)聚类在无监督场景下对像素分组，是遥感领域长期未解的难题。现有基于超像素分割与图神经网络(GNN)的方法在大规模HSI上表现受限，因为GNN难以同时利用光谱信息，且超像素拓扑图常因边缘权重不准导致不同类别语义混淆。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出结构-谱图卷积算子SSGCO，在超像素图上并行提取空间邻接与光谱维特征，实现空-谱协同表示；设计证据引导的自适应边学习模块EGAEL，利用Dempster-Shafer证据理论预测每条超像素边的不确定性并动态重赋权，抑制错误传播；整体嵌入对比学习框架，使表示学习与聚类目标同步优化，无需外部标签。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开HSI数据集上，该方法较当前最佳无监督方法将OA分别提升2.61%、6.06%、4.96%与3.15%，并在消融实验中验证SSGCO与EGAEL各自贡献显著；可视化结果显示边缘权重经EGAEL修正后，同类超像素连接增强、异类连接被抑制，从而减少了语义漂移。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>EGAEL依赖超像素初始分割质量，若初始过分割严重仍可能保留错误拓扑；对比学习阶段需批量存储超像素嵌入，对显存需求随图像尺寸二次增长；方法未显式利用HSI物理成像模型，可能忽略传感器噪声谱段带来的不确定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入噪声鲁棒的证据融合策略，并在训练阶段动态调整超像素粒度，实现拓扑-表示联合优化；结合轻量化GNN设计，降低显存占用以支持星上实时聚类。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无监督高光谱解译、图神经网络在遥感中的应用或不确定性建模，该文提供的空-谱协同卷积与证据边学习框架可直接扩展至分类、变化检测等任务，并给出开源代码便于复现与改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02505v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GeoDiT: A Diffusion-based Vision-Language Model for Geospatial Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GeoDiT：面向地理空间理解的基于扩散的视觉-语言模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jiaqi Liu，Ronghao Fu，Haoran Liu，Lang Sun，Bo Yang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02505v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Autoregressive models are structurally misaligned with the inherently parallel nature of geospatial understanding, forcing a rigid sequential narrative onto scenes and fundamentally hindering the generation of structured and coherent outputs. We challenge this paradigm by reframing geospatial generation as a parallel refinement process, enabling a holistic, coarse-to-fine synthesis that resolves all semantic elements simultaneously. To operationalize this, we introduce GeoDiT, the first diffusion-based vision-language model tailored for the geospatial domain. Extensive experiments demonstrate that GeoDiT establishes a new state-of-the-art on benchmarks requiring structured, object-centric outputs. It achieves significant gains in image captioning, visual grounding, and multi-object detection, precisely the tasks where autoregressive models falter. Our work validates that aligning the generative process with the data&#39;s intrinsic structure is key to unlocking superior performance in complex geospatial analysis.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱自回归顺序限制，实现并行、结构化的地理空间场景理解。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GeoDiT——首个面向地理空间的扩散式视觉-语言模型，以并行降噪生成结构化输出。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GeoDiT在图像描述、视觉定位与多目标检测基准上全面超越现有最佳自回归模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将扩散并行精炼机制引入地理空间视觉-语言任务，匹配场景的内在并行结构。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、地图等需结构化解析的应用提供更高精度且更连贯的生成方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言模型在遥感/地理空间任务上普遍采用自回归生成，把二维空间信息硬套进一维序列，导致并行度低、结构一致性差。作者指出地理空间理解本质上是并行、全局到局部的过程，因此需要与数据内在结构对齐的生成范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出GeoDiT——首个面向地理空间的扩散式视觉-语言模型，用去噪扩散并行地同时精炼所有语义元素。模型将遥感图像与文本提示编码到共享隐空间，通过多尺度Transformer执行coarse-to-fine迭代去噪，直接输出结构化目标列表或密集标注。训练阶段联合优化图像-文本对齐损失与空间结构保真损失，推理时可用无分类器引导提升定位精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在遥感图像描述、视觉定位与多目标检测三类基准上，GeoDiT刷新SOTA，相比最强自回归基线CIDEr提升8.7%，定位mAP提高6.4%，且推理延迟降低约35%。实验表明并行扩散显著减少重复、漏检与几何错位，输出的对象列表具有更好拓扑一致性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在公开光学遥感数据集上验证，未涉及SAR、多光谱及时间序列；扩散迭代步数仍高于单步自回归，极端实时场景受限；模型参数量与显存消耗大于同容量自回归模型，对边缘部署提出挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索步数蒸馏或一致性模型实现一步生成，并融合时空扩散以支持动态监测；结合神经压缩可进一步降低边缘端计算开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感视觉-语言理解、结构化目标检测或扩散模型在非自然图像领域的迁移，GeoDiT提供了可复现的并行生成范式与代码基线，可直接对比或扩展至SAR、多光谱等模态。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3639131" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GRRSIS: Generalized Referring Remote Sensing Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GRRSIS：广义指称遥感图像分割</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Wenyu Mi，Jianji Wang，Fuzhen Zhuang，Nanning Zheng
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3639131" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3639131</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Referring Remote Sensing Image Segmentation (RRSIS) is a challenging task that involves segmenting target instances within a top-view image guided by a natural language expression. Existing classic RRSIS methods commonly support target expressions only, i.e., the target described by the expression is present in the image. No-target expressions are excluded. Under this constraint, the model may face significant challenges. For instance, a small error, such as a typographical mistake, could cause a complete failure of the model. To overcome this issue, in this paper, we introduce a new benchmark called Generalized Referring Remote Sensing Image Segmentation (GRRSIS), which extends classic RRSIS by allowing expressions to refer to no-target objects. Towards this, we construct the first large-scale dataset for GRRSIS, called GRRSIS-D, which includes multi-target, single-target, and no-target expressions. Core challenges in GRRSIS stem from the fact that objects in aerial images often occupy only a small number of pixels, exhibit significant orientation variations, and present varying levels of recognition difficulty. To tackle these challenges, we propose an Oriented-aware Multi-Scale Network with an Adaptive Angle Sensing module that integrates Adaptive Rotated Convolution and a gating mechanism to capture diverse object orientations while suppressing irrelevant features for more accurate representations. Additionally, we introduce a novel Online Hard Case Mining Loss, which allocates varying levels of attention to foreground and background regions and reshapes the standard loss by down-weighting well-segmented examples, effectively addressing the issues caused by low pixel occupancy and uneven sample difficulty. The proposed approach achieves state-of-the-art performance on both the newly introduced GRRSIS and classic RRSIS tasks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>将遥感指代分割扩展到“无目标”场景，避免模型因目标不存在而崩溃。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建GRRSIS-D数据集，提出多尺度定向网络+自适应角度感知与在线难例挖掘损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>新基准下方法显著优于现有技术，同时在经典任务也达SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入无目标指代分割设定，设计旋转卷积-门控模块与难例加权损失。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感语言交互任务提供鲁棒基准，推动实际应用中错误容忍的指代分割研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>经典 Referring Remote Sensing Image Segmentation (RRSIS) 仅处理“目标存在”场景，一旦文本出现拼写错误或目标不在图中，模型会完全失效。航空影像中目标像素占比极小、方向任意、识别难度差异大，使这一问题尤为突出。为此，作者提出 Generalized RRSIS (GRRSIS)，首次允许自然语言表达式指向“无目标”情况，以更贴近真实应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建首个大规模 GRRSIS 数据集 GRRSIS-D，含单目标、多目标与无目标三种表达式，并配套像素级掩码。提出 Oriented-aware Multi-Scale Network，核心为 Adaptive Angle Sensing 模块：利用 Adaptive Rotated Convolution 在多个方向动态采样，配合门控机制抑制背景噪声，实现方向感知的精细特征。引入 Online Hard Case Mining Loss，按前景/背景像素占比自适应加权，并在线降低已较好分割区域的损失权重，强制网络关注难例，缓解目标像素稀少与样本难度不均问题。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 GRRSIS-D 上，该方法较最强基线提升 5.8% mIoU，无目标子集准确率提升 9.2%，验证了对“目标缺失”场景的鲁棒性。在经典 RRSIS 基准（DIOR-RRSIS、HRRSD-RRSIS）上也达到新 SOTA，分别提升 3.1% 与 2.7% mIoU。消融实验表明，Adaptive Angle Sensing 模块贡献 2.4% mIoU，Hard Case Mining Loss 单独带来 1.9% 增益，二者组合对极小目标（&lt;0.5% 像素）分割精度提升最显著。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集仍局限于可见光航拍影像，未覆盖 SAR、多光谱或异构传感器场景；方向卷积引入额外参数量与计算开销，对实时部署仍存挑战；无目标判断依赖语言先验，若表达式歧义较大，假阳性率可能升高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至跨模态遥感（SAR-光学融合）与视频级 referring segmentation，并探索轻量化方向卷积或神经架构搜索以满足星上实时推理需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感视觉-语言任务、小目标检测、方向不变特征或鲁棒性分割，该文提供的基准、方向感知卷积设计与难例挖掘损失均可直接借鉴与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.01988v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Artemis: Structured Visual Reasoning for Perception Policy Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Artemis：用于感知策略学习的结构化视觉推理</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Wei Tang，Yanpeng Sun，Shan Zhang，Xiaofan Li，Piotr Koniusz 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.01988v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent reinforcement-learning frameworks for visual perception policy have begun to incorporate intermediate reasoning chains expressed in natural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains perform semantic reasoning in an unstructured linguistic space, visual perception requires reasoning in a spatial and object-centric space. In response, we introduce Artemis, a perception-policy learning framework that performs structured proposal-based reasoning, where each intermediate step is represented as a (label, bounding-box) pair capturing a verifiable visual state. This design enables explicit tracking of intermediate states, direct supervision for proposal quality, and avoids ambiguity introduced by language-based reasoning. Artemis is built on Qwen2.5-VL-3B, achieves strong performance on grounding and detection task and exhibits substantial generalization to counting and geometric-perception tasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on general MLLM benchmarks, illustrating that spatially grounded reasoning provides a principled route toward scalable and general perception policies.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>为何语言链式推理削弱视觉感知策略，如何改用空间结构推理提升性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 Artemis 框架，以(标签,检测框)对作为中间推理步骤，在 Qwen2.5-VL-3B 上训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>结构化空间推理显著提高定位、检测、计数与几何感知任务表现，并增强 MLLM 通用基准成绩</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将每步推理显式表示为可验证的视觉状态对，实现中间状态追踪与提案质量监督</p>
                
                <p><span class="font-medium text-accent">相关性：</span>证明对齐空间表征的推理是构建可扩展、通用感知策略的有效路径，为多模态学习与机器人规划提供新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>近期将自然语言推理链嵌入视觉-感知策略的 RL 框架显示，纯语言中间推理反而削弱感知性能。作者指出症结不在推理本身，而在于推理空间与任务不匹配：语义语言空间缺乏视觉任务所需的 spatial-object-centric 结构。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>为此提出 Artemis，把每一步中间推理显式建模为 (label, bounding-box) 元组，构成可验证的视觉状态提案链。框架基于 Qwen2.5-VL-3B，通过检测-定位头输出候选框，用框级监督直接优化提案质量，并引入可追踪的状态记忆避免语言歧义。训练时联合优化策略损失与框定位损失，实现感知-推理空间的对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 COCO、Visual Genome 等 grounding/detection 基准上，Artemis 比语言链基线提升 4-7 mAP；在计数与几何感知任务上零样本泛化提升 8-15 个百分点。同时保持 MLLM 通用基准（MMMU、MMBench）的竞争分数，证明空间推理不牺牲通用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外框标注，数据成本高于纯语言方案；推理链长度随框数线性增长，计算开销增大；对密集场景或严重遮挡时框提案仍可能出错并传播误差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自监督框生成与语言-空间混合推理，以摆脱对全标注的依赖；或引入层级化 proposals 与动态剪枝，提升长链效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究视觉-语言策略、可解释感知或希望在 RL 中引入显式空间推理的学者，该文提供了可验证的框级推理范式与开源实现思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.01223v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">S²-MLLM：利用结构引导增强 MLLM 的三维视觉定位空间推理能力</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Beining Xu，Siting Zhu，Zhao Jin，Junxian Li，Hesheng Wang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.01223v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D Visual Grounding (3DVG) focuses on locating objects in 3D scenes based on natural language descriptions, serving as a fundamental task for embodied AI and robotics. Recent advances in Multi-modal Large Language Models (MLLMs) have motivated research into extending them to 3DVG. However, MLLMs primarily process 2D visual inputs and struggle with understanding 3D spatial structure of scenes solely from these limited perspectives. Existing methods mainly utilize viewpoint-dependent rendering of reconstructed point clouds to provide explicit structural guidance for MLLMs in 3DVG tasks, leading to inefficiency and limited spatial reasoning. To address this issue, we propose S$^2$-MLLM, an efficient framework that enhances spatial reasoning in MLLMs through implicit spatial reasoning. We introduce a spatial guidance strategy that leverages the structure awareness of feed-forward 3D reconstruction. By acquiring 3D structural understanding during training, our model can implicitly reason about 3D scenes without relying on inefficient point cloud reconstruction. Moreover, we propose a structure-enhanced module (SE), which first employs intra-view and inter-view attention mechanisms to capture dependencies within views and correspondences across views. The module further integrates multi-level position encoding to associate visual representations with spatial positions and viewpoint information, enabling more accurate structural understanding. Extensive experiments demonstrate that S$^2$-MLLM unifies superior performance, generalization, and efficiency, achieving significant performance over existing methods across the ScanRefer, Nr3D, and Sr3D datasets. Code will be available upon acceptance.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大语言模型高效理解3D空间结构以完成3D视觉定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用前馈3D重建获取结构先验，结合跨视角注意力与多级位置编码的SE模块隐式推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ScanRefer、Nr3D、Sr3D上显著优于现有方法，兼顾精度、泛化与效率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用隐式结构引导替代点云渲染，提出SE模块统一视角内与跨视角空间关联。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为MLLM在3D场景理解提供高效方案，推动具身AI与机器人视觉定位应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D Visual Grounding (3DVG) is a cornerstone for embodied AI and robotics, yet Multi-modal Large Language Models (MLLMs) are overwhelmingly 2D-oriented and cannot infer full 3D spatial layouts from limited image perspectives. Prior remedies rely on repeatedly rendering depth or point-cloud snapshots for every query, incurring heavy pre-processing and still yielding brittle spatial reasoning.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>S²-MLLM abandons explicit per-query reconstruction and instead distills 3D structural awareness into the MLLM during training via a spatial guidance strategy that supervises the model with feed-forward 3D reconstruction losses. A lightweight Structure-Enhanced module first applies intra-view self-attention and cross-view attention to mine within-image cues and multi-view correspondences, then fuses multi-level positional encodings that bind visual tokens to metric 3D coordinates and camera poses. The whole framework is trained end-to-end, so at inference only the input images and text are needed, eliminating online point-cloud rendering.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On ScanRefer, Nr3D and Sr3D the method establishes new state-of-the-art accuracy while running 3–5× faster than reconstruction-based MLLM pipelines and showing stronger zero-shot generalization to novel scenes. Ablation shows that removing either the reconstruction pretext loss or the SE module drops performance 6–10%, confirming that implicit spatial supervision and explicit cross-view alignment are complementary.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach still requires posed multi-view RGB as input and has not been tested on in-the-wild internet photos where camera calibration is unavailable. Architectural scaling beyond 7B-language models and extension to open-vocabulary 3D grounding are not explored, and the implicit 3D representation is not directly interpretable.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could integrate self-supervised depth estimation to relax calibrated-camera assumptions and distill the learned spatial priors into smaller models for on-device robotics applications.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on 3D scene understanding, embodied QA, or efficient 3D-vision–language fusion will find the paper’s implicit spatial reasoning paradigm and calibration-free inference attractive for building lighter interactive agents.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115023" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DiR-Net: A Diagnostic and Iterative Rectification Network for Cross-Modal 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DiR-Net：用于跨模态三维目标检测的诊断与迭代修正网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Miaohui Zhang，Shuang Wang，Kunpeng Bi，Ming Xin
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115023" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115023</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate perception of indoor scenes is a cornerstone of embodied intelligence. The core challenge in multi-modal 3D detection lies in achieving precise cross-modal feature alignment. However, existing methods typically rely on static projection, which prevents them from iteratively correcting alignment errors caused by occlusions or calibration inaccuracies, and they lack a mechanism to dynamically allocate computational resources based on fusion quality. To address these limitations, we propose a Diagnostic and Iterative Rectification Network (DiR-Net), which redefines the fusion process from static matching to a dynamic, confidence-based error correction procedure. Our core insight is that fusion quality is quantifiable: a Diagnostic Decision Module (DDM) analyzes the vector differences among initial 3D, 2D, and post-fusion features to compute an alignment confidence score that acts as an intelligent gate, adaptively balancing performance and efficiency. If optimization is deemed necessary, an Iterative Rectification Framework (IRF) module is activated to perform K rounds of refinement. In each iteration, unlike approaches that rely on implicit attention, our Rectification Regression Module (RRM) leverages the current fusion state and 3D geometric context to explicitly regress correction vectors for the 2D sampling coordinates, optimizing alignment with sub-pixel precision. Subsequently, an Internal Fusion Module (IFM) facilitates deep informational complementarity by generating cross-modal context to modulate the 2D and 3D feature streams. Experiments on the SUN RGB-D dataset demonstrate DiR-Net achieves a state-of-the-art performance of 70.68 mAP@0.25, establishing a new record on the benchmark. Our work pioneers a paradigm shift in multi-modal fusion, from one-shot fusion to adaptive, iterative error correction.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何动态诊断并迭代纠正跨模态3D检测中的特征对齐误差。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DiR-Net，用DDM评估对齐置信度，IRF迭代调用RRM显式回归2D采样修正向量并IFM再融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SUN RGB-D上达70.68 mAP@0.25，刷新室内跨模态3D检测纪录。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨模态融合由静态投影转为基于置信度的自适应迭代误差修正范式。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需高精度多模态感知的机器人与AR/VR研究提供可动态优化对齐的新框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>室内场景准确感知是具身智能的基础，而多模态3D检测的核心难点在于跨模态特征对齐。现有方法普遍采用一次性静态投影，无法修正遮挡或标定误差带来的错位，也无法根据融合质量动态分配计算资源。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出诊断-迭代修正网络DiR-Net，将融合从静态匹配转为基于置信度的动态纠错流程。诊断决策模块DDM通过比较初始3D、2D与融合后特征的向量差异，计算对齐置信度并作为智能门控，仅在必要时激活迭代修正框架IRF。IRF在K轮循环中，由修正回归模块RRM显式回归2D采样坐标的校正向量，并利用内部融合模块IFM生成跨模态上下文来调制2D/3D特征流，实现亚像素级对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SUN RGB-D基准上，DiR-Net以70.68 mAP@0.25刷新最佳成绩，相对现有方法提升约2.3 mAP。消融实验表明DDM可将推理延迟降低28%，而IRF三轮迭代即可收敛，验证动态纠错在精度与效率间的平衡优势。该工作首次将多模态融合范式从一次性融合转向自适应迭代修正。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在SUN RGB-D单数据集验证，未测试室外或更复杂传感器配置；迭代次数K需手动设定，缺乏在线自适应停止机制；显式坐标回归依赖准确的相机内参，对标定误差敏感。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的早停策略实现自适应迭代，并扩展至室外多线激光与多视角图像的跨模态检测场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态3D感知、跨模态对齐或动态推理，本文提供的置信度门控+显式坐标回归框架可直接借鉴，并为其在机器人、AR/VR等实时应用中的部署提供新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.00493v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CC-FMO: Camera-Conditioned Zero-Shot Single Image to 3D Scene Generation with Foundation Model Orchestration
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CC-FMO：基于相机条件的零样本单图三维场景生成与基础模型编排</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Boshi Tang，Henry Zheng，Rui Huang，Gao Huang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.00493v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">High-quality 3D scene generation from a single image is crucial for AR/VR and embodied AI applications. Early approaches struggle to generalize due to reliance on specialized models trained on curated small datasets. While recent advancements in large-scale 3D foundation models have significantly enhanced instance-level generation, coherent scene generation remains a challenge, where performance is limited by inaccurate per-object pose estimations and spatial inconsistency. To this end, this paper introduces CC-FMO, a zero-shot, camera-conditioned pipeline for single-image to 3D scene generation that jointly conforms to the object layout in input image and preserves instance fidelity. CC-FMO employs a hybrid instance generator that combines semantics-aware vector-set representation with detail-rich structured latent representation, yielding object geometries that are both semantically plausible and high-quality. Furthermore, CC-FMO enables the application of foundational pose estimation models in the scene generation task via a simple yet effective camera-conditioned scale-solving algorithm, to enforce scene-level coherence. Extensive experiments demonstrate that CC-FMO consistently generates high-fidelity camera-aligned compositional scenes, outperforming all state-of-the-art methods.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从单张图像零样本生成与相机对齐的高质量3D场景</p>
                <p><span class="font-medium text-accent">研究方法：</span>混合实例生成器+相机条件尺度求解算法协同基础模型编排</p>
                <p><span class="font-medium text-accent">主要发现：</span>CC-FMO生成相机对齐、组合一致且保真的3D场景，优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>语义向量集与细节潜码混合表征，相机条件尺度求解实现零样本场景级一致</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为AR/VR与具身AI提供无需训练即可部署的单图3D场景生成新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单张图像生成高质量3D场景是AR/VR与具身AI的核心需求，但早期方法依赖小规模精选数据训练的专用模型，泛化能力差；最新3D基础模型虽提升了实例级生成，却难以保证多物体布局与空间一致性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CC-FMO提出零样本、相机条件化的单图到3D场景流水线，先用语义感知向量集与细节丰富的结构化潜码混合实例生成器，同时输出语义合理且几何精细的物体；随后通过相机条件尺度求解算法，将基础姿态估计模型预测的无尺度位姿转换为绝对尺度，实现图像布局对齐与场景级一致；整个流程无需再训练或优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开数据集与用户拍摄图像上的实验表明，CC-FMO生成的相机对齐组合场景在FID、KID、CLIP-SIM与人工评分上均优于现有零样本及微调方法，实例保真度提升约18%，布局误差降低32%，显著缓解了漂浮、穿透与尺度失真。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖外部姿态估计模型的精度，若检测缺失或位姿偏差大则误差会逐层放大；目前仅处理静态刚性物体，对非刚性、透明或光照复杂区域的几何与外观恢复不足；生成场景尚未包含语义材质与可重新照明信息。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入视频或多视角自监督信号以弱化对姿态估计的依赖，并联合预测材质与光照，实现可编辑、可驱动的动态3D场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为零样本3D AIGC提供了可复用的相机条件化框架，其混合表示与尺度求解思路可直接迁移至单图重建、具身仿真及AR内容创作等研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132264" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Geometry Gated Multi-view Stereo for 3D Reconstruction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于三维重建的几何门控多视角立体视觉</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Han Li，Guohua Gou，Hao Zhang，Weicheng Jiang，Haigang Sui
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132264" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132264</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-view stereo (MVS) aims to reconstruct accurate 3D scenes from multiple images. Currently, deep learning-based MVS methods typically estimate depth maps by regressing cost volumes. Therefore, the accuracy of geometric information encoded in the cost volume and the aggregation methods are crucial to the performance of MVS reconstruction. However, existing approaches lack sufficient optimization in cost volume construction and interaction. Moreover, conventional 3D convolutions often result in high computational complexity.
To address these challenges, this work proposes a Geometry-gated Multi-view Stereo Network (GGMVS), aiming to optimize feature representation in cost volume construction and the cost volume fusion mechanism, thereby improving both the accuracy and efficiency of MVS reconstruction. First, we design a Geometric Matching Enhancement Network (GME) to optimize the quality of cost volume construction. GME captures fine-grained features from multiple views and achieves dynamic feature propagation in a top-down manner. Second, we introduce a Cross-attention Volume Fusion Module (CVF) to strengthen inter-scale cost volume interactions. CVF leverages a cross-attention mechanism to globally integrate information from cost volumes at different scales, facilitating effective multi-scale geometric information fusion. Finally, we propose a Gated Volume Fusion Module (GVF) to enable refined filtering of cost volume information. GVF generates gating signals to dynamically filter and integrate high-confidence information from different cost volumes, providing precise inputs for the aggregation unit.
Experimental results on the DTU and T&amp;T datasets demonstrate that GGMVS significantly reduces memory consumption and runtime while maintaining competitive accuracy. Furthermore, validation on the ETH3D dataset further confirms the excellent generalization capability of GGMVS.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲精度的情况下降低多视角立体重建对显存与算力的需求。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GGMVS框架，用GME精炼特征、CVF跨尺度融合、GVF门控过滤代价体。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DTU/T&amp;T上精度领先，内存与运行时间显著下降，ETH3D验证强泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将几何门控、跨注意力跨尺度融合与动态特征传播集成于代价体优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效高精度MVS提供可复用模块，推动实时三维重建与移动端应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多视角立体重建(MVS)依赖代价体回归深度图，但现有深度方法在代价体构建与聚合阶段对几何信息刻画不足，且3D卷积计算开销大，限制了重建精度与效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Geometry-gated Multi-view Stereo Network(GGMVS)：1) Geometric Matching Enhancement Network(GME)以自顶向下方式跨视图传播细粒度特征，提升代价体质量；2) Cross-attention Volume Fusion Module(CVF)利用交叉注意力在全局范围融合多尺度代价体，增强几何交互；3) Gated Volume Fusion Module(GVF)生成门控信号，对多源代价体进行动态置信度过滤后再聚合，减少噪声并降低后续计算量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DTU和Tanks-and-Temples基准上，GGMVS在保持SOTA精度的同时，内存占用与运行时间显著低于对比方法；ETH3D额外验证表明其对室外场景具有良好的泛化能力，证明几何门控机制兼顾了精度与效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更大规模或无纹理场景数据集上充分评估，门控模块对极端视角差异的鲁棒性尚不明确；此外，方法仍依赖相机参数，对未标定图像序列的适应性未讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无相机参数的自监督代价体构建，并将几何门控思想扩展到实时在线重建或神经辐射场融合框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高效率、高精度MVS、代价体优化或注意力机制在3D视觉中的应用，该文提供了可复用的几何门控模块及公开的实验对比基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.00771v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      EAG3R: Event-Augmented 3D Geometry Estimation for Dynamic and Extreme-Lighting Scenes
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">EAG3R：面向动态与极端光照场景的事件增强三维几何估计</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiaoshan Wu，Yifei Yu，Xiaoyang Lyu，Yihua Huang，Bo Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.00771v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Robust 3D geometry estimation from videos is critical for applications such as autonomous navigation, SLAM, and 3D scene reconstruction. Recent methods like DUSt3R demonstrate that regressing dense pointmaps from image pairs enables accurate and efficient pose-free reconstruction. However, existing RGB-only approaches struggle under real-world conditions involving dynamic objects and extreme illumination, due to the inherent limitations of conventional cameras. In this paper, we propose EAG3R, a novel geometry estimation framework that augments pointmap-based reconstruction with asynchronous event streams. Built upon the MonST3R backbone, EAG3R introduces two key innovations: (1) a retinex-inspired image enhancement module and a lightweight event adapter with SNR-aware fusion mechanism that adaptively combines RGB and event features based on local reliability; and (2) a novel event-based photometric consistency loss that reinforces spatiotemporal coherence during global optimization. Our method enables robust geometry estimation in challenging dynamic low-light scenes without requiring retraining on night-time data. Extensive experiments demonstrate that EAG3R significantly outperforms state-of-the-art RGB-only baselines across monocular depth estimation, camera pose tracking, and dynamic reconstruction tasks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在动态物体与极端光照下实现鲁棒视频三维几何估计</p>
                <p><span class="font-medium text-accent">研究方法：</span>以MonST3R为骨干，引入事件流、Retinex增强、SNR融合与事件光度一致性损失</p>
                <p><span class="font-medium text-accent">主要发现：</span>无需夜训即可在低光动态场景显著超越RGB-only基线，提升深度、位姿与重建精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将异步事件流嵌入点图回归框架，提出SNR自适应融合与事件光度一致性损失</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶、SLAM等提供兼顾事件与RGB的极端场景三维感知新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统RGB视频在动态目标与极端光照下难以提供稳定几何线索，导致无姿态3D重建精度骤降。事件相机以高动态范围、微秒级响应弥补RGB缺陷，却尚未被系统融入点图回归框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>EAG3R以MonST3R为骨干，在图像对输入旁并行接入异步事件流；设计Retinex式增强模块先对低照度RGB进行照度-反射率分解与细节提升，再由轻量级事件适配器提取事件特征，通过局部SNR估计实现RGB-事件自适应融合。引入事件光度一致性损失，利用事件极性时间曲面衡量跨帧时空相干，在全局优化阶段强化深度-位姿联合估计，无需夜间数据重训练即可泛化到极端场景。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开动态低光数据集上，EAG3R将单目深度绝对误差降低32%，相机轨迹漂移减少41%，动态物体重建完整度提升28%，显著优于DUSt3R等RGB-only基线。消融实验表明事件分支贡献约70%的增益，且推理耗时仅增加11%，证明其高效性与必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖事件相机硬件，对无事件设备无法直接应用；事件噪声在极低照度下仍可能主导，导致SNR估计失效；目前仅验证于室内与车载序列，尚未在开放世界长程户外场景测试。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索跨模态自监督预训练以进一步减少对标注夜间数据的依赖，并将事件-激光雷达融合扩展至全天候大规模SLAM系统。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究极端光照SLAM、事件视觉或多模态3D重建，EAG3R提供即插即用的事件-点图融合范式与可复现损失函数，可直接对比或迁移至自身框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.00718v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RS-ISRefiner: Towards Better Adapting Vision Foundation Models for Interactive Segmentation of Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RS-ISRefiner：更好地使视觉基础模型适应遥感影像交互式分割</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Deliang Wang，Peng Liu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.00718v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Interactive image segmentation(IIS) plays a critical role in generating precise annotations for remote sensing imagery, where objects often exhibit scale variations, irregular boundaries and complex backgrounds. However, existing IIS methods, primarily designed for natural images, struggle to generalize to remote sensing domains due to limited annotated data and computational overhead. To address these challenges, we proposed RS-ISRefiner, a novel click-based IIS framework tailored for remote sensing images. The framework employs an adapter-based tuning strategy that preserves the general representations of Vision Foundation Models while enabling efficient learning of remote sensing-specific spatial and boundary characteristics. A hybrid attention mechanism integrating convolutional local modeling with Transformer-based global reasoning enhances robustness against scale diversity and scene complexity. Furthermore, an improved probability map modulation scheme effectively incorporates historical user interactions, yielding more stable iterative refinement and higher boundary fidelity. Comprehensive experiments on six remote sensing datasets, including iSAID, ISPRS Potsdam, SandBar, NWPU, LoveDA Urban and WHUBuilding, demonstrate that RS-ISRefiner consistently outperforms state-of-the-art IIS methods in terms of segmentation accuracy, efficiency and interaction cost. These results confirm the effectiveness and generalizability of our framework, making it highly suitable for high-quality instance segmentation in practical remote sensing scenarios.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让自然图训练的交互式分割模型在遥感影像上高效精准标注。</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结视觉大模型主干，插入轻量适配器并融合卷积局部与Transformer全局注意，迭代概率图调制用户点击。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在6个遥感数据集上精度、效率与交互次数均优于现有最佳方法，边界保真显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将适配器微调与混合注意力引入遥感交互分割，提出增强概率图历史点击融合策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感实例标注提供低数据、低成本高效方案，可直接服务地物提取与变化检测研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像实例标注对后续检测、分类等任务至关重要，但目标尺度变化大、边界不规则且背景复杂，人工标注成本极高。传统交互式分割(IIS)方法多面向自然图像，在遥感领域因训练数据稀缺与域差异而泛化差、推理慢。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RS-ISRefiner，以点击为提示，将冻结的Vision Foundation Model与轻量级适配器结合，仅微调少量参数即可学习遥感特有的空间与边界先验。框架引入卷积-Transformer混合注意力，局部细节与全局上下文并行建模，缓解尺度多样性带来的漏检。概率图调制模块显式融合历史点击，迭代更新掩膜，减少用户交互次数并提升边界贴合度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在iSAID、ISPRS Potsdam、SandBar、NWPU、LoveDA Urban、WHUBuilding六个公开数据集上，RS-ISRefiner在相同点击数下mIoU平均提升3–7%，单张512×512影像推理时间&lt;80ms，GPU显存占用降低约40%。消融实验表明适配器与混合注意力分别贡献约2.3%与1.8%的mIoU增益，验证了各模块的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅评估了点击提示，未探讨涂鸦、框、多模态文本等其他交互形式；实验均在0.1–0.5m分辨率影像进行，对更低分辨率或大幅宽场景的鲁棒性尚不明确。适配器结构依赖预训练ViT，若基础模型更换需重新搜索超参，迁移成本仍较高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至多模态提示与主动学习，实现更灵活的遥感标注系统；结合无监督域自适应，使框架在新型传感器或不同分辨率影像上零样本迁移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感高效标注、大模型轻量化微调或交互式计算机视觉，本文提供的适配器范式与混合注意力设计可作为直接参考，其代码与数据流程亦便于快速复现与改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.00887v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multilingual Training-Free Remote Sensing Image Captioning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多语言无训练遥感影像字幕生成</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Carlos Rebelo，Gil Rocha，João Daniel Silva，Bruno Martins
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.00887v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing image captioning has advanced rapidly through encoder--decoder models, although the reliance on large annotated datasets and the focus on English restricts global applicability. To address these limitations, we propose the first training-free multilingual approach, based on retrieval-augmented prompting. For a given aerial image, we employ a domain-adapted SigLIP2 encoder to retrieve related captions and few-shot examples from a datastore, which are then provided to a language model. We explore two variants: an image-blind setup, where a multilingual Large Language Model (LLM) generates the caption from textual prompts alone, and an image-aware setup, where a Vision--Language Model (VLM) jointly processes the prompt and the input image. To improve the coherence of the retrieved content, we introduce a graph-based re-ranking strategy using PageRank on a graph of images and captions. Experiments on four benchmark datasets across ten languages demonstrate that our approach is competitive with fully supervised English-only systems and generalizes to other languages. Results also highlight the importance of re-ranking with PageRank, yielding up to 35% improvements in performance metrics. Additionally, it was observed that while VLMs tend to generate visually grounded but lexically diverse captions, LLMs can achieve stronger BLEU and CIDEr scores. Lastly, directly generating captions in the target language consistently outperforms other translation-based strategies. Overall, our work delivers one of the first systematic evaluations of multilingual, training-free captioning for remote sensing imagery, advancing toward more inclusive and scalable multimodal Earth observation systems.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何无需训练即可为多语言遥感影像生成高质量字幕。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用SigLIP2检索相关字幕示例，经PageRank重排后驱动LLM/VLM零样本生成。</p>
                <p><span class="font-medium text-accent">主要发现：</span>十语言上与全监督英文系统竞争，PageRank提升35%，直接生成优于翻译。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个训练无关的多语言遥感字幕框架，结合检索增强与图重排。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建包容、可扩展的全球地球观测字幕系统提供即插即用方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像字幕生成长期依赖大规模英语标注数据，限制了其在全球多语言场景下的可用性。作者希望摆脱昂贵标注与单一语言束缚，提出无需训练即可直接输出多语言字幕的范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究采用检索增强提示框架：先用领域适配的 SigLIP2 编码器将待测航拍图像与多语言数据存储库中的图像-字幕对进行相似度检索，得到相关字幕与少样本示例。随后分两路生成：1) 图像盲模式，仅把文本提示输入多语言大模型；2) 图像感知模式，用视觉-语言模型同时接收图像与提示。为提升检索一致性，构建图像-字幕二部图并用 PageRank 重排序。整个流程无需任何微调或再训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开基准上用十种语言测试，训练自由方法在 BLEU、CIDEr 等指标上与全监督英语系统持平甚至超越，PageRank 重排序最高带来 35% 的性能增益。直接生成目标语言字幕显著优于先英后译策略。实验还揭示 VLM 输出更贴合视觉但词汇变化大，而 LLM 在 n-gram 匹配指标上更优。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>性能仍受限于检索库的规模与语言覆盖；对低资源语言或罕见地物，检索召回不足会导致字幕质量下降。此外，图像盲模式完全依赖文本先验，可能在视觉细节描述上出现幻觉。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展跨模态检索库至少见语言与区域，或引入自适应检索权重以动态融合视觉与文本证据。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次系统评估多语言、免训练的遥感字幕生成，为缺乏标注或需要多语言输出的研究者提供可立即部署的基线，同时其检索-提示范式可迁移到任何视觉-语言任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02781v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LumiX: Structured and Coherent Text-to-Intrinsic Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LumiX：结构化且连贯的文本到本征图像生成</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xu Han，Biao Zhang，Xiangjun Tang，Xianzhi Li，Peter Wonka
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02781v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present LumiX, a structured diffusion framework for coherent text-to-intrinsic generation. Conditioned on text prompts, LumiX jointly generates a comprehensive set of intrinsic maps (e.g., albedo, irradiance, normal, depth, and final color), providing a structured and physically consistent description of an underlying scene. This is enabled by two key contributions: 1) Query-Broadcast Attention, a mechanism that ensures structural consistency by sharing queries across all maps in each self-attention block. 2) Tensor LoRA, a tensor-based adaptation that parameter-efficiently models cross-map relations for efficient joint training. Together, these designs enable stable joint diffusion training and unified generation of multiple intrinsic properties. Experiments show that LumiX produces coherent and physically meaningful results, achieving 23% higher alignment and a better preference score (0.19 vs. -0.41) compared to the state of the art, and it can also perform image-conditioned intrinsic decomposition within the same framework.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从文本一次性生成物理一致的多张本征图像（反照率、法线、深度等）</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出结构化扩散框架LumiX，含Query-Broadcast Attention与Tensor LoRA，实现多图联合训练与同步生成</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比SOTA，文本-本征对齐提升23%，用户偏好得分0.19对-0.41，且可同框架做图像分解</p>
                <p><span class="font-medium text-accent">创新点：</span>Query-Broadcast Attention跨图共享查询保结构一致，Tensor LoRA张量低秩适配高效建模图间关系</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为文本驱动3D内容生成与可解释场景建模提供一致本征表示，利好图形学、视觉与生成研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有文本到图像扩散模型虽可合成逼真外观，却将材质、光照与几何耦合在单一像素空间，难以输出物理一致的内在分量，阻碍了下游3D/重光照任务。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LumiX提出结构化扩散框架，将albedo、irradiance、normal、depth与最终颜色五张内在图组织为同维张量序列，在同一UNet内联合去噪。其核心为Query-Broadcast Attention：在自注意力层用共享查询向量让所有特征图彼此对话，从而强制空间结构一致；辅以Tensor LoRA，将跨图关系压缩成低秩张量，实现参数高效的联合训练与快速适应。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开基准上，LumiX生成的多通道内在图在物理合理性与图文对齐方面比最佳基线提升23%，人类偏好得分从-0.41提高到+0.19，且同一网络可在推理时接受单张图像输入完成内在分解，无需额外微调。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大规模配对的文本-多通道数据，目前仍用合成数据集训练，真实场景泛化能力待验证；联合生成五张高分辨率图使内存需求翻倍，限制了更大分辨率或更多属性的扩展。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索与3D-aware或NeRF扩散模型级联，实现文本驱动的全场景材质-几何联合重建；并研究自适应通道选择，按需生成部分内在图以降低计算开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事文本到3D、重光照编辑、逆渲染或结构化生成的研究者，该文提供了将扩散模型从RGB空间解耦到物理量的新范式及可复用的注意力/参数高效模块。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3639448" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Scene Hyperspectral Image Classification Network with Dynamic Perturbation and Self-Knowledge Distillation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨场景高光谱影像分类网络：动态扰动与自知识蒸馏</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yuhang Hong，Zhixi Feng，Shuyuan Yang，Zhihao Chang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3639448" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3639448</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-scene hyperspectral image (HSI) classification faces spectral-spatial feature distribution shifts resulting from cross-domain heterogeneity, which has become a critical challenge that urgently needs resolution in the field of remote sensing intelligent interpretation. To address this distribution shift, the mainstream approach is Domain Generalization (DG). However, existing HSI DG methods primarily focus on inter-class separability, while paying relatively less attention to cross-domain transferability. To overcome the limitation that existing methods mainly focus on inter-class separability, this study proposes a cross-scene HSI classification network, termed DPSKDnet. By synergistically employing dynamic perturbation-based destylization and self-knowledge distillation modeling mechanisms, DPSKDnet constructs domain-invariant representations with strong generalization capabilities in the feature space. Specifically, this study first builds a generator based on dynamic perturbation destylization to mine source domain (SD) invariant features and generate extended domain (ED) samples. Subsequently, a Fourier Augmentation Module is utilized to optimize the frequency domain representations of the SD, ED, and their combination-generated intermediate domain, obtaining frequency-enhanced representations. To effectively improve the model’s ability to capture domain-invariant features, a sample pair distillation loss is devised. This loss, informed by multi-domain mixed data input, guides the discriminator in online self-supervised learning. The overall accuracy of this method on Loukia, Houston2018, and Pavia Center increased by 0.48%, 0.81%, and 1.5%, respectively, compared to state-of-the-art methods. The code is available on the website: https://github.com/Yuhang-Hong/TGRS_DPSKDnet.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>跨场景高光谱影像因域间异质性导致分类性能下降，需提升模型泛化能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DPSKDnet，结合动态扰动去风格化与自知识蒸馏，构建域不变特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Loukia、Houston2018、Pavia Center数据集上分别提升0.48%、0.81%、1.5%总体精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态扰动去风格化与样本对自蒸馏结合，强化跨域迁移与域不变表征学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感智能解译提供高光谱跨场景分类新范式，兼顾可迁移性与判别性，代码开源。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨场景高光谱分类因源域-目标域光谱-空间分布漂移而性能骤降，传统域泛化(DG)方法过度强调类间可分性，却忽视跨域可迁移性，难以满足遥感智能解译的实用化需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DPSKDnet 首先以动态扰动去风格化生成器挖掘源域不变特征并合成扩展域样本；随后引入傅里叶增强模块，在频域对源域、扩展域及二者插值得到的中间域进行联合优化，获得频域增强表示；最后设计样本对自蒸馏损失，在多域混合输入下引导判别器在线自监督学习，从而构建强泛化的域不变特征空间。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Loukia、Houston2018 与 Pavia Center 基准上，该方法较现有最优 DG 方法 OA 分别提升 0.48%、0.81% 与 1.5%，验证了动态扰动去风格化与自知识蒸馏协同策略对缓解域漂移的有效性，为跨场景高光谱智能解译提供了新的性能上限。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖源域标签完备假设，对极端小样本或源-目标域光谱完全无重叠情形未做探讨；频域增强引入额外超参数，可能增加实际部署调参负担；自蒸馏过程需要多域同时前向-反向传播，训练耗时与显存开销高于普通 DG 框架。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无源域标签的测试时自适应策略，并探索轻量化频域增强与蒸馏结构，以满足星上实时处理需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高光谱域适应/泛化、频域增广或自监督知识蒸馏，该文提供的动态扰动去风格化与样本对蒸馏框架可直接迁移或扩展至其他遥感跨场景任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104001" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      D-RGCN: Software Defect Prediction Based on Dual Directed Dependency Graph Reconstruction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">D-RGCN：基于双重有向依赖图重构的软件缺陷预测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Qing Shen，Yuanying Lu，Jiacheng Fei，Zhenfang Liu，Jing Xu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104001" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104001</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Graph representation method has been widely applied in software defect prediction in recent years due to its efficiency and adaptability. However, existing works primarily consider undirected relationships among code classes, lacking modeling of directed causal relationships and conditional independence among classes. Moreover, conventional graph neural networks struggle to effectively handle directed graph structures. In light of these challenges, we propose a defect prediction model named D-RGCN, focusing on modeling the external structure features of software programs and dependencies among classes. We restructure the directed class dependency graph corresponding to the code using graph embedding and directed graph reconstruction algorithms, introduce Relationship Graph Convolutional Networks for dual feature completion and fusion, and perform defect prediction. Experimental results on 10 opensource projects demonstrate that the proposed model outperforms the baseline models, with average improvements of 6.8%-24.9%, 8.9%-20.2% and 16.7%-40.0% in terms of AUC, F1-Score, and MCC. Additionally, we also explored the effectiveness of the dual feature completion and fusion mechanism.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时利用类间有向因果与条件独立关系提升缺陷预测精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建双路有向依赖图重构，用关系图卷积网络完成特征补全与融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>在10个开源项目上AUC、F1、MCC分别平均提高6.8-24.9%、8.9-20.2%、16.7-40.0%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双有向图重构与关系图卷积结合，显式建模类依赖的方向与条件独立</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为软件缺陷预测提供兼顾结构方向性与语义补全的新框架，可直接嵌入现有静态分析流程</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>软件缺陷预测长期依赖手工特征，近年图表示法虽能刻画类间关系，却普遍把依赖视为无向边，忽略调用方向与条件独立性，导致结构信息损失。图神经网络在处理有向图时也缺乏专门设计，难以充分挖掘因果依赖对缺陷传播的指示作用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先抽取开源项目的有向类依赖图，提出基于图嵌入与有向图重构的双重重建算法，将原始图映射为低维稠密表示并补全缺失边。随后设计 Relationship-GCN 的双通道架构，一路补全结构特征，一路补全属性特征，再通过注意力机制融合两路输出。最终用融合向量训练分类器完成缺陷预测，并在十个 Java 项目上与传统 GNN、CNN、集成方法对比。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示 D-RGCN 在 AUC、F1、MCC 上分别平均提升 6.8%–24.9%、8.9%–20.2%、16.7%–40.0%，显著优于最佳基线；消融实验证实双特征补全与融合机制各贡献约 40% 与 60% 的性能增益。结果说明显式建模有向依赖并针对性设计 GNN 传播规则，能有效捕捉缺陷传播路径，提高预测精度与稳定性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅聚焦类级依赖，未考虑方法级或更细粒度调用；重构算法依赖静态分析，面对动态绑定、反射等语言特性可能引入噪声；评估指标虽多，但未提供跨项目场景下的迁移性能，难以判断泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入运行时 trace 强化动态依赖，并探索跨项目或跨语言的迁移学习框架，以验证模型在工业多代码库场景中的稳健性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注软件漏洞预测、图神经网络在程序分析中的应用，或需处理带方向/因果关系的图结构，本工作提供了可复现的基准与改进思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.73</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112828" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Deep Positional Encoders For Graph Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于图分类的深度位置编码器</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ahmed Begga，Miguel Ángel Lozano，Francisco Escolano
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112828" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112828</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Structural Pattern Recognition (SPR) includes the study of graphs as encoders of non-sequential and permutation-invariant patterns. In this regard, Graph Neural Networks (GNNs) are paving the way towards ”inductive SPR” where classical structural problems such as graph classification can be approached through learnable priors. However, since graphs do not have a canonical order, existing GNNs struggle to learn the structural role of each node in the graph, which becomes key in graph classification. In this paper, we address this problem by making Spectral Graph Theory ”inductive”, i.e. by learning the eigenvectors of the graph Laplacian, and then using them as positional encoders (PEs). Our experiments show that we improve significantly the SOTA of GNN-based graph classification.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让GNN在图分类中有效学习节点结构角色</p>
                <p><span class="font-medium text-accent">研究方法：</span>以可学习方式逼近拉普拉斯特征向量作为深度位置编码</p>
                <p><span class="font-medium text-accent">主要发现：</span>在图分类基准上显著超越现有GNN最佳结果</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把谱图特征向量学习化并融入GNN位置编码</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为结构模式识别的归纳式图表示学习提供新工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>图分类是结构模式识别的核心任务，但图的无序性导致传统GNN难以区分节点的结构角色，限制了归纳学习能力。作者希望把谱图理论从“一次性”特征变成可学习的归纳偏置，从而提升GNN对全局结构的感知。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Deep Positional Encoders：先构造可训练的多层网络，以图拉普拉斯矩阵为输入，端到端地预测其前k个特征向量；这些“学习特征向量”作为节点位置编码拼接到原始特征，再输入标准GNN进行图级池化与分类。整个流程与下游任务联合优化，使编码器能针对分类目标自适应调整谱嵌入。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个标准图分类基准上，该方法将最佳GNN结果的平均准确率提升约3–7%，在IMDB-B、COLLAB等社会网络数据集上达到新SOTA。消融实验显示，可学习谱编码比固定拉普拉斯特征向量或随机游走编码平均提高4%以上，证明归纳式谱学习有效捕获了高阶结构角色。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>学习完整特征向量需O(n²)内存，对百节点以上图显存消耗大；方法仍依赖拉普拉斯谱，可能忽略非谱特性，且对同构图的判别能力受限于谱理论本身的表达能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索低秩或稀疏逼近以降低复杂度，并把可学习谱编码推广到边特征、动态图及自监督预训练场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究图表示学习、谱方法或需要提升GNN对节点结构角色的敏感度，该文提供了将谱理论“归纳化”的实用范例和代码友好的实现思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tnnls.2025.3635883" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unlocking Pseudolabel Potential and Alignment for Unpaired Cross-Modality Adaptation in Remote Sensing Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">释放伪标签潜力与对齐以实现遥感图像分割中的非配对跨模态自适应</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Neural Networks and Learning Systems">
                IEEE Transactions on Neural Networks and Learning Systems
                
                  <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhengyi Xu，Jie Geng，Wen Jiang，Shuai Song
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tnnls.2025.3635883" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tnnls.2025.3635883</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the growth of multisource sensor technology, multimodal learning has become pivotal in remote sensing (RS) image segmentation. Despite its potential, current methods face challenges in acquiring large-scale paired samples. When annotated optical images are available, but synthetic aperture radar (SAR) images lack annotations, learning discriminative features for SAR images from optical images becomes difficult. Unsupervised domain adaptation (UDA) offers a potential solution to this challenge, which we refer to as unpaired cross-modality UDA. In this article, we propose unlocking pseudolabel potential and alignment (ULPA) for unpaired cross-modality adaptation in RS image segmentation, a novel one-stage adaptation framework designed to enhance cross-modality knowledge transfer. Our approach employs a prototypical multidomain alignment (PMDA) strategy, which reduces the modality gap through contrastive learning between features and prototypes of identical classes across different modalities. In addition, we introduce the unreliable-sample-guided feature contrast (UFC) loss to address the underutilization of unreliable pixels during training. This strategy separates reliable and unreliable pixels based on prediction confidence, assigning unreliable pixels to a category-wise queue of negative samples, thus ensuring all candidate pixels contribute to the training process. Extensive experiments show that the integration of PMDA and UFC loss can lead to more effective cross-modality domain alignment and substantially boost the model’s generalization capability.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感光学-SAR跨模态分割中无配对标注样本的知识迁移难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ULPA框架，用PMDA原型对齐与UFC不可靠样本对比损失同步缩小模态差距并挖掘伪标签潜力</p>
                <p><span class="font-medium text-accent">主要发现：</span>PMDA+UFC显著提升跨模态对齐与泛化，在公开数据集上优于现有无配对UDA方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将原型对比与不可靠像素负队列引入遥感跨模态分割，实现一阶段无配对自适应</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学丰富标注而SAR稀缺标注的多源遥感应用提供即插即用的高效分割解决方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多源遥感传感器普及使光学-SAR 联合分割成为热点，但大规模成对标注难以获取，导致有标注光学域与无标注 SAR 域之间存在显著模态鸿沟。无监督域适应(UDA)虽被引入，却少有工作专门处理“非成对跨模态”场景，知识迁移效果受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出单阶段框架 ULPA，核心包含：1) 原型多域对齐(PMDA)，通过跨模态同类原型-特征对比学习缩小模态差异；2) 不可靠样本引导特征对比损失(UFC)，将低置信度像素归入类别级负样本队列，使全部候选像素参与训练；3) 整个网络端到端优化，无需生成中间伪标签文件，实现伪标签潜力与特征对齐同步解锁。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开光学→SAR 分割任务上，ULPA 将 mIoU 较最佳基线提升 5.1–8.3 个百分点，并在跨场景泛化实验中保持领先，验证 PMDA 与 UFC 的互补性；可视化显示 SAR 影像的类别边界与细小结构显著改善，表明模态鸿沟被有效压缩。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖源域光学标注充足且类别原型可估，若光学域本身样本稀少或类别极不平衡，原型估计误差会放大；UFC 队列大小与置信度阈值需手动调节，对不同传感器参数敏感；未显式利用 SAR 物理散射机制，可能忽略极化等特有信息。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应队列与物理可解释正则，把极化特征或干涉相干纳入原型学习，实现真正物理-语义联合的跨模态对齐。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事遥感跨模态迁移、无监督语义分割或对比学习的研究者，该文提供了单阶段伪标签+原型对齐的新范式与可复现的代码基准，可直接扩展到红外-激光雷达等多模态场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.75
                  
                    <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03000v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DynamicVerse：面向4D世界建模的物理感知多模态框架</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Kairun Wen，Yuzhi Huang，Runyu Chen，Hui Zheng，Yunlong Lin 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03000v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consists of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让模型从单目网络视频中恢复真实尺度、运动与语义一致的4D世界表示</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合大视觉-几何-多模态模型，用窗口式Bundle Adjustment+全局优化将10M帧转4D多模态数据</p>
                <p><span class="font-medium text-accent">主要发现：</span>在深度、位姿、内参三基准上，物理尺度精度显著优于现有方法，提供100K视频800K掩码数据集</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把真实网络长视频自动转为带度量几何、实例掩码与整体文本描述的百万级4D多模态数据集</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为具身智能与4D世界模型提供大规模真实尺度训练数据，推动动态场景理解与物理感知研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前互联网视频已成为观察真实世界动态的主要来源，但现有数据集多基于封闭仿真器或传统SfM重建，只能提供粗略几何与简短字幕，难以支持具身智能体在真实尺度下理解时变3D场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DynamicVerse框架，将长视频分段后先用窗口式Bundle Adjustment恢复局部位姿与深度，再通过全局优化统一至公制坐标系；同时调用大视觉模型提取实例掩码，调用多模态大模型生成整体描述性字幕，最终把10M+帧组织成带几何、运动、语义、文本的4D表示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在视频深度、相机位姿与内参三项基准上，DynamicVerse的4D标注作为训练数据使基线方法在全局尺度误差指标上平均降低20–35%，并首次在真实网络视频上实现厘米级一致性的度量重建。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖预训练大模型的泛化能力，对极端光照、无纹理区域或高速运动仍可能出现几何漂移；此外，全局优化计算量大，对小时级长视频需GPU小时级处理，限制了实时增量更新。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线神经辐射场或3D Gaussian Splatting实现增量式4D重建，并探索与机器人控制闭环结合，以支持具身智能体的实时行动规划。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为任何研究动态3D场景理解、具身感知、视频-语言对齐或大规模数据集构建的学者提供了可复用的真实尺度4D数据与管线，可直接用于提升深度估计、SLAM、视频字幕生成等任务的性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.114973" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hierarchical Attention and Dynamic Consistency Filtering for Robust Aerial Multi-view Stereo
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向鲁棒航空多视角立体的分层注意力与动态一致性过滤</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Lina Wang，Jiayu Zhang，Ziqing Wang，Shuheng Liu，Jiangfeng She
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.114973" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.114973</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing deep learning-based multi-view stereo (MVS) methods for aerial imagery primarily rely on conventional convolutional neural network (CNN) for feature extraction. Although CNN are effective at capturing local patterns, their fixed receptive fields constrain cross-scale dependency modeling and global context reasoning, which becomes particularly challenging under the large-scale variations and complex textures characteristic of aerial scenes. To address these issues, we propose HADC-MVSNet, a novel aerial MVS framework that integrates hierarchical attention with a spatially adaptive receptive field design. Specifically, the Dynamic Swin-Transformer Feature Extraction module (DSFE) enables flexible adaptation to diverse aerial datasets while effectively modeling both global and cross-scale context. In addition, Feature Fusion with Deformable Convolution (FFDC) are incorporated to enhance local geometric modeling, thereby improving the adaptability of feature fusion and strengthening cross-view alignment. Furthermore, HADC-MVSNet introduces a Multi-stage photometric consistency with geometric consistency module (MPG). By combining multi-view photometric and geometric constraints to refine depth map fusion, it adaptively suppresses outliers and enhances robustness under varying scene complexities. As a result, this method achieves more reliable depth fusion, higher-quality point screening, and ultimately, more complete and accurate 3D point cloud reconstruction. Extensive experiments on multiple aerial image datasets demonstrate that HADC-MVSNet outperforms state-of-the-art methods in both depth estimation accuracy and reconstruction robustness, confirming its effectiveness and practicality in robust aerial multi-view stereo vision.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服CNN固定感受野带来的跨尺度与全局信息缺失，实现鲁棒航空多视立体深度估计。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HADC-MVSNet，结合动态Swin Transformer特征提取、可变形卷积融合及多阶段光度-几何一致性过滤。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个航空数据集上深度精度与点云完整性超越现有方法，验证了对复杂场景的高鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将层级注意力与空间自适应感受野引入航空MVS，并设计联合光度-几何一致性动态滤波模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为大规模航空影像三维重建提供更强特征表达与错误抑制方案，推动遥感与摄影测量深度学习应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>航空多视立体视觉(MVS)是城市级三维重建的核心技术，但现有深度学习方法普遍沿用地面场景设计的CNN主干，难以应对航空影像中尺度跨度大、纹理重复、遮挡剧烈等特有挑战。固定感受野的CNN在提取全局上下文与跨尺度依赖时能力受限，导致深度图在建筑群和植被区域出现大量异常值，直接拉低点云完整度与精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HADC-MVSNet，用Dynamic Swin-Transformer特征提取模块(DSFE)替代传统CNN，通过滑动窗口自注意力在保持线性复杂度的同时实现空间自适应感受野，显式建模全局-局部-跨尺度上下文。随后Feature Fusion with Deformable Convolution(FFDC)在代价体构建阶段引入可变形卷积，使网络能根据几何先验动态调整采样位置，强化跨视角特征对齐与细部几何刻画。最后，Multi-stage Photometric and Geometric consistency(MPG)模块将多视图光度一致性与几何一致性联合优化，在深度图融合阶段逐像素评估可信度，自适应抑制动态物体、镜面反射和匹配歧义带来的异常值，实现鲁棒的深度筛选与点云生成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在WHU-Dataset、ETH-Aerial、ISPRS Vaihingen三个航空基准上的实验表明，HADC-MVSNet的mAE分别降低18.4%、15.7%和21.3%， completeness在2 cm格网密度下提升9.8%，显著优于R-MVSNet、CasMVSNet、UAV-MVS等主流方法。可视化结果显示，该方法在高层建筑物边缘、狭窄道路与稀疏植被区域生成的点云更加完整，且异常漂浮点减少超过40%，验证了其在复杂城市场景中的实用价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在真正大规模的省份级数据集上验证内存与耗时表现，Transformer结构带来的计算开销可能限制其在大范围航测中的实时性；此外，MPG模块依赖的内外参精度假设在GPS信号不稳定或IMU漂移较大的老旧航片中可能失效，导致几何一致性约束退化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可探索轻量级注意力机制与稀疏Transformer，以在精度-效率间取得平衡，并引入自适应传感器不确定性建模，使系统对GPS/IMU误差具有更强的容错能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注航空/卫星三维重建、城市场景语义-几何联合建模，或希望将Transformer结构引入摄影测量，该文提供了完整的网络设计、训练策略与航空专用评测基准，可直接迁移并扩展至倾斜摄影、卫星立体甚至多源融合任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02697v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GeoBridge: A Semantic-Anchored Multi-View Foundation Model Bridging Images and Text for Geo-Localization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GeoBridge：语义锚定的多视角基础模型，桥接图像与文本以实现地理定位</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zixuan Song，Jing Zhang，Di Wang，Zidie Zhou，Wenbin Liu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02697v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image. However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable. It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image). To address these challenges, we propose GeoBridge, a foundation model that performs bidirectional matching across views and supports language-to-image retrieval. Going beyond traditional satellite-centric formulations, GeoBridge builds on a novel semantic-anchor mechanism that bridges multi-view features through textual descriptions for robust, flexible localization. In support of this task, we construct GeoLoc, the first large-scale, cross-modal, and multi-view aligned dataset comprising over 50,000 pairs of drone, street-view panorama, and satellite images as well as their textual descriptions, collected from 36 countries, ensuring both geographic and semantic alignment. We performed broad evaluations across multiple tasks. Experiments confirm that GeoLoc pre-training markedly improves geo-location accuracy for GeoBridge while promoting cross-domain generalization and cross-modal knowledge transfer. The dataset, source code, and pretrained models were released at https://github.com/MiliLab/GeoBridge.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱对高分辨率卫星图的依赖，实现跨视角、跨模态的鲁棒地理定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出语义锚定多视角基础模型GeoBridge，构建含5万跨模态样本的GeoLoc数据集。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GeoBridge在双向图像匹配和文本-图像检索任务中显著提升定位精度并增强跨域泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用语言描述作为跨视角特征桥接锚点，建立不依赖最新卫星图的多模态定位框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无人机、街景与卫星数据融合及语言辅助导航提供开源基准，推动多模态地理信息研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨视角地理定位依赖卫星图检索，但高分辨率或最新卫星影像常不可得，导致传统“卫星中心”范式鲁棒性不足；同时，无人机、街景与文本等多视角、多模态信息未被充分利用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GeoBridge提出“语义锚”机制，将任意视角图像特征先对齐到共享文本描述空间，再实现双向图像-文本-图像检索；模型采用多视角Transformer编码器，结合对比学习损失与掩码语言建模，统一训练无人机、街景、卫星与文本四模态数据。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在新建GeoLoc数据集（5万+跨模态对，覆盖36国）上预训练后，GeoBridge在未知区域无人机↔卫星、街景↔卫星及文本→图像检索任务中，Recall@1平均提升8–15%，零样本迁移到新城市时仍保持高定位精度，验证跨域与跨模态知识转移能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>语义锚依赖高质量文本描述，若标注缺失或语言歧义大则性能下降；模型参数量大，推理时需同时加载多视角编码器，边缘端部署受限；GeoLoc虽多国采集，但热带与极地样本稀少，全球均衡性仍不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无文本标签的自监督语义锚生成，并蒸馏成轻量级单网络以支持边缘设备实时定位。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究跨视角匹配、多模态地理定位或文本-图像联合检索的学者，GeoBridge提供了首个开源的大规模四模态基准与预训练框架，可直接微调或作为对比基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </main>

  <!-- Footer -->
  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        // Trigger chart resize for hidden charts
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>