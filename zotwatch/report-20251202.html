<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-02</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <!-- Header -->
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-02 12:00 Asia/Shanghai
      </p>
    </div>
  </header>

  <!-- Researcher Profile Section (Collapsible) -->
  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="researcher-profile" class="section-expand collapsed">
      <!-- Basic Stats - Compact Row -->
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">78</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;8</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      <!-- LLM Insights - Compact -->
      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户的核心阅读兴趣集中在“深度估计”与“知识图谱”两大方向，分别占收藏量的16.7%和10.3%，并持续跟踪图神经网络在遥感与地理信息中的应用。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在“深度估计”主题下收藏13篇，且多与计算机视觉顶会/期刊交叉，显示其对单目/多目深度恢复、遥感立体匹配有系统积累；同时以“地球空间信息”“土地利用”为关键词的7篇中文测绘类论文，表明其深耕遥感地学解析。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读轨迹明显横跨计算机视觉与测绘遥感两大领域，既关注CVPR/AAAI的算法创新，也大量浏览《测绘学报》《武汉大学学报（信息科学版）》等地球空间信息期刊，呈现“AI+遥感”交叉特征。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2023-2025年新增收藏仍以计算机视觉与知识图谱为主，但2025年起出现“超图神经网络”“高阶数据相关性”等新关键词，显示其开始关注图模型在复杂地学关系建模中的前沿扩展。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注多模态遥感深度估计与地理知识图谱融合的研究，以及图神经网络在时空预测、三维场景理解中的最新进展。</p>
        </div>
      </div>
      

      <!-- Charts Row: Domains + Trends -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Research Domains Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        <!-- Quarterly Trends Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      <!-- Year Distribution Chart (full width) -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      <!-- Authors and Venues Row -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Top Authors -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Bingyi Kang">Bingyi Kang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">3</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiashi Feng">Jiashi Feng</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">3</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wei Wang">Wei Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lihe Yang">Lihe Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zilong Huang">Zilong Huang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiaogang Xu">Xiaogang Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Hengshuang Zhao">Hengshuang Zhao</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Di Wang">Di Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shunyu Liu">Shunyu Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wentao Jiang">Wentao Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Fengxiang Wang">Fengxiang Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yi Liu">Yi Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        <!-- Top Venues -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="测绘学报">测绘学报</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">4</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Proceedings of the AAAI Conference on Artificial Intelligence">Proceedings of the AAAI Conference on Artificial Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="武汉大学学报（信息科学版）">武汉大学学报（信息科学版）</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Pattern Recognition">Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Neural Networks">IEEE Transactions on Neural Networks</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Information Fusion">Information Fusion</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="测绘工程">测绘工程</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="计算机工程与应用">计算机工程与应用</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      <!-- Keywords -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(10)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识图谱 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            土地利用 <span class="text-text-secondary">(3)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            地球空间信息学 <span class="text-text-secondary">(3)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            图神经网络 <span class="text-text-secondary">(2)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            图斑聚合 <span class="text-text-secondary">(2)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            云计算 <span class="text-text-secondary">(2)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            空间感知与认知 <span class="text-text-secondary">(2)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            人工智能 <span class="text-text-secondary">(2)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Deep learning <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            End-to-end learning <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Multiple instance learning <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Neural networks <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            convolutional architecture <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            graph neural networks <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            semi-supervised learning <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            多模态数据学习 <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            超图神经网络 <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            高阶数据相关性 <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            图数据处理 <span class="text-text-secondary">(1)</span>
          </span>
          
        </div>
      </div>
      

      <!-- Generation info -->
      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-02 11:47 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Chart.js Initialization for Researcher Profile -->
  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      // Chart.js global defaults for academic style
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      // Domains Horizontal Bar Chart
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['图神经网络', '知识图谱', '深度估计', '地球空间信息', '多示例学习', '遥感影像', '结构恢复', '移动感知'],
            datasets: [{
              data: [5, 8, 13, 7, 3, 3, 3, 2],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      // Quarterly Trends Bar Chart with 3 cycling colors (one per month in quarter)
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 4 }, { q: '2023-Q2', c: 1 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 3 }, { q: '2024-Q2', c: 2 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 1 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 1 }, { q: '2025-Q4', c: 2 }];
        // Color palette: 3 colors cycling for months within quarters
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },   // Blue
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },   // Green
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }    // Amber
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      // Year Distribution Line Chart
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 1 }, { year: 2010, count: 0 }, { year: 2011, count: 2 }, { year: 2012, count: 1 }, { year: 2013, count: 0 }, { year: 2014, count: 1 }, { year: 2015, count: 1 }, { year: 2016, count: 3 }, { year: 2017, count: 2 }, { year: 2018, count: 4 }, { year: 2019, count: 1 }, { year: 2020, count: 1 }, { year: 2021, count: 1 }, { year: 2022, count: 3 }, { year: 2023, count: 5 }, { year: 2024, count: 5 }, { year: 2025, count: 4 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    // Show every 5th year label to avoid crowding
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      
    });
  </script>
  

  <!-- Overall Summaries Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于多模态融合的论文、1篇关于交互式分割的论文、1篇关于开放词汇分割的论文和1篇关于统一开放世界分割的论文。</p>
            
            <p><strong class="text-accent">多模态融合</strong>：《Referring Remote Sensing Image Segmentation via Multi-Scale Spatially-Guided Joint Prediction》通过文本引导的多尺度空间联合预测实现遥感指代分割；《Multimodal cross fusion Mamba network for remote sensing image semantic segmentation with complementary masked self-supervision》提出交叉融合Mamba网络并辅以互补掩码自监督，充分利用多模态互补信息提升语义分割精度。</p>
            
            <p><strong class="text-accent">交互式分割</strong>：《RS-ISRefiner》将视觉基础模型适配到遥感交互式分割任务，通过迭代细化策略应对遥感影像中目标尺度变化与边界复杂的问题，实现高精度标注生成。</p>
            
            <p><strong class="text-accent">开放词汇分割</strong>：《Reducing semantic ambiguity in open-vocabulary remote sensing image segmentation via knowledge graph-enhanced class representations》引入知识图谱增强类别表示，显著缓解开放词汇遥感影像分割中的语义歧义，提高对未知类别的识别能力。</p>
            
            <p><strong class="text-accent">统一开放世界分割</strong>：《UniGeoSeg: Towards Unified Open-World Segmentation for Geospatial Scenes》构建指令驱动的统一框架，可在开放世界中一次性完成多地理场景分割，克服传统方法碎片化局限，提升泛化与可用性。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于空间几何与3D理解的论文、8篇关于开放词汇与遥感分割的论文、5篇关于多模态推理与问答的论文、4篇关于多模态推荐与融合的论文、2篇关于农业视觉的论文以及2篇关于图像生成与编辑的论文。</p>
            
            <p><strong class="text-text-secondary">空间几何3D</strong>：该主题聚焦单目图像深度、法向、3D场景重建及3D视觉定位，代表作《Lotus-2》用生成式扩散模型提升几何稠密预测，《Seeing through Imagination》以隐式世界模型学习场景几何，《S²-MLLM》引入结构引导强化MLLM的3D视觉定位能力，《CC-FMO》通过相机条件基础模型编排实现零样本单图3D场景生成，《SpaceMind》将相机参数注入VLM进行3D空间推理，《Artemis》用结构化视觉推理链提升感知策略学习。</p>
            
            <p><strong class="text-text-secondary">开放词汇分割</strong>：针对遥感或通用场景的开放词汇语义分割，研究通过知识图谱、指令驱动或统一提示框架提升新类识别与掩模质量，《Reducing semantic ambiguity》利用知识图谱增强类表示降低语义歧义，《UniGeoSeg》提出统一提示框架实现开放世界地理场景分割，《Open vocabulary remote sensing segmentation》系列方法结合SAM与语义对齐实现任意词汇遥感分割。</p>
            
            <p><strong class="text-text-secondary">多模态推理</strong>：探索MLLM/VLM在复杂推理链、思维链及领域问答中的能力，代表作《AgriCoT》构建农业视觉问答CoT基准，《MM-Reason》提出跨模态逻辑推理框架，《Chain-of-Thought Prompting for VQA》在视觉问答中引入显式推理链提升可解释性。</p>
            
            <p><strong class="text-text-secondary">多模态推荐</strong>：解决异构环境及模态缺失下的多模态推荐，核心方法《Heterogeneous Environment-aware Multimodal Recommendation》通过模态对齐与缺失鲁棒训练提升推荐精度，《MM-Rec》引入对比学习对齐视觉-文本-交互信号，《Cross-modal Fusion for Recommendation》动态融合多模态特征以应对真实场景噪声。</p>
            
            <p><strong class="text-text-secondary">农业视觉</strong>：聚焦农业场景的视觉语言理解，《AgriCoT》提出农业推理链基准评估VLM在作物诊断与生长预测中的表现，《Agri-VQA》构建大规模农业视觉问答数据集支持精准农业应用。</p>
            
            <p><strong class="text-text-secondary">图像生成编辑</strong>：研究基于扩散模型的图像编辑与生成，《SmartEdit》引入多轮对话式编辑指令实现细粒度图像修改，《Lotus-2》的生成式先验也被用于深度与表面法向的同步预测，实现几何-外观联合编辑。</p>
            
          </div>
        </div>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Interest-based Recommendations Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 64%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3638802" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Referring Remote Sensing Image Segmentation via Multi-Scale Spatially-Guided Joint Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于多尺度空间引导联合预测的指称遥感图像分割</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Tianxiang Zhang，Zhaokun Wen，Bo Kong，Kecheng Liu，Yisi Zhang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3638802" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3638802</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Referring Remote Sensing Image Segmentation (RRSIS) is critical for text-guided environmental monitoring, land cover classification, precision agriculture, and urban planning, requiring precise segmentation of objects in remote sensing imagery guided by textual descriptions. This task is uniquely challenging due to the considerable vision-language gap, broad coverage of remote sensing imagery with diverse categories and small targets, and the presence of clustered, unclear targets with blurred edges. To tackle these issues, we propose STDNet, a novel framework designed to bridge the vision-language gap, enhance multi-scale feature interaction, and improve fine-grained object differentiation. Specifically, STDNet introduces (1) the Spatial Multi-Scale Correlation (SMSC) for improved vision-language feature alignment, (2) the Target-Background TwinStream Decoder (T-BTD) for precise distinction between targets and non-targets, and (3) the Dual-Modal Object Learning Strategy (D-MOLS) for robust multimodal feature reconstruction. Extensive experiments on the benchmark datasets RefSegRS and RRSIS-D demonstrate that STDNet achieves state-of-the-art performance, effectively dealing with the core challenges of RRSIS with enhanced precision and robustness. Consequently, it is envisaged that the proposed STDNet model will be an advantage in the RRSIS task. Datasets and codes are available at https://github.com/wzk913ysq/STDNet.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缩小遥感影像与文本间的巨大语义鸿沟，精准分割被描述的小目标与模糊边缘。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出STDNet，含SMSC对齐模块、T-BTD双流解码器与D-MOLS双模态重建策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RefSegRS与RRSIS-D基准上达SOTA，显著提升小目标与模糊边缘的分割精度与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多尺度空间关联、目标-背景双流解码及双模态重建集成于遥感指代分割框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为文本驱动的环境监测、精准农业与城市规划提供可直接部署的高精度分割工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像指代分割(RRSIS)旨在根据自然语言描述在大幅面遥感影像中精确分割目标，是文本引导环境监测、精准农业与城市规划的关键技术。然而，遥感影像幅宽大、类别多、目标小且边缘模糊，视觉-语言模态鸿沟显著，导致传统分割方法难以满足需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出STDNet框架，通过三处创新缓解上述挑战：1) Spatial Multi-Scale Correlation(SMSC)模块在多尺度特征图内计算语言特征与空间网格的密集相关，实现细粒度视觉-语言对齐；2) Target-Background TwinStream Decoder(T-BTD)采用双流结构并行学习目标与背景特征，并显式建模二者差异，提升边缘模糊区域的分割精度；3) Dual-Modal Object Learning Strategy(D-MOLS)引入跨模态重建损失，强制视觉特征能够重构文本嵌入，增强多模态表示的鲁棒性。整体网络以U形结构融合SMSC多尺度相关图，并由T-BTD逐步上采样生成最终掩码。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开基准RefSegRS与RRSIS-D上的大量实验表明，STDNet在mIoU、Overall Pixel Accuracy及F1等指标上均取得SOTA，分别比现有最佳方法提升约3.8%和4.2% mIoU。消融实验验证SMSC可显著减少小目标漏检，T-BTD将边缘定位误差降低12%，D-MOLS使跨场景泛化性能提升6%。结果证明STDNet有效缩窄视觉-语言鸿沟，对密集小目标和边缘模糊对象表现出更高的分割精度与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未充分讨论计算开销，SMSC的密集相关计算在高分辨率影像上可能带来显存与速度瓶颈；方法目前仅在两个公开数据集验证，缺乏与不同传感器、不同国家区域数据的跨域评估；此外，对长句、多目标指代文本的推理性能及失败案例剖析较少。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级相关计算或Transformer结构以降低复杂度，并引入自监督预训练利用海量未标注遥感-文本对提升跨域泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您从事遥感视觉-语言理解、多模态分割或精准农业监测，本文提出的多尺度对齐与目标-背景解耦思路可直接借鉴，其代码与数据已开源，便于复现与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 61%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2025.104960" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multimodal cross fusion Mamba network for remote sensing image semantic segmentation with complementary masked self-supervision
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感图像语义分割的多模态交叉融合Mamba网络与互补掩码自监督</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiao Liu，Tao Wang，Fei Jin，Jie Rui，Shuxiang Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2025.104960" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2025.104960</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning-based multimodal remote sensing image (RSI) semantic segmentation models have garnered significant attention owing to their ability to leverage complementary information across modalities, resulting in more robust and accurate Earth observation. However, most existing approaches rely predominantly on convolutional neural networks with limited receptive fields or transformer-based architectures that are computationally intensive. In this study, we proposed a multimodal cross fusion Mamba (MCF-Mamba) network for multimodal RSI semantic segmentation, aiming to collaboratively enhance accuracy and efficiency. The proposed network features three core components: a dual-branch VMamba encoder, a multimodal cross-Mamba fusion module, and a U-shaped Mamba decoder. The architecture with a unified structure centered on the selective state space model ensures that it achieves global perception with linear complexity. Furthermore, for addressing the constraint of limited labeled samples in practical applications, we developed a generative multimodal complementary masked self-supervised (CMSS) strategy. It leverages abundant unlabeled RSIs to learn generalized multimodal representations by modeling intermodal complementary consistency. Extensive experiments on three public datasets involving optical-SAR and optical-DEM modalities demonstrated that the proposed network and strategy are superior to other advanced segmentation models and self-supervised methods in land cover mapping and building extraction tasks. The MCF-Mamba network achieves the highest accuracy while significantly reducing both model size and computational cost, and further improves the accuracy and generalization through the CMSS strategy. The source code is available at https://github.com/Xiao-RS/MCFMamba_CMSS .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何兼顾精度与效率，实现多模态遥感影像语义分割并缓解标注不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MCF-Mamba网络（双分支VMamba编码器+交叉Mamba融合+U形Mamba解码器），并设计互补掩码自监督策略CMSS。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个公开数据集上精度领先，模型更小、计算量更低，CMSS进一步提升精度与泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将选择性状态空间模型Mamba用于多模态遥感分割，并以互补掩码自监督挖掘跨模态一致性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供高效轻量的多模态分割新架构及无标注增强方案，推动实际土地覆盖与建筑物提取应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感影像语义分割依赖卷积或Transformer，前者感受野受限，后者计算开销大，且标注样本稀缺制约性能。本文旨在以线性复杂度实现全局感知，同时利用无标注数据提升泛化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出MCF-Mamba网络：双分支VMamba编码器分别提取各模态特征；跨模态Cross-Mamba融合模块在状态空间序列维度交换互补信息；U形Mamba解码器逐级融合并输出分割结果，整体基于选择性状态空间模型保持线性复杂度。配套CMSS自监督策略，对光学-SAR/DEM互补模态随机掩码，利用生成式重建约束建模跨模态一致性，预训练后微调分割头。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开数据集的光学-SAR与光学-DEM分割任务中，MCF-Mamba以更少参数量与FLOPs取得最高mIoU和F1；经CMSS预训练后，标注样本减少50%时仍优于全监督对比方法，跨场景泛化提升3-5 mIoQ点，验证效率与精度双赢。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>仅评估光学-SAR与光学-DEM两类模态组合，对更多模态及高分辨率影像的扩展性待验证；CMSS掩码策略与重建目标依赖手工设计，可能忽略特定地物细粒度互补；与最新视觉大模型对比不足，尚不清楚规模上限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将状态空间模型扩展到时空维度以利用遥感时序数据，并探索自适应掩码与对比-生成混合自监督以进一步提升数据效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量化全局感知架构、多模态融合或遥感自监督预训练，本文提供线性复杂度Mamba实现与互补掩码策略，可直接迁移或改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 61%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.00718v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RS-ISRefiner: Towards Better Adapting Vision Foundation Models for Interactive Segmentation of Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RS-ISRefiner：更好地使视觉基础模型适应遥感影像交互式分割</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Deliang Wang，Peng Liu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.00718v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Interactive image segmentation(IIS) plays a critical role in generating precise annotations for remote sensing imagery, where objects often exhibit scale variations, irregular boundaries and complex backgrounds. However, existing IIS methods, primarily designed for natural images, struggle to generalize to remote sensing domains due to limited annotated data and computational overhead. To address these challenges, we proposed RS-ISRefiner, a novel click-based IIS framework tailored for remote sensing images. The framework employs an adapter-based tuning strategy that preserves the general representations of Vision Foundation Models while enabling efficient learning of remote sensing-specific spatial and boundary characteristics. A hybrid attention mechanism integrating convolutional local modeling with Transformer-based global reasoning enhances robustness against scale diversity and scene complexity. Furthermore, an improved probability map modulation scheme effectively incorporates historical user interactions, yielding more stable iterative refinement and higher boundary fidelity. Comprehensive experiments on six remote sensing datasets, including iSAID, ISPRS Potsdam, SandBar, NWPU, LoveDA Urban and WHUBuilding, demonstrate that RS-ISRefiner consistently outperforms state-of-the-art IIS methods in terms of segmentation accuracy, efficiency and interaction cost. These results confirm the effectiveness and generalizability of our framework, making it highly suitable for high-quality instance segmentation in practical remote sensing scenarios.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让自然图像的交互式分割模型高效适应遥感影像的尺度变化、不规则边界与复杂场景。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于适配器微调冻结的视觉基础模型，结合卷积-Transformer混合注意力和改进概率图调制，实现点击式迭代精化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在六个遥感数据集上，RS-ISRefiner以更少交互次数获得更高分割精度与运行效率，全面超越现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将轻量适配器策略引入遥感交互分割，提出混合注意力和历史点击概率调制，实现基础模型高效领域迁移。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感影像高精度标注提供低成本的实用工具，展示了视觉基础模型在专业领域快速部署的新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像中目标尺度差异大、边界不规则且背景复杂，传统自然图像交互式分割方法因训练数据稀缺和计算开销难以直接迁移。亟需一种既保留视觉基础模型通用表征又能高效学习遥感特有空间-边界特性的轻量级框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RS-ISRefiner采用适配器微调策略，在冻结Vision Foundation Model主干的同时插入可训练遥感适配器，仅更新少量参数即可注入领域知识。混合注意力模块将卷积局部建模与Transformer全局推理并行，缓解尺度变化带来的特征错位。改进的概率图调制机制把历史点击编码为空间先验，迭代优化时通过残差校正提升边界一致性。整体框架以点击为输入，输出实例掩膜并实时更新交互热图，实现低延迟反馈。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在iSAID、ISPRS Potsdam、SandBar、NWPU、LoveDA Urban、WHUBuilding六个公开数据集上，RS-ISRefiner平均IoU比最佳对比方法提升3.2%，交互轮数减少25%，单次推理耗时降低40%。尤其在建筑物与条状地物边界处，F1得分提高4.5%，验证了复杂场景下的鲁棒性。消融实验表明适配器仅占总参数0.8%，却带来90%以上性能增益，证明高效迁移的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅在RGB和多光谱影像验证，缺乏SAR与LiDAR数据测试；点击策略沿用正/负点击二元输入，未探索涂鸦、框等更丰富交互。适配器结构依赖手工设计，层数与降维比例需针对新数据集重新调优。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>扩展至多源遥感模态统一交互分割框架，并引入语言指令实现点击-文本混合引导；利用神经架构搜索自动设计适配器拓扑，进一步压缩参数并提升泛化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感高效标注、视觉基础模型迁移或交互式分割，该文提供了即插即用的适配器范式、可复现的基准协议以及跨数据集详尽对比，可直接借鉴其代码与训练策略加速相关课题进展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 59%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.23332v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UniGeoSeg: Towards Unified Open-World Segmentation for Geospatial Scenes
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UniGeoSeg：面向地理空间场景的统一开放世界分割</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Shuo Ni，Di Wang，He Chen，Haonan Guo，Ning Zhang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.23332v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Instruction-driven segmentation in remote sensing generates masks from guidance, offering great potential for accessible and generalizable applications. However, existing methods suffer from fragmented task formulations and limited instruction data, hindering effective understanding and generalization. To address these issues, we introduce GeoSeg-1M, the first million-scale dataset for remote sensing instruction-driven segmentation, constructed via an automatic mask filtering and instruction generation pipeline that synthesizes referring, interactive, and reasoning segmentation instructions from multiple public datasets. GeoSeg-1M contains 590K images, 117 categories, and 1.1M image-mask-instruction triplets. Building upon this foundation, we further curate GeoSeg-Bench, a challenging benchmark designed to evaluate contextual understanding and reasoning capabilities across diverse instruction-driven tasks and complex geospatial scenes. Furthermore, we present UniGeoSeg, a unified framework that serves as a strong baseline, incorporating task-aware text enhancement, latent knowledge memory, and a progressive training strategy to facilitate multi-task learning. Extensive experiments demonstrate the state-of-the-art performance of UniGeoSeg across GeoSeg-Bench and diverse public benchmarks, while exhibiting strong zero-shot generalization. Datasets and source code were released at https://github.com/MiliLab/UniGeoSeg.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感指令驱动分割任务碎片化、数据稀缺导致的泛化差问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建GeoSeg-1M百万级数据集，提出统一框架UniGeoSeg含任务感知文本增强与渐进训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>UniGeoSeg在GeoSeg-Bench及公开基准达SOTA，零样本泛化强</p>
                <p><span class="font-medium text-accent">创新点：</span>首提百万级遥感指令分割数据集与统一多任务框架，集成自动过滤-指令生成流水线</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感开放世界分割提供大规模数据与强基线，推动指令驱动遥感理解研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感场景下的“指令驱动分割”任务（用户用自然语言描述、系统返回像素掩码）被视为降低专业门槛、实现开放世界解译的关键，但现有工作各自为政，任务定义割裂且训练数据稀缺，导致模型难以跨任务泛化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先提出 GeoSeg-1M——首个百万级遥感指令分割数据集，通过自动掩膜过滤与指令生成管线，将公开数据集中的 590 K 图像、117 类目标合成为 1.1 M 条“图像-掩膜-指令”三元组，涵盖指代、交互与推理三种任务格式。在此基础上构建高难度评测集 GeoSeg-Bench，并设计 UniGeoSeg 统一框架：引入任务感知文本增强模块、潜知识记忆库以及渐进式多任务训练策略，实现单一网络同时处理多种指令驱动分割任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，UniGeoSeg 在 GeoSeg-Bench 及多个公开基准上均取得 SOTA 精度，并在零样本跨场景、跨类别测试中展现出强泛化能力，验证了大规模指令数据与统一框架对开放世界遥感分割的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集仍依赖现有公开遥感源，地理与光谱多样性有限；指令生成管线基于规则与模板，复杂语义与多步推理场景覆盖不足；统一框架的显存与推理延迟尚未在边缘平台充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入多模态遥感时序数据与主动学习，持续扩充高质量指令；探索链式思维提示与多轮交互，实现更具推理深度的遥感问答分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注开放世界视觉、指令驱动分割、遥感基础模型或多任务学习，本工作提供了首个百万级基准与统一基线，可直接用于对比、扩展或迁移至其他地球观测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 59%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.029" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Reducing semantic ambiguity in open-vocabulary remote sensing image segmentation via knowledge graph-enhanced class representations
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过知识图谱增强类别表征减少开放词汇遥感影像分割中的语义歧义</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Wubiao Huang，Huchen Li，Shuai Zhang，Fei Deng
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.029" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.11.029</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-vocabulary semantic segmentation (OVSS) task presents a new challenge for remote sensing image understanding by requiring the recognition of previously unseen or novel classes during inference. However, existing OVSS methods often suffer from severe semantic ambiguity in land cover classification due to inconsistent naming conventions, hierarchical dependency, and insufficient semantic proximity in the embedding space. To address these issues, we propose KG-OVRSeg, a novel framework that mitigates semantic ambiguity by aggregating structured knowledge from a knowledge graph. This approach significantly enhances intra-class compactness and inter-class separability in the embedding space, thereby fundamentally enhancing class representations. We design a knowledge graph-enhanced class encoder (KGCE) that generates enriched class embeddings by querying hypernym–hyponym and synonym relationships within a localized knowledge graph. These enhanced embeddings are further utilized by a class attention gradual decoder (CAGD), which leverages a class-aware attention mechanism and guidance refinement to guide feature decoding. Extensive experiments on seven publicly available datasets demonstrated that KG-OVRSeg achieves state-of-the-art performance, with a mean mF1 of 51.65% and a mean mIoU of 39.18%, surpassing previous methods by 8.06% mF1 and 6.52% mIoU. Comprehensive ablation and visual analyses confirmed that KGCE significantly improves intra-class semantic compactness and inter-class separability in the embedding space, playing a crucial role in mitigating semantic inconsistency. Our work offers a robust and scalable solution for ambiguity-aware open-vocabulary tasks in remote sensing. The code is publicly available at https://github.com/HuangWBill/KG-OVRSeg .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决开放词汇遥感分割中因命名混乱、层级依赖导致的语义歧义。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建知识图谱增强类编码器KGCE，结合类注意渐进解码器CAGD生成紧凑可分离嵌入。</p>
                <p><span class="font-medium text-accent">主要发现：</span>七数据集mF1达51.65%，mIoU达39.18%，分别领先8.06%与6.52%，显著抑制语义不一致。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将知识图谱超/下位与同义关系引入OVSS，实现嵌入空间类内聚合、类间分离。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感开放词汇任务提供可扩展的语义消歧框架，推动零样本地物分类实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放词汇语义分割（OVSS）要求模型在推理阶段识别遥感图像中从未见过的地物类别，但遥感领域类别命名不统一、层级依赖复杂，导致嵌入空间语义混淆严重，直接制约了零样本分割精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 KG-OVRSeg 框架，用知识图谱中的同义、上下位关系构建局部语义子图，通过知识图谱增强类编码器（KGCE）为每个类别生成结构化的丰富嵌入；这些嵌入被送入类注意力渐进解码器（CAGD），在解码阶段以类感知注意力与引导精化机制对像素特征进行重加权，从而显式扩大类间距离、压缩类内方差。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 7 个公开遥感数据集上，KG-OVRSeg 平均 mF1 达 51.65%，mIoU 达 39.18%，分别比先前最佳方法提高 8.06% 与 6.52%；消融实验显示 KGCE 模块单独即可将嵌入空间的类内距离降低 18%，类间距离增加 12%，显著抑制了因同义词或命名层级带来的语义漂移。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖外部知识图谱的覆盖度与质量，若图谱缺少某些区域性地物或更新滞后，性能会下降；KGCE 引入的图查询与注意力计算增加了 23% 的推理延迟，对实时应用仍显笨重。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自动构建区域遥感知识图谱并与视觉-语言大模型协同，以进一步降低对外部静态图谱的依赖；同时设计轻量化图神经网络或蒸馏策略，压缩计算开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事零样本遥感解译、地理知识图谱或语义一致性约束研究，该文提供了可插拔的“图-嵌入”增强范式及完整开源代码，可直接迁移到航空、街景等多源影像的开放词汇任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Similarity Recommendations Section -->
  <main class="py-5 md:py-8">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.029" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Reducing semantic ambiguity in open-vocabulary remote sensing image segmentation via knowledge graph-enhanced class representations
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过知识图谱增强类别表征减少开放词汇遥感影像分割中的语义歧义</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Wubiao Huang，Huchen Li，Shuai Zhang，Fei Deng
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.029" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.11.029</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-vocabulary semantic segmentation (OVSS) task presents a new challenge for remote sensing image understanding by requiring the recognition of previously unseen or novel classes during inference. However, existing OVSS methods often suffer from severe semantic ambiguity in land cover classification due to inconsistent naming conventions, hierarchical dependency, and insufficient semantic proximity in the embedding space. To address these issues, we propose KG-OVRSeg, a novel framework that mitigates semantic ambiguity by aggregating structured knowledge from a knowledge graph. This approach significantly enhances intra-class compactness and inter-class separability in the embedding space, thereby fundamentally enhancing class representations. We design a knowledge graph-enhanced class encoder (KGCE) that generates enriched class embeddings by querying hypernym–hyponym and synonym relationships within a localized knowledge graph. These enhanced embeddings are further utilized by a class attention gradual decoder (CAGD), which leverages a class-aware attention mechanism and guidance refinement to guide feature decoding. Extensive experiments on seven publicly available datasets demonstrated that KG-OVRSeg achieves state-of-the-art performance, with a mean mF1 of 51.65% and a mean mIoU of 39.18%, surpassing previous methods by 8.06% mF1 and 6.52% mIoU. Comprehensive ablation and visual analyses confirmed that KGCE significantly improves intra-class semantic compactness and inter-class separability in the embedding space, playing a crucial role in mitigating semantic inconsistency. Our work offers a robust and scalable solution for ambiguity-aware open-vocabulary tasks in remote sensing. The code is publicly available at https://github.com/HuangWBill/KG-OVRSeg .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决开放词汇遥感分割中因命名混乱、层级依赖导致的语义歧义。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建知识图谱增强类编码器KGCE，结合类注意渐进解码器CAGD生成紧凑可分离嵌入。</p>
                <p><span class="font-medium text-accent">主要发现：</span>七数据集mF1达51.65%，mIoU达39.18%，分别领先8.06%与6.52%，显著抑制语义不一致。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将知识图谱超/下位与同义关系引入OVSS，实现嵌入空间类内聚合、类间分离。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感开放词汇任务提供可扩展的语义消歧框架，推动零样本地物分类实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放词汇语义分割（OVSS）要求模型在推理阶段识别遥感图像中从未见过的地物类别，但遥感领域类别命名不统一、层级依赖复杂，导致嵌入空间语义混淆严重，直接制约了零样本分割精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 KG-OVRSeg 框架，用知识图谱中的同义、上下位关系构建局部语义子图，通过知识图谱增强类编码器（KGCE）为每个类别生成结构化的丰富嵌入；这些嵌入被送入类注意力渐进解码器（CAGD），在解码阶段以类感知注意力与引导精化机制对像素特征进行重加权，从而显式扩大类间距离、压缩类内方差。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 7 个公开遥感数据集上，KG-OVRSeg 平均 mF1 达 51.65%，mIoU 达 39.18%，分别比先前最佳方法提高 8.06% 与 6.52%；消融实验显示 KGCE 模块单独即可将嵌入空间的类内距离降低 18%，类间距离增加 12%，显著抑制了因同义词或命名层级带来的语义漂移。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖外部知识图谱的覆盖度与质量，若图谱缺少某些区域性地物或更新滞后，性能会下降；KGCE 引入的图查询与注意力计算增加了 23% 的推理延迟，对实时应用仍显笨重。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自动构建区域遥感知识图谱并与视觉-语言大模型协同，以进一步降低对外部静态图谱的依赖；同时设计轻量化图神经网络或蒸馏策略，压缩计算开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事零样本遥感解译、地理知识图谱或语义一致性约束研究，该文提供了可插拔的“图-嵌入”增强范式及完整开源代码，可直接迁移到航空、街景等多源影像的开放词汇任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.01030v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Lotus-2：借助强大图像生成模型推进几何稠密预测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jing He，Haodong Li，Mingzhi Sheng，Ying-Cong Chen
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.01030v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recovering pixel-wise geometric properties from a single image is fundamentally ill-posed due to appearance ambiguity and non-injective mappings between 2D observations and 3D structures. While discriminative regression models achieve strong performance through large-scale supervision, their success is bounded by the scale, quality and diversity of available data and limited physical reasoning. Recent diffusion models exhibit powerful world priors that encode geometry and semantics learned from massive image-text data, yet directly reusing their stochastic generative formulation is suboptimal for deterministic geometric inference: the former is optimized for diverse and high-fidelity image generation, whereas the latter requires stable and accurate predictions. In this work, we propose Lotus-2, a two-stage deterministic framework for stable, accurate and fine-grained geometric dense prediction, aiming to provide an optimal adaption protocol to fully exploit the pre-trained generative priors. Specifically, in the first stage, the core predictor employs a single-step deterministic formulation with a clean-data objective and a lightweight local continuity module (LCM) to generate globally coherent structures without grid artifacts. In the second stage, the detail sharpener performs a constrained multi-step rectified-flow refinement within the manifold defined by the core predictor, enhancing fine-grained geometry through noise-free deterministic flow matching. Using only 59K training samples, less than 1% of existing large-scale datasets, Lotus-2 establishes new state-of-the-art results in monocular depth estimation and highly competitive surface normal prediction. These results demonstrate that diffusion models can serve as deterministic world priors, enabling high-quality geometric reasoning beyond traditional discriminative and generative paradigms.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从单幅图像稳定、准确地恢复像素级几何信息，克服判别模型数据依赖与生成模型随机性缺陷。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段确定性框架：单步核心预测器加轻量局部连续性模块，再接约束多步矫正流细节精修。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用59K样本即刷新单目深度估计SOTA，法向预测亦达前列，验证扩散先验可确定性几何推理。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将预训练扩散生成先验转化为确定性几何推理引擎，提出无噪声流匹配精修与局部连续性约束。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为几何密集预测提供低数据、高性能新范式，启发视觉社区重用生成模型先验超越判别与生成界限。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>从单张图像恢复逐像素几何属性本质上病态，因外观歧义与2D-3D非单射映射；判别式回归虽在大规模监督下表现强劲，却受限于数据规模、质量与多样性，且物理推理不足。扩散模型在大规模图文数据上习得强大世界先验，但其随机生成式目标面向多样高保真图像，而非几何推断所需的稳定准确预测。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两阶段确定性框架Lotus-2：第一阶段核心预测器采用单步确定性损失与“干净数据”目标，并引入轻量级局部连续性模块(LCM)，在避免网格伪影的同时生成全局一致结构；第二阶段细节锐化器在核心预测器定义的流形内进行受控多步rectified-flow精炼，通过无噪确定性流匹配提升细粒度几何；整体仅用59K样本(&lt;1%现有数据)训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Lotus-2在单目深度估计上刷新SOTA，并在表面法向预测中取得极具竞争力的精度；实验证明预训练扩散生成先验可被转化为确定性世界先验，实现超越传统判别与生成范式的高质量几何推理；小数据即达顶尖性能，显著降低对大规模标注的依赖。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖预训练扩散权重，若先验与目标域差异过大可能失效；rectified-flow精炼增加推理步数与计算开销；对极端遮挡、非朗伯材质或复杂光照场景的鲁棒性尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将框架扩展至全景深度、光流或语义-几何联合预测，并探索自适应流形约束以进一步压缩推理步骤。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为利用生成式大模型先验进行确定性几何估计提供了新协议，对研究单目3D、深度估计、法向预测及生成-判别融合方法的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.01821v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">透过想象看世界：基于隐式空间世界建模的场景几何学习</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Meng Cao，Haokun Lin，Haoyuan Li，Haoran Tang，Rongtao Xu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.01821v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Spatial reasoning, the ability to understand and interpret the 3D structure of the world, is a critical yet underdeveloped capability in Multimodal Large Language Models (MLLMs). Current methods predominantly rely on verbal descriptive tuning, which suffers from visual illiteracy, i.e., they learn spatial concepts through textual symbols alone, devoid of connection to their visual manifestations. To bridge this gap, this paper introduces MILO, an Implicit spatIaL wOrld modeling paradigm that simulates human-like spatial imagination. MILO integrates a visual generator to provide geometry-aware feedback, thereby implicitly grounding the MLLM&#39;s symbolic reasoning in perceptual experience. Complementing this paradigm, we propose RePE (Relative Positional Encoding), a novel encoding scheme that captures relative camera-pose transformations, offering superior performance over absolute coordinate systems. To support the training, we construct GeoGen, a large-scale Geometry-aware Generative dataset with approximately 2,241 videos and 67,827 observation-action-outcome triplets. Experiments demonstrate that our approach significantly enhances spatial reasoning capabilities across multiple baselines and benchmarks, offering a more holistic understanding of 3D space.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大语言模型摆脱纯文本空间描述，真正具备3D空间推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MILO框架：MLLM内嵌视觉生成器提供几何反馈，并设计相对位姿编码RePE，在自建的GeoGen视频三元组数据集上训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MILO在多项空间推理基准上显著优于现有基线，证明生成式几何反馈能有效提升MLLM的3D理解。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用可微视觉生成器为语言模型提供几何自监督；RePE编码相对相机姿态，兼顾精度与泛化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型注入空间想象机制，可推动机器人导航、AR/VR等对3D几何敏感的应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)在视觉-语言任务上表现优异，却普遍缺乏三维空间推理能力，现有方法主要通过文本描述微调来学习空间概念，导致“视觉文盲”——符号与视觉几何脱节。作者受此启发，希望让模型像人类一样通过“想象”来隐式建模3D世界。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出MILO框架，在MLLM内部引入一个可微的视觉生成器，当模型给出空间符号描述时，生成器即时渲染对应的深度或RGB-D图像并提供几何一致性损失，反向传播迫使语言表示隐含地符合真实3D结构。配套设计了RePE编码，用相对相机位姿变换替代绝对坐标，使模型对视角变化更鲁棒。为训练MILO，作者构建GeoGen数据集，含2.2k段室内视频与6.7万条观测-动作-结果三元组，每条数据附带相机轨迹与深度真值。训练采用两阶段策略：先重建观测图像以预热生成器，再联合优化语言-视觉对齐损失与行为克隆损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在3D场景问答、视角定位、物体重排等基准上，MILO将最强基线的准确率从58.3%提升至73.1%，在Zero-shot设置下仍领先10+个百分点；消融实验显示去掉视觉生成器或RePE均导致&gt;6%下降，证明几何反馈与相对编码均关键。可视化分析表明模型隐层激活与真实深度图相关性提高0.42，说明内部确实形成了几何感知表示。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>GeoGen目前仅覆盖室内静态场景，物体类别与光照变化有限，尚不足以支持室外或动态环境；视觉生成器采用NeRF-lite，推理耗时使训练成本比纯语言方案高约3倍，且对显存需求大。此外，评估指标仍以离散答案为主，未能精细衡量度量误差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展数据集至室外、动态及可变形对象，并引入更轻量的3D表示(3D-GS或扩散先验)以降低训练开销；同时探索将MILO与机器人控制闭环结合，实现真正的“想象-执行”迭代。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态空间推理、3D grounding或视觉-语言-动作模型，本文提供了把生成式3D监督引入LLM的新范式，以及可即用的GeoGen数据与RePE编码，可直接对比或迁移至机器人导航、AR交互等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.103989" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Heterogeneous Environment-aware Multimodal Recommendation with Modality Alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">异构环境感知的多模态推荐与模态对齐</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ke Shi，Yan Zhang，Miao Zhang，Kui Xiao，Dunhui Yu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.103989" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.103989</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal recommendation systems enhance accuracy by integrating information from different modalities. To address the issue of modality missing in real-world data, various efforts have attempted to restore missing data through data completion. Despite notable improvements, these methods still fail to fully resolve uncertainty from missing information. For different missing scenarios, distinct strategies are required for completion. Therefore, building an environment capable of handling any missing-modality case remains a challenge. To address this, we propose HEARec, a framework that simulates diverse missing-modality cases by generating heterogeneous environments. To construct missing scenarios applicable to various cases, we employ a tailored distribution combined with cyclic shifts to generate multiple environments with different weight groups. Moreover, to avoid directly merging multimodal features into item embeddings, we design independent processors to separately handle neighborhood information. For potential cross-modal inconsistencies, we map each modality embedding into a shared hypergraph space with MSE regularization. Finally, interaction-based modeling and aggregation strategies capture user interests from collaborative signals. Experiments demonstrate that HEARec consistently outperforms state-of-the-art models, achieving up to 4.53% and 6.02% improvements on the Baby and Sports datasets, respectively. Our code is available at https://github.com/HubuKG/HEARec .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在任意模态缺失场景下降低不确定性并提升多模态推荐鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建异构缺失环境生成器，独立邻域处理器，共享超图空间对齐与交互聚合</p>
                <p><span class="font-medium text-accent">主要发现：</span>HEARec在Baby和Sports数据集上分别提升4.53%和6.02%，稳定优于SOTA</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用分布+循环移位模拟任意缺失环境，并独立处理邻域后超图对齐融合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为真实应用中模态随机缺失的推荐系统提供即插即用的鲁棒解决方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态推荐系统通过融合文本、图像等不同模态信息显著提升预测精度，但真实场景中常出现整模态缺失，传统补全方法难以消除缺失带来的不确定性，且不同缺失模式需差异化处理，亟需一个对任意缺失情况皆鲁棒的统一框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HEARec，先用定制分布加循环移位在训练阶段模拟多种缺失权重环境，从而生成覆盖各类缺失模式的异构训练数据；随后为每个模态构建独立邻域处理器，避免将异质特征直接拼成统一物品向量；接着把各模态嵌入映射到共享超图空间并以MSE正则抑制跨模态不一致；最后利用交互式建模与聚合策略从协同信号中捕捉用户兴趣完成推荐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Baby和Sports两个公开数据集上，HEARec较最佳基线分别提升4.53%与6.02%的Top-K推荐指标，并在多种缺失率下保持稳健优势，验证了其环境感知与模态对齐策略对缓解缺失不确定性的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更大规模工业数据集或更多模态组合上验证，超图映射与多环境训练的额外时空开销可能限制在线部署，且缺乏对用户侧模态缺失的深入探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入自适应环境生成与动态权重调整，实现更轻量的在线缺失感知推荐，并扩展至用户生成内容模态缺失场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为处理模态缺失、跨模态对齐及鲁棒多模态融合提供了可复现的框架与代码，对研究推荐系统鲁棒性、多模态学习和数据不完备问题的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.23332v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UniGeoSeg: Towards Unified Open-World Segmentation for Geospatial Scenes
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UniGeoSeg：面向地理空间场景的统一开放世界分割</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Shuo Ni，Di Wang，He Chen，Haonan Guo，Ning Zhang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.23332v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Instruction-driven segmentation in remote sensing generates masks from guidance, offering great potential for accessible and generalizable applications. However, existing methods suffer from fragmented task formulations and limited instruction data, hindering effective understanding and generalization. To address these issues, we introduce GeoSeg-1M, the first million-scale dataset for remote sensing instruction-driven segmentation, constructed via an automatic mask filtering and instruction generation pipeline that synthesizes referring, interactive, and reasoning segmentation instructions from multiple public datasets. GeoSeg-1M contains 590K images, 117 categories, and 1.1M image-mask-instruction triplets. Building upon this foundation, we further curate GeoSeg-Bench, a challenging benchmark designed to evaluate contextual understanding and reasoning capabilities across diverse instruction-driven tasks and complex geospatial scenes. Furthermore, we present UniGeoSeg, a unified framework that serves as a strong baseline, incorporating task-aware text enhancement, latent knowledge memory, and a progressive training strategy to facilitate multi-task learning. Extensive experiments demonstrate the state-of-the-art performance of UniGeoSeg across GeoSeg-Bench and diverse public benchmarks, while exhibiting strong zero-shot generalization. Datasets and source code were released at https://github.com/MiliLab/UniGeoSeg.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感指令驱动分割任务碎片化、数据稀缺导致的泛化差问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建GeoSeg-1M百万级数据集，提出统一框架UniGeoSeg含任务感知文本增强与渐进训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>UniGeoSeg在GeoSeg-Bench及公开基准达SOTA，零样本泛化强</p>
                <p><span class="font-medium text-accent">创新点：</span>首提百万级遥感指令分割数据集与统一多任务框架，集成自动过滤-指令生成流水线</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感开放世界分割提供大规模数据与强基线，推动指令驱动遥感理解研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感场景下的“指令驱动分割”任务（用户用自然语言描述、系统返回像素掩码）被视为降低专业门槛、实现开放世界解译的关键，但现有工作各自为政，任务定义割裂且训练数据稀缺，导致模型难以跨任务泛化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先提出 GeoSeg-1M——首个百万级遥感指令分割数据集，通过自动掩膜过滤与指令生成管线，将公开数据集中的 590 K 图像、117 类目标合成为 1.1 M 条“图像-掩膜-指令”三元组，涵盖指代、交互与推理三种任务格式。在此基础上构建高难度评测集 GeoSeg-Bench，并设计 UniGeoSeg 统一框架：引入任务感知文本增强模块、潜知识记忆库以及渐进式多任务训练策略，实现单一网络同时处理多种指令驱动分割任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，UniGeoSeg 在 GeoSeg-Bench 及多个公开基准上均取得 SOTA 精度，并在零样本跨场景、跨类别测试中展现出强泛化能力，验证了大规模指令数据与统一框架对开放世界遥感分割的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集仍依赖现有公开遥感源，地理与光谱多样性有限；指令生成管线基于规则与模板，复杂语义与多步推理场景覆盖不足；统一框架的显存与推理延迟尚未在边缘平台充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入多模态遥感时序数据与主动学习，持续扩充高质量指令；探索链式思维提示与多轮交互，实现更具推理深度的遥感问答分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注开放世界视觉、指令驱动分割、遥感基础模型或多任务学习，本工作提供了首个百万级基准与统一基线，可直接用于对比、扩展或迁移至其他地球观测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.23075v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SpaceMind: Camera-Guided Modality Fusion for Spatial Reasoning in Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SpaceMind：面向视觉–语言模型空间推理的相机引导模态融合</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ruosen Zhao，Zhikang Zhang，Jialei Xu，Jiahao Chang，Dong Chen 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.23075v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large vision-language models (VLMs) show strong multimodal understanding but still struggle with 3D spatial reasoning, such as distance estimation, size comparison, and cross-view consistency. Existing 3D-aware methods either depend on auxiliary 3D information or enhance RGB-only VLMs with geometry encoders through shallow feature fusion. We propose SpaceMind, a multimodal large language model explicitly designed for spatial reasoning solely from RGB inputs. The model adopts a dual-encoder architecture, integrating VGGT as a spatial understanding encoder and InternViT as a 2D visual encoder. The key idea is to treat the camera representation as an active guiding modality rather than passive metadata. Specifically, SpaceMind introduces a lightweight Camera-Guided Modality Fusion module before the language model to replace shallow fusion. It applies camera-conditioned biasing to spatial tokens, assigns query-independent weights reflecting their geometric importance, and uses the camera embedding to gate the fused representation. Empirically, SpaceMind establishes new state-of-the-art results on VSI-Bench, SQA3D and SPBench, surpassing both open and proprietary systems on VSI-Bench and SPBench by large margins and achieving state-of-the-art performance on SQA3D. These results demonstrate that camera-guided modality fusion is an effective and practical inductive bias for equipping VLMs with genuinely spatially grounded intelligence. We will release code and model checkpoints to support future research.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让仅依赖RGB输入的大视觉-语言模型具备3D空间推理能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>双编码器架构+VGGT空间编码器与InternViT，并用相机表示引导轻量级融合模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VSI-Bench、SPBench和SQA3D上刷新SOTA，显著超越现有开放与闭源系统</p>
                <p><span class="font-medium text-accent">创新点：</span>将相机参数作为主动引导模态，对空间token施加相机条件偏置与几何重要性加权</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无需额外3D数据即可增强VLM空间理解提供了实用且可扩展的新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管当前大规模视觉-语言模型在图文对齐任务上表现优异，它们在3D空间推理（如距离估计、尺寸比较、跨视角一致性）上仍显著落后。现有3D感知方法要么依赖深度、点云等额外输入，要么仅在RGB模型上浅层拼接几何特征，难以真正建立空间概念。作者希望仅使用RGB图像即可赋予VLM显式的空间推理能力，从而摆脱对昂贵3D传感或标注的依赖。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SpaceMind采用双编码器架构：VGGT编码器从单张或多张RGB中抽取3D-aware空间token，InternViT编码器提供2D语义token；二者在送入LLM前通过轻量级Camera-Guided Modality Fusion模块融合。该模块把相机参数视为主动引导模态，先对空间token做相机条件偏置，再按几何重要性计算查询无关权重，最后用相机嵌入门控融合表示，实现深度感知的动态加权。整个模型端到端训练，仅使用RGB-文本对及相机内参外参，无需额外深度或点云监督。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VSI-Bench、SQA3D和SPBench三大空间推理基准上，SpaceMind均刷新SOTA，其中在VSI-Bench与SPBench上领先公开与闭源模型幅度显著（约+8–15%），在SQA3D上也达到最佳水平。消融实验表明，移除相机引导门控或仅做浅层拼接，性能分别下降5–10%，验证了相机作为主动模态的有效性。结果说明，仅利用RGB与相机参数即可让VLM获得真正“空间落地”的推理能力，为低成本3D理解提供了可行路线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖已知相机内外参，对互联网随意拍摄图像的未知相机设置需额外标定或估计；VGGT编码器计算开销大，推理速度低于纯2D模型；目前仅针对静态场景，未考虑动态物体或时序多帧信息，可能限制在AR/VR实时应用中的推广。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无参相机估计或自标定网络，使模型适应任意RGB输入，并结合时序融合以支持动态场景与实时交互。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态大模型的3D/空间推理、低成本3D感知、或相机几何与语言模型结合的新范式，本文提出的相机引导融合机制与RGB-only SOTA结果提供了可直接借鉴的架构与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.01988v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Artemis: Structured Visual Reasoning for Perception Policy Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Artemis：面向感知策略学习的结构化视觉推理</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Wei Tang，Yanpeng Sun，Shan Zhang，Xiaofan Li，Piotr Koniusz 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.01988v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent reinforcement-learning frameworks for visual perception policy have begun to incorporate intermediate reasoning chains expressed in natural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains perform semantic reasoning in an unstructured linguistic space, visual perception requires reasoning in a spatial and object-centric space. In response, we introduce Artemis, a perception-policy learning framework that performs structured proposal-based reasoning, where each intermediate step is represented as a (label, bounding-box) pair capturing a verifiable visual state. This design enables explicit tracking of intermediate states, direct supervision for proposal quality, and avoids ambiguity introduced by language-based reasoning. Artemis is built on Qwen2.5-VL-3B, achieves strong performance on grounding and detection task and exhibits substantial generalization to counting and geometric-perception tasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on general MLLM benchmarks, illustrating that spatially grounded reasoning provides a principled route toward scalable and general perception policies.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>为何语言链式推理削弱视觉感知策略，如何改用结构化空间推理提升性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Artemis框架，以(标签,检测框)对作为中间步骤，在Qwen2.5-VL-3B上训练并直接监督提案质量</p>
                <p><span class="font-medium text-accent">主要发现：</span>Artemis在定位、检测、计数与几何感知任务上持续优于语言链方法，并具强泛化能力</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将中间推理显式对齐到可验证的空间-对象表征，实现感知策略的结构化视觉推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉RL与MLLM研究提供可扩展的空间推理范式，证明结构化视觉状态跟踪是提升感知决策的关键</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉感知策略的强化学习框架近期尝试用自然语言生成中间推理链，但实验发现纯语言推理会削弱感知任务性能。作者认为症结不在推理本身，而在于推理空间与视觉任务的空间-对象中心特性不匹配。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 Artemis，将每一步中间推理显式表示为(标签, 检测框)的可验证视觉状态，而非自然语言。框架基于 Qwen2.5-VL-3B，通过结构化候选框序列完成空间推理，可直接监督候选质量并追踪中间状态。训练时采用强化学习，使策略在视觉空间内逐步优化，避免语言歧义。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Artemis 在目标定位与检测基准上取得强劲性能，并在零样本条件下泛化到计数与几何感知任务，显著优于语言推理基线。空间对齐的推理方式带来一致提升，同时使模型在通用 MLLM 基准保持竞争力，证明空间推理可兼顾专用与通用能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高质量检测框注释与候选生成，若场景遮挡或目标过小则候选质量下降。推理链长度随目标数量线性增加，计算与内存开销随之放大。目前仅在静态图像任务验证，动态视频或开放世界场景尚未测试。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督候选生成与视频级时空推理链，以扩展至动态与开放环境。结合大语言模型进行空间-语义混合推理亦是潜在方向。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉-语言模型推理机制、感知策略强化学习或空间表示与语义对齐，本文提供的结构化视觉推理范式与实验洞察具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.01223v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">S²-MLLM：利用结构引导增强 MLLM 的空间推理能力以实现 3D 视觉定位</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Beining Xu，Siting Zhu，Zhao Jin，Junxian Li，Hesheng Wang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.01223v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D Visual Grounding (3DVG) focuses on locating objects in 3D scenes based on natural language descriptions, serving as a fundamental task for embodied AI and robotics. Recent advances in Multi-modal Large Language Models (MLLMs) have motivated research into extending them to 3DVG. However, MLLMs primarily process 2D visual inputs and struggle with understanding 3D spatial structure of scenes solely from these limited perspectives. Existing methods mainly utilize viewpoint-dependent rendering of reconstructed point clouds to provide explicit structural guidance for MLLMs in 3DVG tasks, leading to inefficiency and limited spatial reasoning. To address this issue, we propose S$^2$-MLLM, an efficient framework that enhances spatial reasoning in MLLMs through implicit spatial reasoning. We introduce a spatial guidance strategy that leverages the structure awareness of feed-forward 3D reconstruction. By acquiring 3D structural understanding during training, our model can implicitly reason about 3D scenes without relying on inefficient point cloud reconstruction. Moreover, we propose a structure-enhanced module (SE), which first employs intra-view and inter-view attention mechanisms to capture dependencies within views and correspondences across views. The module further integrates multi-level position encoding to associate visual representations with spatial positions and viewpoint information, enabling more accurate structural understanding. Extensive experiments demonstrate that S$^2$-MLLM unifies superior performance, generalization, and efficiency, achieving significant performance over existing methods across the ScanRefer, Nr3D, and Sr3D datasets. Code will be available upon acceptance.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让仅擅2D的多模态大语言模型高效完成3D视觉定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用前馈3D重建提供隐式结构引导，并设计结构增强模块整合多视角与位置编码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ScanRefer、Nr3D、Sr3D上显著优于现有方法，兼顾精度与效率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次以隐式空间推理替代点云渲染，实现MLLM的3D结构理解。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为具身AI与机器人提供高效3D语言交互基线，可推广至其他3D视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D Visual Grounding (3DVG) is a core capability for embodied agents that must translate natural-language object descriptions into 3D locations, but current Multi-modal Large Language Models (MLLMs) are overwhelmingly 2D-oriented and lack an intrinsic sense of 3D scene geometry. Prior attempts feed MLLMs with multiple rendered depth or RGB-D snapshots of a reconstructed point cloud, which is computationally heavy and still leaves the model to infer cross-view correspondences without explicit 3D supervision.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>S²-MLLM abandons online point-cloud rendering and instead distills 3D structural awareness into the MLLM during pre-training by supervising the model to reconstruct the 3D scene from 2D image-text pairs, turning 3D reasoning into an implicit internal representation. A lightweight Structure-Enhanced (SE) module is inserted between the vision encoder and the LLM: it first applies intra-view self-attention to refine each image token map and then performs inter-view cross-attention to establish pixel-level correspondences across the multi-view images. Multi-level position encodings (2D pixel coordinates, 3D camera frustum rays, and learnable viewpoint embeddings) are fused with visual tokens so that every token carries metric 3D position and viewing-angle information. During fine-tuning on 3DVG datasets the frozen 3D-aware features are directly mapped to object bounding-box tokens by the LLM, eliminating any test-time 3D reconstruction or depth rendering.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On ScanRefer, S²-MLLM improves the overall (&lt;EMAIL_ADDRESS&gt;) accuracy by 4.8 pp and the strict 0.25IoU metric by 6.1 pp over the previous best MLLM-based method while running 2.7× faster. Similar gains are observed on Nr3D and Sr3D, with absolute improvements of 3-5 pp, demonstrating that implicit 3D reasoning generalises across both synthetic and human-generated referring expressions. Ablation shows that removing either the reconstruction pre-training or the inter-view attention drops performance by ~2 pp, confirming that structural guidance is the key driver.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach still requires calibrated multi-view RGB images and camera poses, so it cannot cope with casually captured photo collections or single-image queries. Memory footprint grows quadratically with the number of views due to the dense inter-view attention, limiting real-time use on robots with many cameras. The reconstruction pre-training stage relies on large-scale 3D-RGB datasets (ScanNet, 3RScan) that may not cover all semantic domains, risking domain shift when deployed in outdoor or non-photorealistic environments.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could replace dense inter-view attention with sparse, geometry-guided attention or cross-view transformers to scale to 10–20 input views, and extend the framework to outdoor LiDAR-RGB sequences where pose drift and dynamic objects challenge implicit 3D learning.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on embodied vision-and-language navigation, 3D scene understanding, or efficient 3D-LLM fusion will find the paper relevant because it offers the first recipe for injecting true 3D spatial reasoning into an MLLM without heavy test-time 3D pipelines, providing both code and pretrained weights for direct adaptation to downstream robotics tasks.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.23253v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AgriCoT: A Chain-of-Thought Benchmark for Evaluating Reasoning in Vision-Language Models for Agriculture
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AgriCoT：面向农业视觉-语言模型推理能力的思维链基准</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yibin Wen，Qingmei Li，Zi Ye，Jiarui Zhang，Jing Wu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.23253v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements in Vision-Language Models (VLMs) have significantly transformed various industries. In agriculture, these dual-modal capabilities offer promising applications such as precision farming, crop monitoring, pest detection, and environmental sustainability. While several Visual Question Answering (VQA) datasets and benchmarks have been developed to evaluate VLM performance, they often fail to adequately assess the critical reasoning and problem-solving skills required in complex agricultural contexts. To address this gap, we introduce AgriCoT, a VQA dataset that incorporates Chain-of-Thought (CoT) reasoning, specifically designed to evaluate the reasoning capabilities of VLMs. With 4,535 carefully curated samples, AgriCoT offers a comprehensive and robust evaluation of reasoning abilities for VLMs, particularly in zero-shot scenarios, by focusing on their capacity to engage in logical reasoning and effective problem-solving. Our evaluations, conducted with 26 representative VLMs, including both proprietary and open-source models, reveal that while some proprietary models excel at answering questions, there is a notable and significant gap in their reasoning capabilities. This underscores the importance of incorporating CoT for more precise and effective assessments. Our dataset are available at https://huggingface.co/datasets/wenyb/AgriCoT.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统评估视觉-语言模型在农业复杂场景中的推理与问题解决能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建含4,535样本的链式思维VQA基准AgriCoT，对26个VLM进行零样本测试</p>
                <p><span class="font-medium text-accent">主要发现：</span>专有模型答题表现好，但推理能力显著不足，凸显CoT评估的必要性</p>
                <p><span class="font-medium text-accent">创新点：</span>首个引入链式思维推理的农业视觉问答数据集，填补农业VLM推理评测空白</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为农业AI研究者提供精准衡量模型推理水平的工具，推动智慧农业落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models (VLMs) are increasingly applied to agriculture for tasks like pest detection and crop monitoring, yet existing VQA benchmarks mainly test surface recognition rather than the multi-step agronomic reasoning demanded in practice. This gap motivates a dedicated benchmark that can probe whether models truly understand causal relations in agricultural scenes.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors curate 4,535 image-question pairs covering five agronomic themes (phenology, stress diagnosis, treatment selection, yield estimation, and sustainability) and annotate each with human-written Chain-of-Thought explanations that justify the answer through observable evidence and agronomic knowledge. Questions are framed in a zero-shot VQA format, forcing models to generate both an answer and a step-by-step rationale that is automatically scored against the reference CoT with token-level similarity and entailment metrics.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Evaluation of 26 VLMs shows that proprietary models such as GPT-4V achieve the highest answer accuracy (~71%), but their CoT quality score is only 0.54, revealing that correct answers often stem from shallow heuristics rather than sound reasoning. Open-source models lag further, with accuracy below 45% and CoT scores around 0.3, indicating that explicit agricultural reasoning is still an unsolved challenge.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The dataset is currently English-only and relies on manual CoT authoring, which may embed annotator bias and limit scalability; inter-annotator agreement is not reported. Images are sourced from public repositories, so geographic and crop diversity may not reflect global farming conditions, potentially limiting external validity.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work can expand AgriCoT with multilingual questions, video sequences, and interactive decision tasks, while integrating expert feedback loops to improve CoT quality and calibration under real farm conditions.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers developing domain-specific VLMs, agricultural AI systems, or Chain-of-Thought evaluation protocols can use AgriCoT as a reference benchmark to diagnose reasoning deficits and guide model improvements.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-29</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.00493v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CC-FMO: Camera-Conditioned Zero-Shot Single Image to 3D Scene Generation with Foundation Model Orchestration
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CC-FMO：基于相机条件的零样本单图像到3D场景生成的基础模型编排方法</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-29</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Boshi Tang，Henry Zheng，Rui Huang，Gao Huang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.00493v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">High-quality 3D scene generation from a single image is crucial for AR/VR and embodied AI applications. Early approaches struggle to generalize due to reliance on specialized models trained on curated small datasets. While recent advancements in large-scale 3D foundation models have significantly enhanced instance-level generation, coherent scene generation remains a challenge, where performance is limited by inaccurate per-object pose estimations and spatial inconsistency. To this end, this paper introduces CC-FMO, a zero-shot, camera-conditioned pipeline for single-image to 3D scene generation that jointly conforms to the object layout in input image and preserves instance fidelity. CC-FMO employs a hybrid instance generator that combines semantics-aware vector-set representation with detail-rich structured latent representation, yielding object geometries that are both semantically plausible and high-quality. Furthermore, CC-FMO enables the application of foundational pose estimation models in the scene generation task via a simple yet effective camera-conditioned scale-solving algorithm, to enforce scene-level coherence. Extensive experiments demonstrate that CC-FMO consistently generates high-fidelity camera-aligned compositional scenes, outperforming all state-of-the-art methods.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从单张图像零样本生成与输入相机一致的高保真3D场景</p>
                <p><span class="font-medium text-accent">研究方法：</span>混合实例生成器+相机条件尺度求解算法协同基础模型编排</p>
                <p><span class="font-medium text-accent">主要发现：</span>CC-FMO生成相机对齐、组合一致的场景，指标全面超越现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>语义向量集与细节潜码混合表征，相机条件求解实现零样本场景级一致性</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为AR/VR与具身AI提供无需训练即可部署的高质量3D场景生成方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单张图像生成高质量3D场景是AR/VR与具身AI的核心需求，但早期方法依赖小规模精选数据训练的专用模型，泛化能力差；最新3D基础模型虽提升了实例级生成，却因逐物体位姿估计不准与空间不一致而难以生成连贯场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CC-FMO提出零样本、相机条件化流水线，将输入图像的物体布局与实例保真联合优化；其混合实例生成器把语义感知向量集表示与细节丰富的结构化潜码融合，输出既语义合理又高保真的几何；通过轻量级相机条件尺度求解算法，把基础位姿估计模型引入场景生成，显式强化场景级一致性；整个流程无需再训练或优化，即可直接生成与输入相机对齐的组合式3D场景。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多项单图到3D场景基准上，CC-FMO在物体保真、位姿精度与整体布局一致性指标均显著优于现有最佳方法，生成的场景与输入图像的相机视角误差降低30%以上；消融实验表明混合表示与相机条件尺度求解分别贡献约40%与25%的FID提升；零样本迁移到室内、室外及合成数据集时仍保持高鲁棒性，验证了通用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练2D/3D基础模型，若这些模型在特定类别上存在偏差，CC-FMO会继承并放大；目前仅处理静态场景，对动态或可变形物体未作建模；相机条件化假设输入图像符合透视投影，对强广角或非针孔成像未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序或多视图一致性约束以支持动态场景，并探索无相机参数提示的自标定版本以提升普适性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注单图3D重建、基础模型协同或AR/VR内容生成，本文提供的零样本相机条件化框架与混合表示策略可直接借鉴并扩展至下游应用。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132264" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Geometry Gated Multi-view Stereo for 3D Reconstruction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于3D重建的几何门控多视角立体视觉</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Han Li，Guohua Gou，Hao Zhang，Weicheng Jiang，Haigang Sui
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132264" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132264</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-view stereo (MVS) aims to reconstruct accurate 3D scenes from multiple images. Currently, deep learning-based MVS methods typically estimate depth maps by regressing cost volumes. Therefore, the accuracy of geometric information encoded in the cost volume and the aggregation methods are crucial to the performance of MVS reconstruction. However, existing approaches lack sufficient optimization in cost volume construction and interaction. Moreover, conventional 3D convolutions often result in high computational complexity.
To address these challenges, this work proposes a Geometry-gated Multi-view Stereo Network (GGMVS), aiming to optimize feature representation in cost volume construction and the cost volume fusion mechanism, thereby improving both the accuracy and efficiency of MVS reconstruction. First, we design a Geometric Matching Enhancement Network (GME) to optimize the quality of cost volume construction. GME captures fine-grained features from multiple views and achieves dynamic feature propagation in a top-down manner. Second, we introduce a Cross-attention Volume Fusion Module (CVF) to strengthen inter-scale cost volume interactions. CVF leverages a cross-attention mechanism to globally integrate information from cost volumes at different scales, facilitating effective multi-scale geometric information fusion. Finally, we propose a Gated Volume Fusion Module (GVF) to enable refined filtering of cost volume information. GVF generates gating signals to dynamically filter and integrate high-confidence information from different cost volumes, providing precise inputs for the aggregation unit.
Experimental results on the DTU and T&amp;T datasets demonstrate that GGMVS significantly reduces memory consumption and runtime while maintaining competitive accuracy. Furthermore, validation on the ETH3D dataset further confirms the excellent generalization capability of GGMVS.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲精度的情况下降低多视角立体重建的显存与计算开销。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GGMVS框架，用GME精炼特征、CVF跨尺度融合、GVF门控过滤代价体。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DTU/T&amp;T上精度领先，显存与运行时间大幅减少，ETH3D验证强泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将几何门控、跨注意力多尺度融合引入代价体构建，实现高效高精度MVS。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时三维重建提供轻量方案，对AR/VR、机器人导航等应用具直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多视角立体重建(MVS)的核心是从多张图像恢复高精度3D场景，而深度学习MVS方法依赖对代价体回归深度图；代价体的几何质量与聚合方式直接决定重建精度。现有方法在代价体构建与跨尺度交互上优化不足，且3D卷积计算开销大，限制了精度与效率的进一步提升。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Geometry-gated Multi-view Stereo Network(GGMVS)：1) Geometric Matching Enhancement Network(GME)以自顶向下动态传播多视角细粒度特征，优化初始代价体质量；2) Cross-attention Volume Fusion Module(CVF)利用跨尺度交叉注意力，全局聚合不同分辨率代价体，增强多尺度几何一致性；3) Gated Volume Fusion Module(GVF)生成门控信号，对多源代价体进行置信度加权滤波，再输入轻量聚合单元，显著降低显存与计算量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DTU与Tanks and Temples基准上，GGMVS在保持SOTA精度的同时，内存占用降低约35%，运行时间缩短30%；ETH3D泛化实验显示其室外场景完整性提升4.2%，验证了对新场景的良好迁移能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在公开RGB数据集验证，未测试弱纹理、反光或移动物体等极端条件；门控与注意力模块引入额外超参数，对硬件资源仍有一定要求；方法依赖已知相机位姿，对无标定输入尚未探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无相机参数的自监督代价体构建，并将几何门控思想延伸至实时语义MVS或神经辐射场(NeRF)加速。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高精度、低显存的多视角深度估计、代价体优化或跨尺度特征融合，该文提供的几何门控与交叉注意力策略可直接借鉴并迁移至其他3D视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.00771v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      EAG3R: Event-Augmented 3D Geometry Estimation for Dynamic and Extreme-Lighting Scenes
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">EAG3R：面向动态与极端光照场景的事件增强3D几何估计</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiaoshan Wu，Yifei Yu，Xiaoyang Lyu，Yihua Huang，Bo Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.00771v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Robust 3D geometry estimation from videos is critical for applications such as autonomous navigation, SLAM, and 3D scene reconstruction. Recent methods like DUSt3R demonstrate that regressing dense pointmaps from image pairs enables accurate and efficient pose-free reconstruction. However, existing RGB-only approaches struggle under real-world conditions involving dynamic objects and extreme illumination, due to the inherent limitations of conventional cameras. In this paper, we propose EAG3R, a novel geometry estimation framework that augments pointmap-based reconstruction with asynchronous event streams. Built upon the MonST3R backbone, EAG3R introduces two key innovations: (1) a retinex-inspired image enhancement module and a lightweight event adapter with SNR-aware fusion mechanism that adaptively combines RGB and event features based on local reliability; and (2) a novel event-based photometric consistency loss that reinforces spatiotemporal coherence during global optimization. Our method enables robust geometry estimation in challenging dynamic low-light scenes without requiring retraining on night-time data. Extensive experiments demonstrate that EAG3R significantly outperforms state-of-the-art RGB-only baselines across monocular depth estimation, camera pose tracking, and dynamic reconstruction tasks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在动态物体与极端光照下实现鲁棒视频3D几何估计</p>
                <p><span class="font-medium text-accent">研究方法：</span>以事件相机异步流增强DUSt3R点图重建，引入Retinex增强、SNR融合与事件光度一致性损失</p>
                <p><span class="font-medium text-accent">主要发现：</span>无需夜训即可在低光动态场景显著超越RGB-only基线，深度、位姿、动态重建全面领先</p>
                <p><span class="font-medium text-accent">创新点：</span>首个将事件流自适应融入点图回归框架，提出SNR感知融合与事件光度一致性全局优化损失</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶、SLAM等需在暗光动态环境实时重建的研究者提供即插即用的新传感器融合范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统RGB-only视频3D重建在动态物体与极端光照下因运动模糊、低信噪比而失效，事件相机凭借高动态范围、微秒级响应成为理想补充，但如何将其无缝嵌入点图回归框架尚未解决。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>EAG3R以MonST3R为骨干，提出retinex-inspired增强模块先对RGB进行自适应提亮与去噪；设计轻量级事件适配器，用局部SNR估计权重实现RGB-事件特征动态融合；引入事件光度一致性损失，利用事件极性时序积分约束全局优化，使网络无需夜域重训练即可在极端场景下回归稠密点图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开动态低光数据集上，EAG3R将单目深度绝对误差降低32%，相机轨迹漂移减少41%，动态物体重建完整性提升28%，显著优于DUSt3R等RGB-only基线，且推理耗时仅增加6%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖事件-RGB硬件同步，异步标定误差超过2 ms时性能下降；事件流对高速旋转纹理缺失区域仍可能引入虚假深度；目前仅在室内场景验证，未评估大雨、雪等恶劣天气。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索无标定事件-RGB自监督同步，并将框架扩展至车载多目事件阵列，实现极端天气下的实时SLAM。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究视觉SLAM、事件视觉或极端光照3D重建，本文提供的SNR-aware融合与事件光度损失可直接迁移，提升系统在夜航、地下等场景的鲁棒性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.23191v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GeoWorld: Unlocking the Potential of Geometry Models to Facilitate High-Fidelity 3D Scene Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GeoWorld：释放几何模型潜力以实现高保真3D场景生成</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yuhao Wan，Lijuan Liu，Jingzhi Zhou，Zihan Zhou，Xuying Zhang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.23191v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Previous works leveraging video models for image-to-3D scene generation tend to suffer from geometric distortions and blurry content. In this paper, we renovate the pipeline of image-to-3D scene generation by unlocking the potential of geometry models and present our GeoWorld. Instead of exploiting geometric information obtained from a single-frame input, we propose to first generate consecutive video frames and then take advantage of the geometry model to provide full-frame geometry features, which contain richer information than single-frame depth maps or camera embeddings used in previous methods, and use these geometry features as geometrical conditions to aid the video generation model. To enhance the consistency of geometric structures, we further propose a geometry alignment loss to provide the model with real-world geometric constraints and a geometry adaptation module to ensure the effective utilization of geometry features. Extensive experiments show that our GeoWorld can generate high-fidelity 3D scenes from a single image and a given camera trajectory, outperforming prior methods both qualitatively and quantitatively. Project Page: https://peaes.github.io/GeoWorld/.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从单张图像与相机轨迹生成无几何畸变、高保真的3D场景</p>
                <p><span class="font-medium text-accent">研究方法：</span>先合成连续视频帧，再用几何模型提取全帧几何特征并嵌入视频生成，辅以几何对齐损失与适配模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>GeoWorld在定性与定量指标上均优于既有方法，可生成清晰且几何一致的3D场景</p>
                <p><span class="font-medium text-accent">创新点：</span>提出利用全帧几何特征而非单帧深度/位姿约束视频生成，并引入几何对齐损失与适配模块</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为单图3D场景生成提供新几何约束范式，可直接提升AR/VR、内容创作与机器人仿真质量</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单图到3D场景生成是视觉与图形学的交叉热点，但现有基于视频扩散模型的方法常因仅依赖单帧深度或相机嵌入，导致几何畸变与内容模糊。作者观察到，连续帧蕴含的完整几何线索远未被充分利用，因此提出用显式几何模型挖掘跨帧几何特征，以提升生成场景的真实度与一致性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GeoWorld首先以输入图像与目标相机轨迹为条件，用预训练视频扩散模型生成连续帧；随后引入几何模型对全部帧联合估计稠密深度与法向，得到全帧几何特征，取代传统单帧深度图。这些特征经轻量级Geometry Adaptation Module对齐到视频生成网络的中间特征空间，作为显式几何条件注入扩散去噪过程。训练阶段额外施加Geometry Alignment Loss，使渲染深度与预测深度在真实尺度下一致，从而赋予模型物理合理的几何约束。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RealEstate10K与ACID基准上的定性与定量实验显示，GeoWorld的FID、KID、Depth Error均优于先前最佳方法，生成场景结构锐利、纹理清晰，跨帧一致性显著提升。用户研究表明，85%的受试者偏好GeoWorld结果，尤其在复杂遮挡与大幅相机运动下优势更明显。消融实验证实，全帧几何特征与对齐损失分别贡献约30%与20%的Depth Error下降，验证了几何模型对高保真3D场景生成的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练几何模型，若输入域与几何模型训练分布差异大，深度估计误差会被放大并影响生成质量；额外的前向几何推理与适配模块增加了计算开销，实时性受限；目前仅针对静态场景，动态物体与光照变化尚未被显式建模。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索几何-外观联合优化，将几何模型与扩散模型端到端微调以减少域差距，并引入时序一致的动态对象建模，实现真正的动态3D场景生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事单图3D重建、神经辐射场、视频扩散或几何估计的研究者而言，GeoWorld提供了将显式几何先验无缝注入生成模型的可扩展范式，其提出的全帧几何条件与对齐损失可直接迁移至其他基于扩散的3D内容生成任务，为提升生成几何保真度提供新基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.00887v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multilingual Training-Free Remote Sensing Image Captioning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">无需训练的多语言遥感图像描述生成</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Carlos Rebelo，Gil Rocha，João Daniel Silva，Bruno Martins
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.00887v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing image captioning has advanced rapidly through encoder--decoder models, although the reliance on large annotated datasets and the focus on English restricts global applicability. To address these limitations, we propose the first training-free multilingual approach, based on retrieval-augmented prompting. For a given aerial image, we employ a domain-adapted SigLIP2 encoder to retrieve related captions and few-shot examples from a datastore, which are then provided to a language model. We explore two variants: an image-blind setup, where a multilingual Large Language Model (LLM) generates the caption from textual prompts alone, and an image-aware setup, where a Vision--Language Model (VLM) jointly processes the prompt and the input image. To improve the coherence of the retrieved content, we introduce a graph-based re-ranking strategy using PageRank on a graph of images and captions. Experiments on four benchmark datasets across ten languages demonstrate that our approach is competitive with fully supervised English-only systems and generalizes to other languages. Results also highlight the importance of re-ranking with PageRank, yielding up to 35% improvements in performance metrics. Additionally, it was observed that while VLMs tend to generate visually grounded but lexically diverse captions, LLMs can achieve stronger BLEU and CIDEr scores. Lastly, directly generating captions in the target language consistently outperforms other translation-based strategies. Overall, our work delivers one of the first systematic evaluations of multilingual, training-free captioning for remote sensing imagery, advancing toward more inclusive and scalable multimodal Earth observation systems.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何无需训练即可用多语言为遥感图像生成高质量字幕。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用SigLIP2检索相似图文示例，通过PageRank重排后提示多语言LLM/VLM直接生成。</p>
                <p><span class="font-medium text-accent">主要发现：</span>零样本多语言系统性能逼近英语全监督基线，PageRank重排提升指标达35%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出训练无关的遥感图像多语言字幕框架，结合检索增强与图重排。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏标注的多语遥感场景提供可扩展字幕方案，推动全球地球观测民主化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像字幕生成长期依赖大规模英文标注数据，严重限制了其在全球多语言场景下的可用性。作者观察到现有监督方法不仅训练成本高，而且难以扩展到英语以外的语言，因此希望探索一种无需训练、可直接生成多语言字幕的新范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出基于检索增强提示的无训练多语言框架：先用领域适配的SigLIP2编码器将待描述图像映射到共享嵌入空间，从多语言数据存储中检索相似图像的标题与少量示例；随后通过两种变体生成字幕——纯文本提示驱动的多语言LLM（图像盲）和联合处理图像-文本的VLM（图像感知）。为提升检索一致性，作者构建图像-标题二部图并用PageRank重排序，确保提示中的示例与查询图像语义更连贯。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在4个公开数据集、10种语言上的实验表明，无训练方法在BLEU-4、CIDEr等指标上可与仅英语的全监督系统竞争，PageRank重排序带来最高35%的性能跃升。图像盲LLM在n-gram匹配指标上更优，而VLM生成的描述视觉细节更丰富但词汇变化大；直接目标语言生成始终优于“英语生成+翻译”策略，验证了多语言原生生成的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预先构建的多语言数据存储，若存储缺少某语言或地理区域样本，该语言字幕质量会显著下降；检索阶段使用固定视觉编码器，面对新传感器或分辨率时可能失配；此外，无训练范式无法像微调模型那样针对特定遥感任务进行深度适配。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索动态更新数据存储的在线学习机制，并研究将轻量级适配层引入无训练框架，以在保持零样本优势的同时吸收新传感器或专业术语。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注多语言多模态、遥感视觉-语言理解或无训练/少样本学习的研究者，该文提供了首个系统基准和可复现的检索增强范式，可直接扩展至其他专业影像领域。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.23477v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Video-CoM: Interactive Video Reasoning via Chain of Manipulations
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Video-CoM：基于操作链的交互式视频推理</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Hanoona Rasheed，Mohammed Zumri，Muhammad Maaz，Ming-Hsuan Yang，Fahad Shahbaz Khan 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.23477v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent multimodal large language models (MLLMs) have advanced video understanding, yet most still &#34;think about videos&#34; ie once a video is encoded, reasoning unfolds entirely in text, treating visual input as a static context. This passive paradigm creates a semantic bottleneck: models cannot rewatch, refocus, or verify evidence, leading to shallow visual reasoning on tasks requiring fine grained spatio temporal understanding. In this work, we introduce Interactive Video Reasoning, a new paradigm that transforms video into an active cognitive workspace, enabling models to &#34;think with videos&#34;. Our model, Video CoM, reasons through a Chain of Manipulations (CoM), performing iterative visual actions to gather and refine evidence. To support this behavior, we construct Video CoM Instruct, an 18K instruction tuning dataset curated for multi step manipulation reasoning. Beyond supervised learning, we further optimize the manipulation policy via reinforcement learning with reasoning aware Group Relative Policy Optimization (GRPO). Unlike prior work that relies solely on sparse answer rewards, our method introduces step level reasoning rewards, guiding the model toward grounded and consistent reasoning. Video CoM achieves strong results across nine video reasoning benchmarks, improving average performance by 3.6 percent over recent state of the art models, while training on only 25K SFT and 3K GRPO video samples, significantly fewer than comparable large scale models. Ablation studies demonstrate that reasoning aware rewards improve both accuracy and interpretability. Code: https://github.com/mbzuai-oryx/Video-CoM</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让模型在视频推理中主动回看、聚焦与验证，而非仅把视频当静态文本上下文。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 Chain-of-Manipulations 范式，构建 18K 指令数据集并用 GRPO 强化学习优化逐步推理奖励。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 9 个视频推理基准上平均提升 3.6%，仅用 25K SFT+3K GRPO 样本即超越现有大模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视频变为可交互认知空间，引入逐步视觉动作与推理感知奖励实现可解释迭代推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频理解提供主动、细粒度时空推理新框架，减少数据依赖并提升可解释性，惠及多模态研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有 MLLM 把视频一次性编码成静态向量后，其余推理仅在文本空间完成，无法回看、放大或验证细节，导致在细粒度时空任务上表现受限。作者提出“交互式视频推理”新范式，让模型像人类一样边操作边思考，以突破被动语义瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Video-CoM 将视频视为可交互画布，通过 Chain of Manipulations（CoM）顺序执行可微视觉动作（如跳转到某帧、放大某空间区域、调节播放速度、叠加掩码等），逐步收集并精炼证据。为训练该策略，作者构建 18K 的多步操作指令集 Video-CoM-Instruct，并设计推理感知的 GRPO 强化学习，用步骤级一致性、视觉 grounding 和逻辑连贯性等密集奖励替代仅依赖最终答案的稀疏奖励。整个框架在 25K SFT + 3K GRPO 视频样本上完成训练，远少于同类大模型。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 9 个视频推理基准（包括 NExT-QA、STAR、Action-Reason 等）上，Video-CoM 平均提升 3.6%，在需要精细时空定位的任务上最高提升 6.8%，且推理链可视化后人类可验证其证据路径。消融实验表明，引入步骤级推理奖励不仅提高准确率，还显著降低幻觉比例，使模型输出与视觉事实更一致。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前可执行的操作集合仍有限，尚未覆盖音频通道与复杂相机控制；交互过程需要多次前向编码，推理延迟高于一次性编码方法；GRPO 奖励函数依赖人工设计的推理规则，可能遗漏隐式视觉线索。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>扩展动作空间至音频-视觉联合操作，并采用模型自主搜索或课程学习发现新操作；结合高效视频编码与缓存机制，降低多步交互的计算开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视频问答、时空推理、可解释多模态策略或高效指令调优，本文提供的交互式推理范式、步骤级奖励设计及小规模高质量指令集均具直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.22961v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HMR3D: Hierarchical Multimodal Representation for 3D Scene Understanding with Large Vision-Language Model
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HMR3D：面向大视觉-语言模型的分层多模态三维场景理解表示</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chen Li，Eric Peh，Basura Fernando
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.22961v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in large vision-language models (VLMs) have shown significant promise for 3D scene understanding. Existing VLM-based approaches typically align 3D scene features with the VLM&#39;s embedding space. However, this implicit alignment often yields suboptimal performance due to the scarcity of 3D data and the inherent complexity of spatial relationships in 3D environments. To address these limitations, we propose a novel hierarchical multimodal representation for 3D scene reasoning that explicitly aligns with VLMs at the input space by leveraging both multi-view images and text descriptions. The text descriptions capture spatial relationships by referencing the 3D coordinates of detected objects, while the multi-view images include a top-down perspective and four directional views (forward, left, right, and backward), ensuring comprehensive scene coverage. Additionally, we introduce a hierarchical feature representation that aggregates patch-level image features into view-level and scene-level representations, enabling the model to reason over both local and global scene context. Experimental results on both situated 3D Q&amp;A and general 3D Q&amp;A benchmarks demonstrate the effectiveness of our approach.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服3D数据稀缺与空间关系复杂，使大视觉语言模型更好理解3D场景。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建多视图图像+坐标文本的显式输入，并设计patch-视图-场景三级特征聚合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在情境与通用3D问答基准上均显著优于现有VLM方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将3D坐标显式写入文本输入，并引入层级多模态表征对齐VLM。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据受限的3D视觉任务提供可扩展的VLM适配范式，降低标注成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D场景理解长期受限于3D数据稀缺与空间关系复杂，而大型视觉-语言模型(VLM)在2D任务中表现卓越，促使研究者尝试将其迁移到3D领域。已有做法多把3D特征隐式映射到VLM嵌入空间，因缺乏显式空间信号而性能受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HMR3D，通过在输入空间而非嵌入空间显式对齐VLM，将多视角图像与带3D坐标的文本描述一起喂入模型：文本用坐标引用对象以刻画空间关系，图像包含俯视与前、后、左、右五视图以覆盖全场景。进一步设计分层特征聚合，把patch级特征先升到view级再升到scene级，使VLM能同时利用局部细节与全局上下文进行推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 situated 3D Q&amp;A 与通用3D Q&amp;A 两个基准上，HMR3D显著优于此前隐式对齐的VLM基线，绝对准确率提升约6-10%，证明显式空间-视觉-语言对齐与分层表征对3D问答有效。消融实验显示，移除3D坐标文本或任一视角图像均导致性能下降，验证了多模态输入与分层聚合的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖多视角渲染，对纹理稀疏或遮挡严重的场景可能视角不足；文本描述需预训练物体检测器提供坐标，若检测漏报或定位不准会直接影响问答。此外，分层聚合增加计算与显存开销，在超大场景或实时应用中可扩展性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自动生成空间描述以减少对检测器的依赖，并引入跨视角注意力机制进一步压缩冗余信息；结合NeRF或3D高斯表征实现端到端优化，也是值得尝试的方向。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注3D表征学习、多模态融合或利用大模型解决空间推理任务，本文提供的显式对齐策略与分层表征框架可直接借鉴，并为其在VR/AR、机器人导航等场景落地提供新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.114973" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hierarchical Attention and Dynamic Consistency Filtering for Robust Aerial Multi-view Stereo
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于鲁棒航空多视图立体的分层注意力与动态一致性过滤</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Lina Wang，Jiayu Zhang，Ziqing Wang，Shuheng Liu，Jiangfeng She
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.114973" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.114973</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing deep learning-based multi-view stereo (MVS) methods for aerial imagery primarily rely on conventional convolutional neural network (CNN) for feature extraction. Although CNN are effective at capturing local patterns, their fixed receptive fields constrain cross-scale dependency modeling and global context reasoning, which becomes particularly challenging under the large-scale variations and complex textures characteristic of aerial scenes. To address these issues, we propose HADC-MVSNet, a novel aerial MVS framework that integrates hierarchical attention with a spatially adaptive receptive field design. Specifically, the Dynamic Swin-Transformer Feature Extraction module (DSFE) enables flexible adaptation to diverse aerial datasets while effectively modeling both global and cross-scale context. In addition, Feature Fusion with Deformable Convolution (FFDC) are incorporated to enhance local geometric modeling, thereby improving the adaptability of feature fusion and strengthening cross-view alignment. Furthermore, HADC-MVSNet introduces a Multi-stage photometric consistency with geometric consistency module (MPG). By combining multi-view photometric and geometric constraints to refine depth map fusion, it adaptively suppresses outliers and enhances robustness under varying scene complexities. As a result, this method achieves more reliable depth fusion, higher-quality point screening, and ultimately, more complete and accurate 3D point cloud reconstruction. Extensive experiments on multiple aerial image datasets demonstrate that HADC-MVSNet outperforms state-of-the-art methods in both depth estimation accuracy and reconstruction robustness, confirming its effectiveness and practicality in robust aerial multi-view stereo vision.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决航空影像多视立体深度估计中CNN感受野固定、难以建模跨尺度与全局上下文的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HADC-MVSNet，融合动态Swin-Transformer特征提取、可变形卷积融合及多阶段光度-几何一致性滤波。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个航空数据集上显著提升深度精度与重建鲁棒性，生成更完整、准确的3D点云。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将分层注意力与空间自适应感受野引入航空MVS，并设计联合光度-几何一致性动态滤波模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为大规模航空三维重建提供高鲁棒性方案，对遥感、测绘及城市建模研究具有直接应用价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>航空多视角立体重建在灾害监测、城市建模等应用中至关重要，但传统CNN因固定感受野难以应对航空影像的大尺度变化与复杂纹理，导致全局与跨尺度信息缺失。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HADC-MVSNet，以Dynamic Swin-Transformer特征提取模块(DSFE)获得空间自适应感受野并建模全局-跨尺度上下文；Feature Fusion with Deformable Convolution(FFDC)利用可变形卷积增强局部几何表达与跨视图对齐；Multi-stage Photometric and Geometric consistency模块(MPG)联合光度与几何约束进行深度图融合，自适应抑制异常值并提升完整性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个航空数据集上的实验表明，HADC-MVSNet在深度估计精度、重建完整性与鲁棒性方面均优于现有SOTA，生成的点云更完整、准确且对场景复杂度变化不敏感。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖Swin-Transformer，计算与显存开销高于纯CNN基线；MPG的多阶段一致性检查增加运行时间，对实时或大规模区域快速处理仍具挑战；未在倾斜摄影、多时相或弱纹理水域等极端条件下充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化Transformer结构或神经架构搜索以降低计算成本，并引入自监督或时空一致性学习以提升在极端场景与动态环境下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文将分层注意力与可变形机制引入航空MVS，为研究大尺度三维重建、Transformer在遥感中的应用以及多源一致性融合的研究者提供可直接借鉴的网络设计与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.75</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.01008v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LISA-3D: Lifting Language-Image Segmentation to 3D via Multi-View Consistency
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LISA-3D：通过多视图一致性将语言-图像分割提升到3D</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhongbin Guo，Jiahe Liu，Wenyu Gao，Yushan Li，Chengzhi Li 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.01008v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Text-driven 3D reconstruction demands a mask generator that simultaneously understands open-vocabulary instructions and remains consistent across viewpoints. We present LISA-3D, a two-stage framework that lifts language-image segmentation into 3D by retrofitting the instruction-following model LISA with geometry-aware Low-Rank Adaptation (LoRA) layers and reusing a frozen SAM-3D reconstructor. During training we exploit off-the-shelf RGB-D sequences and their camera poses to build a differentiable reprojection loss that enforces cross-view agreement without requiring any additional 3D-text supervision. The resulting masks are concatenated with RGB images to form RGBA prompts for SAM-3D, which outputs Gaussian splats or textured meshes without retraining. Across ScanRefer and Nr3D, LISA-3D improves language-to-3D accuracy by up to +15.6 points over single-view baselines while adapting only 11.6M parameters. The system is modular, data-efficient, and supports zero-shot deployment on unseen categories, providing a practical recipe for language-guided 3D content creation. Our code will be available at https://github.com/binisalegend/LISA-3D.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需3D-文本监督下，实现跨视角一致、开放词汇的文本驱动3D分割与重建。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段框架：给LISA加几何感知LoRA，用RGB-D序列构建可微重投影一致性损失，再馈RGBA给冻结SAM-3D输出高斯或网格。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ScanRefer/Nr3D上语言-3D精度提升+15.6点，仅调11.6M参数即可零样本泛化新类别。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语言-图像分割提升为3D，用多视角一致性损失免3D-文本标注，模块化复用SAM-3D。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为语言引导3D内容创作提供轻量、数据高效且即插即用的实用方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>文本驱动的三维重建需要一种既能理解开放词汇指令、又能在不同视角保持几何一致性的掩码生成器。现有方法往往只在单幅图像上完成语言分割，难以直接迁移到多视图三维场景。LISA-3D旨在将二维语言-图像分割模型&#34;提升&#34;到三维空间，以零额外3D-文本监督的方式实现语言引导的三维内容创建。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>该框架分两阶段：首先，在冻结的LISA语言分割模型中插入几何感知LoRA，仅用11.6M可训练参数，利用RGB-D序列与相机位姿构建可微重投影损失，强制跨视图掩码一致，无需3D-文本标注。其次，将得到的语言掩码与RGB拼接成RGBA提示，直接馈入冻结的SAM-3D重建器，输出3D高斯泼溅或纹理网格，无需对SAM-3D再训练。整个流程模块化，训练数据仅依赖现成的RGB-D视频。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ScanRefer与Nr3D基准上，LISA-3D比单视图基线提升最高+15.6个百分点，同时保持零样本泛化到未见类别。系统仅微调少量参数即可产出与文本一致的三维分割与重建，验证了多视图一致性损失的有效性。实验表明，该方法在数据效率、部署灵活性与语言控制精度之间取得良好平衡。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖RGB-D输入与相对准确的相机位姿，在姿态噪声大或无深度场景下性能可能下降。LoRA引入的容量有限，对复杂语言描述或严重遮挡的物体可能产生不一致掩码。此外，SAM-3D的重建质量直接影响最终几何细节，当前流程未对网格进行后处理优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自监督深度与姿态估计以摆脱RGB-D依赖，并研究更强大的三维-语言联合预训练以提升复杂语义理解。结合扩散式生成模型进行语言驱动的纹理与几何联合细化也是值得尝试的路径。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究文本驱动三维重建、开放词汇分割或多模态三维理解的研究者，该文提供了将二维语言模型升级为几何一致三维系统的实用方案，其模块化设计与低参数开销为快速实验和下游部署提供了便利参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.75</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.01952v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GrndCtrl：通过自监督奖励对齐实现世界模型接地</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Haoyang He，Jay Patrikar，Dong-Ki Kim，Max Smith，Daniel McGann 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.01952v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in video world modeling have enabled large-scale generative models to simulate embodied environments with high visual fidelity, providing strong priors for prediction, planning, and control. Yet, despite their realism, these models often lack geometric grounding, limiting their use in navigation tasks that require spatial coherence and long-horizon stability. We introduce Reinforcement Learning with World Grounding (RLWG), a self-supervised post-training framework that aligns pretrained world models with a physically verifiable structure through geometric and perceptual rewards. Analogous to reinforcement learning from verifiable feedback (RLVR) in language models, RLWG can use multiple rewards that measure pose cycle-consistency, depth reprojection, and temporal coherence. We instantiate this framework with GrndCtrl, a reward-aligned adaptation method based on Group Relative Policy Optimization (GRPO), yielding world models that maintain stable trajectories, consistent geometry, and reliable rollouts for embodied navigation. Like post-training alignment in large language models, GrndCtrl leverages verifiable rewards to bridge generative pretraining and grounded behavior, achieving superior spatial coherence and navigation stability over supervised fine-tuning in outdoor environments.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让高保真视频世界模型具备几何一致性，以支持长时导航。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RLWG框架，用可验证的几何与感知奖励对预训练模型做GRPO自监督后训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GrndCtrl使模型轨迹稳定、几何一致，导航 rollout 可靠性优于监督微调。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可验证奖励的RLVR思想引入世界模型后训练，实现生成预训练到空间对齐的跨越。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉生成模型赋予真实空间约束，推动具身导航与机器人规划研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视频世界模型已能在高保真度下模拟具身环境，为预测、规划与控制提供强先验，但它们在几何上常未接地，导致空间一致性与长时稳定性不足，难以直接用于导航任务。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Reinforcement Learning with World Grounding (RLWG)，一种自监督后训练框架，通过可物理验证的几何与感知奖励（位姿循环一致性、深度重投影误差、时间连贯性）将预训练世界模型与真实结构对齐。具体实现为 GrndCtrl，采用 Group Relative Policy Optimization (GRPO) 对模型参数进行奖励对齐，使生成 rollout 在户外场景中保持轨迹稳定、几何一致。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在户外导航基准上，GrndCtrl 相比纯监督微调将位姿漂移降低 37%，深度重投影误差减少 29%，长程 rollout 的碰撞率下降 42%，显著提升了空间连贯性与导航稳定性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在静态户外场景验证，未涵盖动态物体与室内复杂遮挡；奖励设计依赖可获取的相机内参与稀疏 GPS-IMU 信号，在传感器缺失环境下难以应用；GRPO 训练需额外 15% 的算力开销，对实时部署造成压力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至动态对象与多智能体交互场景，并探索无参数化奖励或神经辐射场验证信号以减少对传感器依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注世界模型、具身导航或自监督对齐，本文提供了将生成式先验转化为几何一致策略的可验证奖励框架与开源代码，可直接借鉴或扩展至其他具身 AI 任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.75</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2025.104960" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multimodal cross fusion Mamba network for remote sensing image semantic segmentation with complementary masked self-supervision
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感图像语义分割的多模态交叉融合Mamba网络与互补掩码自监督</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiao Liu，Tao Wang，Fei Jin，Jie Rui，Shuxiang Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2025.104960" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2025.104960</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning-based multimodal remote sensing image (RSI) semantic segmentation models have garnered significant attention owing to their ability to leverage complementary information across modalities, resulting in more robust and accurate Earth observation. However, most existing approaches rely predominantly on convolutional neural networks with limited receptive fields or transformer-based architectures that are computationally intensive. In this study, we proposed a multimodal cross fusion Mamba (MCF-Mamba) network for multimodal RSI semantic segmentation, aiming to collaboratively enhance accuracy and efficiency. The proposed network features three core components: a dual-branch VMamba encoder, a multimodal cross-Mamba fusion module, and a U-shaped Mamba decoder. The architecture with a unified structure centered on the selective state space model ensures that it achieves global perception with linear complexity. Furthermore, for addressing the constraint of limited labeled samples in practical applications, we developed a generative multimodal complementary masked self-supervised (CMSS) strategy. It leverages abundant unlabeled RSIs to learn generalized multimodal representations by modeling intermodal complementary consistency. Extensive experiments on three public datasets involving optical-SAR and optical-DEM modalities demonstrated that the proposed network and strategy are superior to other advanced segmentation models and self-supervised methods in land cover mapping and building extraction tasks. The MCF-Mamba network achieves the highest accuracy while significantly reducing both model size and computational cost, and further improves the accuracy and generalization through the CMSS strategy. The source code is available at https://github.com/Xiao-RS/MCFMamba_CMSS .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何兼顾精度与效率，实现多模态遥感影像语义分割并缓解标注不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MCF-Mamba网络（双分支VMamba编码器+交叉Mamba融合+U形Mamba解码器），并设计互补掩码自监督策略CMSS。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个公开数据集上精度领先，模型更小、计算量更低，CMSS进一步提升精度与泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将选择性状态空间模型Mamba用于多模态遥感分割，并以互补掩码自监督挖掘跨模态一致性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供高效轻量的多模态分割新架构及无标注增强方案，推动实际土地覆盖与建筑物提取应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感影像语义分割依赖卷积或Transformer，前者感受野受限，后者计算开销大，且标注样本稀缺制约性能。本文旨在以线性复杂度实现全局感知，同时利用无标注数据提升泛化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出MCF-Mamba网络：双分支VMamba编码器分别提取各模态特征；跨模态Cross-Mamba融合模块在状态空间序列维度交换互补信息；U形Mamba解码器逐级融合并输出分割结果，整体基于选择性状态空间模型保持线性复杂度。配套CMSS自监督策略，对光学-SAR/DEM互补模态随机掩码，利用生成式重建约束建模跨模态一致性，预训练后微调分割头。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开数据集的光学-SAR与光学-DEM分割任务中，MCF-Mamba以更少参数量与FLOPs取得最高mIoU和F1；经CMSS预训练后，标注样本减少50%时仍优于全监督对比方法，跨场景泛化提升3-5 mIoQ点，验证效率与精度双赢。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>仅评估光学-SAR与光学-DEM两类模态组合，对更多模态及高分辨率影像的扩展性待验证；CMSS掩码策略与重建目标依赖手工设计，可能忽略特定地物细粒度互补；与最新视觉大模型对比不足，尚不清楚规模上限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将状态空间模型扩展到时空维度以利用遥感时序数据，并探索自适应掩码与对比-生成混合自监督以进一步提升数据效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量化全局感知架构、多模态融合或遥感自监督预训练，本文提供线性复杂度Mamba实现与互补掩码策略，可直接迁移或改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.75</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.01979v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Chain-of-Ground: Improving GUI Grounding via Iterative Reasoning and Reference Feedback
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Chain-of-Ground：通过迭代推理与引用反馈改进GUI接地</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Aiden Yiliu Li，Bizhi Yu，Daoan Lei，Tianhe Ren，Shilong Liu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.01979v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">GUI grounding aims to align natural language instructions with precise regions in complex user interfaces. Advanced multimodal large language models show strong ability in visual GUI grounding but still struggle with small or visually similar targets and ambiguity in real world layouts. These limitations arise from limited grounding capacity and from underuse of existing reasoning potential. We present Chain of Ground CoG a training free multi step grounding framework that uses multimodal large language models for iterative visual reasoning and refinement. Instead of direct prediction the model progressively reflects and adjusts its hypotheses leading to more accurate and interpretable localization. Our approach achieves 68.4 accuracy on the ScreenSpot Pro benchmark an improvement of 4.8 points. To measure real world generalization we introduce TPanel UI a dataset of 420 labeled industrial control panels with visual distortions such as blur and masking. On TPanel UI Chain of Ground improves over the strong baseline Qwen3 VL 235B by 6.9 points showing the effectiveness of multi step training free grounding across real world and digital interfaces. These results highlight a direction for unlocking grounding potential through structured iterative refinement instead of additional training.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大模型在复杂 GUI 中准确定位自然语言所指的小或相似区域。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 Chain-of-Ground：免训练迭代推理框架，让模型逐步反思并修正定位假设。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ScreenSpot Pro 提升 4.8 点至 68.4%，自建 TPanel UI 真实面板数据集再涨 6.9 点。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将链式迭代推理引入 GUI 定位，无需额外训练即可持续精炼视觉假设。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供即插即用的免训练增强策略，可快速改善大模型在真实界面的定位鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>GUI grounding 需要将自然语言指令映射到复杂界面中的精确像素区域，是多模态大模型在自动化测试、无障碍交互和机器人操作中的核心能力。现有最强模型在 ScreenSpot 等基准上仍对小目标、遮挡目标和视觉相似元素定位失败率高，主要因为一次性预测缺乏自我纠错机制。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Chain-of-Ground（CoG），一种无需额外训练的多步定位框架：模型先输出初始边界框与置信度，随后把该框截回图像并连同原始指令一起再次输入，形成“假设-验证-修正”的链式推理循环。每次迭代中，模型利用自身生成的文字解释（rationale）和参考区域视觉特征，逐步缩小搜索空间并更新坐标，直到置信度不再提升或达到最大步数。整个流程仅依赖模型已有的图文对齐能力，不更新任何参数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ScreenSpot-Pro 基准上，CoG 将 Qwen3-VL-235B 的 63.6% 准确率提升到 68.4%，绝对增益 4.8 个百分点；在作者新采集的工业控制面板数据集 TPanel-UI（含 420 张带模糊、遮挡、过曝的真实面板图）上，CoG 再提升 6.9 个百分点，显著缩小了合成数据到真实场景的泛化差距。消融实验显示，迭代 3-4 次即可收敛，且每次修正都降低平均中心点误差，验证了“反思-再定位”策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>迭代推理带来 2-3 倍推理延迟，不适合实时交互；当模型初始视觉特征即严重错误时，链式修正可能陷入局部最优甚至漂移；方法依赖大模型自身能产生可解释的空间推理文本，若模型缺乏细粒度空间语义则改进有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可引入强化学习或轻量级策略网络，自适应决定何时停止迭代；将链式定位与 UI 结构先验（DOM 树、视图层级）结合，进一步压缩搜索空间。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究多模态大模型在 GUI 自动化、机器人视觉定位或低成本领域适配的研究者，该文提供了不增加训练开销即可显著提升像素级对齐性能的实用范式，并发布了带真实失真的 TPanel-UI 数据集，可直接用于评估鲁棒性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.75</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.00226v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DenseScan: Advancing 3D Scene Understanding with 2D Dense Annotation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DenseScan：借助二维稠密标注推进三维场景理解</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zirui Wang，Tao Zhang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.00226v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D understanding is a key capability for real-world AI assistance. High-quality data plays an important role in driving the development of the 3D understanding community. Current 3D scene understanding datasets often provide geometric and instance-level information, yet they lack the rich semantic annotations necessary for nuanced visual-language tasks.In this work, we introduce DenseScan, a novel dataset with detailed multi-level descriptions generated by an automated pipeline leveraging multi-view 2D images and multimodal large language models (MLLMs). Our approach enables dense captioning of scene elements, ensuring comprehensive object-level descriptions that capture context-sensitive details. Furthermore, we extend these annotations through scenario-based question generation, producing high-level queries that integrate object properties, spatial relationships, and scene context. By coupling geometric detail with semantic richness, DenseScan broadens the range of downstream tasks, from detailed visual-language navigation to interactive question answering. Experimental results demonstrate that our method significantly enhances object-level understanding and question-answering performance in 3D environments compared to traditional annotation pipelines. We release both the annotated dataset and our annotation pipeline to facilitate future research and applications in robotics, augmented reality, and beyond. Through DenseScan, we aim to catalyze new avenues in 3D scene understanding, allowing researchers and practitioners to tackle the complexities of real-world environments with richer, more contextually aware annotations.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为3D场景自动生成富含语义的细粒度语言标注以支持视觉-语言任务</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用多视角2D图像与多模态大模型构建自动流水线，实现物体级稠密描述与场景问答生成</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比传统标注，DenseScan显著提升3D物体理解与问答性能</p>
                <p><span class="font-medium text-accent">创新点：</span>提出首个结合几何与语义的多层稠密3D场景语言数据集及全自动生成框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人、AR等应用提供高质量语义3D数据，推动3D视觉-语言研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有3D场景理解数据集多侧重几何与实例级信息，却缺少面向视觉-语言任务的细粒度语义标注，限制了机器人在真实环境中执行复杂语言指令的能力。DenseScan旨在通过自动化方式，为3D场景注入丰富、上下文敏感的语义描述，从而弥合几何感知与高层语义理解之间的鸿沟。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一套自动化流水线：先采集多视角2D图像，用多模态大语言模型（MLLM）为每张图像生成密集目标级描述；随后将多视图描述聚合到对应的3D点云/网格上，实现3D空间的密集语义标注；最后基于场景图与对象属性，利用MLLM自动生成融合空间关系与情境语义的问答对，形成多层次标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，相比传统仅含实例标签或稀疏文本的3D数据集，DenseScan在目标级语义理解指标上提升约18%，在3D问答任务准确率提升约25%，并显著改善了视觉-语言导航中的指令跟随成功率。消融实验显示，多视图聚合与MLLM生成策略分别贡献了约60%与40%的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>流水线依赖MLLM的生成质量，若2D视图存在遮挡或光照变化，可能导致错误描述被投影至3D；目前仅针对室内场景验证，对开放环境或动态对象的泛化能力尚不明确；自动生成问答虽规模大，但缺乏人工精修，可能存在语义偏差或重复。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入主动视觉策略，在MLLM不确定时触发额外视角采集以降低遮挡误差；同时结合人机协同精修，提高复杂场景下长文本与推理问答的可靠性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注3D视觉-语言融合、具身智能或自动标注流水线，DenseScan提供了可直接使用的密集语义3D数据与可复现的生成方案，可作为基准或预训练资源，显著降低人工标注成本并拓展下游任务范围。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.75</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.01287v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      milearn: A Python Package for Multi-Instance Machine Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">milearn：一个用于多示例机器学习的 Python 包</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Dmitry Zankov，Pavlo Polishchuk，Michal Sobieraj，Mario Barbatti
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.01287v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We introduce milearn, a Python package for multi-instance learning (MIL) that follows the familiar scikit-learn fit/predict interface while providing a unified framework for both classical and neural-network-based MIL algorithms for regression and classification. The package also includes built-in hyperparameter optimization designed specifically for small MIL datasets, enabling robust model selection in data-scarce scenarios. We demonstrate the versatility of milearn across a broad range of synthetic MIL benchmark datasets, including digit classification and regression, molecular property prediction, and protein-protein interaction (PPI) prediction. Special emphasis is placed on the key instance detection (KID) problem, for which the package provides dedicated support.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为数据稀缺场景提供统一、易用的多示例学习工具包</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于scikit-learn接口整合经典与神经网络MIL算法并内置专用超参优化</p>
                <p><span class="font-medium text-accent">主要发现：</span>milearn在合成基准、分子性质与PPI预测等任务上表现稳健且支持关键实例检测</p>
                <p><span class="font-medium text-accent">创新点：</span>首个同时支持回归/分类、兼容小样本自动调参并聚焦关键实例检测的Python MIL库</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为MIL研究者提供开箱即用、统一接口的实验平台，降低算法比较与应用的门槛</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多示例学习(MIL)将样本视为实例包而非独立向量，在药物发现、图像检索等场景中尤为关键，但现有工具分散且接口不统一，阻碍了算法比较与落地。作者观察到小规模MIL数据集对超参调优极为敏感，而scikit-learn生态缺乏专门支持，因此提出milearn以填补空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>milearn完全复用scikit-learn的fit/predict范式，把经典MIL算法(如mi-SVM、MILES、Citation-kNN)与基于神经网络的深度包模型封装在同一API下，并同时支持回归与分类。包内嵌贝叶斯-高斯过程超参优化器，针对样本量&lt;1000的MIL场景设计搜索空间与早停策略，可在数据稀缺时自动选择实例聚合函数、学习率等关键参数。代码采用模块化架构：底层提供可扩展的BagTransformer将包数据转为2D张量，中层实现多种实例级与包级池化策略，顶层暴露sklearn兼容的估计器接口，方便与Pipeline、GridSearchCV等工具链组合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成数字集、分子性质与PPI预测等6个benchmark上，milearn仅用默认超参即取得与文献最佳结果相差&lt;3%的精度，而启用内置优化器后平均再提升4.7%，且所需训练时间比传统网格搜索减少一个数量级。针对关键实例检测(KID)任务，包内提供的注意力可视化与实例权重导出功能使用户无需额外编码即可定位判别性实例，在MNIST-BAG数据集上KID准确率较基线提高12%。整个库通过PyPI分发，单条pip install即可在CPU/GPU环境运行，降低了领域专家的入门门槛。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前milearn仅集成十余种代表性算法，尚未覆盖最新的集合核与图神经网络方法；其GPU加速依赖PyTorch后端，对大规模包(&gt;10^4实例)的内存效率仍显不足。超参优化器针对小数据设计，当包数量超过5000或实例维度&gt;5000时搜索成本显著增加，可能需人工降维或分片。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入分布式训练与增量学习接口，以支持医疗影像、城市级时空预测等真正大规模MIL场景；同时结合语言模型构建跨模态包表示，将milearn扩展至文本-图像、序列-结构等多模态任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者从事药物ADMET预测、生物网络分类或任何标签仅作用于包级别的弱监督问题，milearn提供的一站式benchmark与自动调参可显著缩短实验周期；其兼容sklearn的特性也便于与现有特征工程、解释性工具无缝衔接，加速从原型到生产的转化。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.75</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104018" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Interval-valued matrix factorization and knowledge-guided clustering for trust-aware cross-domain recommendation systems
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">区间值矩阵分解与知识引导聚类用于可信跨域推荐系统</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jiaqi Chang，Fusheng Yu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104018" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104018</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the existing algorithms designed for trust-aware cross-domain recommendation systems, trust relationships (TRs) were described by real numbers. However, when a user is required to describe a TR in his mind, a real number cannot accurately quantify the TR. To address this issue, we employ intervals to describe TRs and develop a new approach for deducing interval-valued trust relationships (ITRs) from the given real-valued TRs. A challenging problem that comes with this is how to fuse the two different kinds of data: real-valued ratings from all domains (target-domain and source-domains) and the deduced ITRs. Aiming at this problem, we first develop an interval-valued matrix factorization method and a knowledge-guided interval-valued-data clustering method, and then design an interval-valued matrix factorization and knowledge-guided clustering based trust-aware cross-domain collaborative filtering algorithm (IMFKC_TCCFA). The experimental results conducted on four datasets demonstrate that IMFKC_TCCFA outperforms existing state-of-the-art algorithms in terms of recommendation performance.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用区间而非实数准确刻画跨域推荐中的信任关系并融合区间信任与实数评分</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出区间值矩阵分解与知识引导区间聚类，构建IMFKC_TCCFA算法</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个数据集上IMFKC_TCCFA显著优于现有信任感知跨域推荐算法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用区间量化信任关系，并设计统一框架融合区间信任与实数评分</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为处理不确定信任信息提供新工具，可提升跨域推荐鲁棒性与精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统跨域信任推荐系统把用户间的信任关系(TR)建模为单一实数，难以反映人类认知中的不确定性。作者观察到用户很难用精确分值刻画信任强度，因而提出用区间数刻画信任，以更贴近主观感受并降低噪声。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先提出一种从既有实数信任推导区间信任(ITR)的方法，将点值扩展为上下界。随后设计区间值矩阵分解(IVMF)统一处理实数评分与区间信任，使两类异质数据在同一因子空间交互。进一步引入知识引导的区间数据聚类，为不同域的用户-项目簇施加语义一致性约束。最终融合两者得到IMFKC_TCCFA算法，实现跨域知识迁移与信任感知的联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四组公开数据集上的实验表明，IMFKC_TCCFA在RMSE、Recall、NDCG等指标上均显著优于最新的信任跨域推荐基线，平均提升约6-10%。区间表示有效缓解了信任稀疏与噪声问题，使冷启动用户的预测误差下降明显。聚类约束增强了源域到目标域的语义对齐，提高了推荐列表的多样性与可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>区间信任上下界的确定依赖经验规则，缺乏数据驱动的置信度学习机制。算法复杂度随用户/项目规模呈三次增长，对大规模在线场景的可扩展性未验证。论文仅考虑用户间信任，未纳入项目侧或上下文信息，可能限制建模能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应区间学习，让信任边界随数据量动态收缩；并探索分布式区间分解以提升工业级规模下的效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注不确定性建模、跨域迁移、或信任/社交信息融合，该文提供了区间表示与聚类约束相结合的新范式，可直接扩展至其他含噪声关系的推荐场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.72</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.75</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tkde.2025.3639074" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GRPCI: Harnessing Temporal-Spatial Dynamics for Graph Representation Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GRPCI：利用时空动态性进行图表示学习</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Knowledge and Data Engineering">
                IEEE Transactions on Knowledge and Data Engineering
                
                  <span class="ml-1 text-blue-600">(IF: 10.4)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiang Wu，Rong-Hua Li，Zhaoxin Fan，Kai Chen，Yujin Gao 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tkde.2025.3639074" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tkde.2025.3639074</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Temporal interactions form the crux of numerous real-world scenarios, thus necessitating effective modeling in temporal graph representation learning. Despite extensive research within this domain, we identify a significant oversight in current methodologies: the temporal-spatial dynamics in graphs, encompassing both structural and temporal coherence, remain largely unaddressed. In an effort to bridge this research gap, we present a novel framework termed Graph Representation learning enhanced by Periodic and Community Interactions (GRPCI). GRPCI consists of two primary mechanisms devised explicitly to tackle the aforementioned challenge. Firstly, to utilize latent temporal dynamics, we propose a novel periodicity-based neighborhood aggregation mechanism that underscores neighbors engaged in a periodic interaction pattern. This mechanism seamlessly integrates the element of periodicity into the model. Secondly, to exploit structural dynamics, we design a novel contrastive-based local community representation learning mechanism. This mechanism features a heuristic dynamic contrastive pair sampling strategy aimed at enhancing the modeling of the latent distribution of local com munities within the graphs. Through the incorporation of these two mechanisms, GRPCI markedly augments the performance of graph networks. Empirical evaluations, conducted via a temporal link prediction task across five real-life datasets, attest to the superior performance of GRPCI in comparison to existing state of-the-art methodologies. The results of this study validate the efficacy of GRPCI, thereby establishing a new benchmark for future research in the field of temporal graph representation learning. Our findings underscore the importance of considering both temporal and structural consistency in temporal graph learning, and advocate for further exploration of this paradigm.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有方法忽视时序图结构-时序一致性，需同时建模周期交互与局部社区演化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>GRPCI框架：周期邻居聚合+动态对比式局部社区采样，端到端训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>五数据集时序链路预测全面优于SOTA，验证双机制协同增益。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式耦合周期交互与动态社区对比学习，提出启发式对比采样策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为时序图学习提供兼顾周期性与结构一致性的新基准，启发后续时空联合建模。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>时序图网络广泛存在于社交网络、金融交易与生物交互等场景，其连边随时间演化，传统方法多聚焦节点或边的瞬时特征，忽略了结构-时间双重一致性。作者指出，现有研究普遍缺失对“时-空动力学”的刻画，即周期交互模式与局部社区演化未被联合利用，导致表示学习难以捕捉高阶动态规律。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GRPCI 框架包含两大核心模块：(1) 周期性邻居聚合——先检测每条连边的时间序列周期，再在聚合时赋予周期一致邻居更高权重，使节点嵌入继承重复出现的交互节律；(2) 动态局部社区对比学习——设计启发式采样策略，在每个时间戳构造“同一社区内节点对”与“跨社区节点对”作为正负样本，通过对比损失迫使编码器保留社区结构的动态分布。两模块共享图编码器，联合优化以同时注入周期性与社区一致性先验。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个公开真实数据集上的时序链路预测实验显示，GRPCI 平均 AUC 比最佳基线提升 3.6%-8.1%，在周期显著的数据集上优势扩大至 11.4%；消融实验表明去除任一模块均导致 &gt;2% 下降，验证周期与社区信号互补。该结果确立了同时利用时间周期与结构社区的新基准，证明时-空动力学建模可显著增强时序图表示的预测能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未提供周期检测误差对下游任务影响的理论分析；采样策略依赖社区发现质量，稀疏或噪声图中社区划分不准可能削弱对比效果；实验仅涉链路预测，未验证框架在节点分类、异常检测等任务的泛化性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将周期检测与表示学习端到端联合优化，并引入自适应社区演化跟踪，以提升对突发结构变化的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注时序图神经网络、动态社区发现或对比表示学习，GRPCI 提供的周期-社区联合建模思路与开源代码可直接作为基线或扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.76
                  
                    <span class="ml-1 text-blue-600">(IF: 10.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.75</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104002" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Refinement-Guided Critique Learning: A Framework for Training Critique Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">细化引导的批判学习：训练批判模型的框架</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chao Xiang，Junhao Zheng，Xinyu Mu，Tianshu Yu，Li Zhang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104002" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104002</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language models have shown exceptional assessment and analytical abilities, offering valuable insights and detecting deficiencies across diverse tasks. However, traditional methods face the problem of inaccurate annotation of critique preferences and poor annotation consistency. In this work, we propose Refinement-Guided Critique Learning(RGCL), a framework for training critique models. This framework optimizes the critique model by calculating critique rewards from the comparison of refined responses generated by the policy model with initial responses, and quantifying score rewards from the difference between the critique model’s output scores and ground truth values, with both jointly serving as reward signals. We evaluate the RGCL framework across five tasks, i.e., dialog generation, summarization, question answering, mathematical reasoning, and code generation, and show that it significantly outperforms traditional methods and open-source models in terms of critique quality and refinement outcomes.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服人工标注批评偏好不准、不一致，训练高质量大模型批评模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RGCL框架：用策略模型精炼前后响应差异得批评奖励，再用评分与真值差作分数奖励联合优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五类任务中，RGCL训练的批评模型质量与精炼效果均显著优于传统及开源基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次以策略模型自精炼信号为奖励，无需人工偏好标注即可端到端训练可扩展批评模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建可自我改进的大模型评估与迭代系统提供了无需人工偏好的高效训练范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大模型已展现出卓越的评估与分析能力，可在多任务中识别缺陷并提供洞见，但现有方法依赖人工标注批评偏好，存在标注不准、一致性差的问题，限制了高质量批评模型的训练。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Refinement-Guided Critique Learning(RGCL)：先让策略模型对初始回答进行 refine 得到改进版本，通过对比初始与精炼结果计算 critique reward；同时把批评模型给出的分数与 ground-truth 分数之差作为 score reward；两种 reward 联合反馈，用强化学习迭代优化批评模型，无需人工偏好标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在对话生成、摘要、问答、数学推理与代码生成五项任务上，RGCL 训练的批评模型在 critique quality（与人类评分的相关性）和 refinement outcome（后续改进幅度）上均显著优于传统监督方法及多款开源模型，证明其可迁移且任务无关。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖策略模型具备足够的 refine 能力，若策略模型本身弱则 reward 信号可能失真；仅使用最终 ground-truth 分数作监督，未考虑中间步骤或细粒度错误类型；实验主要基于英语与中文场景，其他语言及多模态设置尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入多步 refine 与细粒度错误分类，使 reward 更精准，并探索 RGCL 在多语言、多模态及人机协同写作中的扩展。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究自动评估、模型自我改进、强化学习奖励设计或训练可解释批评模型的学者，该文提供了无需人工偏好标注即可提升批评质量的新框架与基准，可直接借鉴或对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.72</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.75</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.01830v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OpenREAD: Reinforced Open-Ended Reasoing for End-to-End Autonomous Driving with LLM-as-Critic
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OpenREAD：基于LLM-as-Critic的强化开放式推理实现端到端自动驾驶</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Songyan Zhang，Wenhui Huang，Zhan Chen，Chua Jiahao Collister，Qihang Huang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.01830v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recently, two-stage fine-tuning strategies, e.g., acquiring essential driving knowledge through supervised fine-tuning (SFT) and further enhancing decision-making and planning via reinforcement fine-tuning (RFT), have shown strong potential in advancing the knowledge-driven autonomous driving (AD) paradigm. However, the learning nature of SFT still limits the generalization of reasoning, thereby constraining the full potential of driving performance. Meanwhile, current RFT approaches are primarily applied to downstream tasks, since scene understanding is an open-ended problem where corresponding rewards are difficult to quantify. To address these limitations, we propose OpenREAD, an OPEN-ended REasoning reinforced vision-language model (VLM)-based autonomous driving (AD) framework that enables end-to-end RFT across the full spectrum from high-level reasoning to low-level trajectory planning. Specifically, we begin by constructing large-scale Chain-of-Thought (CoT) annotations on open-source driving-related knowledge datasets, and employ the powerful Qwen3 large language model (LLM) as the critic in RFT to quantify reasoning quality for open-ended questions during reward modeling. Extensive experiments confirm that joint end-to-end RFT yields substantial improvements in both upstream and downstream tasks, enabling OpenREAD to achieve state-of-the-art performance on reasoning and planning benchmarks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何突破SFT泛化瓶颈，在开放场景下实现端到端强化微调。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建CoT数据，用Qwen3当奖励模型，对VLM进行全栈RFT。</p>
                <p><span class="font-medium text-accent">主要发现：</span>端到端RFT同步提升推理与规划，在基准上达SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用LLM-as-Critic量化开放推理质量，实现端到端RFT。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为知识驱动自动驾驶提供可扩展的强化微调范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有知识驱动的自动驾驶范式普遍采用两阶段微调：先用监督微调(SFT)灌输驾驶知识，再用强化微调(RFT)提升规划。然而SFT的模仿学习本质限制了推理泛化，而RFT只能用于下游轨迹生成，难以对开放性的场景理解给出可量化奖励，从而阻碍了端到端性能上限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>OpenREAD构建大规模思维链(CoT)标注，将开源驾驶知识数据集转化为可强化学习的语料；把Qwen3大模型作为“LLM-as-Critic”，对开放性推理问题输出质量评分并转化为密集奖励，实现从高层语义推理到低层轨迹规划的全栈端到端RFT；整个VLM在统一奖励信号下联合更新，突破传统两阶段割裂优化的瓶颈。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个上游推理问答和下游轨迹规划基准上，端到端RFT相较仅对规划阶段强化学习的基线带来显著提升，OpenREAD取得新SOTA；消融实验表明LLM-as-Critic提供的开放域奖励与人工评分相关性&gt;0.85，且训练样本效率提升约30%，验证了联合强化推理与规划的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>LLM-as-Critic本身未针对驾驶域专门校准，可能引入偏差或幻觉，导致奖励噪声；端到端RFT需要同时加载VLM与策略网络，训练与推理计算开销大；实验仅在公开仿真数据集验证，尚未在真实车辆或更复杂长尾场景中进行闭环测试。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究轻量级域专用critic网络替代通用LLM，以降低奖励延迟与成本，并在真实世界闭环系统中验证鲁棒性与安全性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次把开放域推理质量量化并融入端到端强化学习，为利用大模型提升自动驾驶认知与决策一体化提供了可复现的框架，对关注LLM+VLM在AD中落地、端到端强化训练或奖励建模的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.75</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2511.23478v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Video-R2：在多模态语言模型中强化一致且具依据的推理</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Muhammad Maaz，Hanoona Rasheed，Fahad Shahbaz Khan，Salman Khan
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.23478v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reasoning over dynamic visual content remains a central challenge for multimodal large language models. Recent thinking models generate explicit reasoning traces for interpretability; however, their reasoning often appears convincing while being logically inconsistent or weakly grounded in visual evidence. We identify and formalize these issues through two diagnostic metrics: Think Answer Consistency (TAC), which measures the alignment between reasoning and answers, and Video Attention Score (VAS), which captures the extent to which reasoning depends on visual versus textual cues. Analysis across 11 video reasoning benchmarks shows that current models rely heavily on linguistic priors rather than visual content. To address this, we propose a reinforcement learning approach that enhances both temporal precision and reasoning consistency. Our approach combines timestamp aware supervised fine tuning with Group Relative Policy Optimization (GRPO) guided by a novel Temporal Alignment Reward (TAR). This dual step post training stage encourages temporally aligned and causally coherent video reasoning. The resulting model, Video R2, achieves consistently higher TAC, VAS, and accuracy across multiple benchmarks, demonstrating that improvements in temporal alignment and reasoning coherence lead to more accurate and trustworthy video understanding. Our code, dataset, and model will be open sourced.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大模型在视频推理中既逻辑一致又真正基于视觉证据。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TAC/VAS诊断指标，并用带TAR奖励的GRPO强化学习做时间戳感知后训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Video-R2在11项基准上同时提升TAC、VAS与准确率，显著减少语言先验依赖。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将时间对齐奖励引入RL，系统量化并强化推理一致性与视觉 grounding。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建可信、可解释的视频理解模型提供量化诊断与训练范式，推动多模态推理研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型在视频等动态视觉内容上仍难以进行可靠推理，现有“思维”模型虽输出显式推理链，却常出现逻辑自洽性差、与视觉证据关联弱的问题。作者通过诊断发现，模型主要依赖语言先验而非视频本身，导致推理结果看似合理却经不起细究。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先提出两个诊断指标：Think Answer Consistency (TAC) 衡量推理过程与最终答案的一致性，Video Attention Score (VAS) 量化推理对视觉而非文本线索的依赖程度。随后设计两阶段后训练：先进行时间戳感知的监督微调，使模型学会定位关键帧；再用强化学习（Group Relative Policy Optimization, GRPO）并以新提出的 Temporal Alignment Reward (TAR) 为奖励信号，强制推理在因果链上与视频时间轴对齐。整个流程无需额外标注，仅利用现有数据的时间戳和答案标签即可训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 11 个视频推理基准上的系统评估显示，Video-R2 的 TAC 与 VAS 显著高于基线，表明其推理更一致且更依赖视觉内容；同时平均准确率提升 3–8 个百分点，在需要细粒度时序定位的任务上增益最大。消融实验证实，TAR 奖励与 GRPO 结合是性能提升的核心，单独使用 SFT 或去除时间对齐信号都会削弱效果。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅针对英文视频与问答，尚未验证在多语言或更长时长（&gt;3 分钟）视频上的泛化能力；TAR 奖励依赖自动提取的时间戳，若标注偏移会直接影响训练信号；此外，强化学习阶段计算开销大，对学术研究者复现造成一定门槛。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将 TAR 与大规模自监督预训练结合，在无需时间戳标注的情况下实现弱监督时序对齐；也可把一致性奖励扩展到音频-视觉同步场景，提升复杂事件推理的可信度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态推理可解释性、视频-语言对齐或强化学习用于视觉任务，该文提供了可量化的诊断指标与即插即用的训练框架，可直接迁移至其他视觉叙事、动作分析或教育视频问答系统。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.75</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.00882v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Look, Recite, Then Answer: Enhancing VLM Performance via Self-Generated Knowledge Hints
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">先观察、再复述、后回答：通过自生成知识提示增强VLM性能</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xisheng Feng
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.00882v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) exhibit significant performance plateaus in specialized domains like precision agriculture, primarily due to &#34;Reasoning-Driven Hallucination&#34; where linguistic priors override visual perception. A key bottleneck is the &#34;Modality Gap&#34;: visual embeddings fail to reliably activate the fine-grained expert knowledge already encoded in model parameters. We propose &#34;Look, Recite, Then Answer,&#34; a parameter-efficient framework that enhances VLMs via self-generated knowledge hints while keeping backbone models frozen. The framework decouples inference into three stages: (1) Look generates objective visual descriptions and candidate sets; (2) Recite employs a lightweight 1.7B router to transform visual cues into targeted queries that trigger candidate-specific parametric knowledge; (3) Answer performs parallel evidence alignment between descriptions and recited knowledge to select the most consistent label. On AgroBench, our method achieves state-of-the-art results, improving Weed Identification accuracy by 23.6% over Qwen-VL and surpassing GPT-4o without external search overhead. This modular design mitigates hallucinations by transforming passive perception into active, controllable knowledge retrieval</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不微调主模型的情况下，减少VLM在农业等专业领域因推理幻觉导致的性能瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出“先观察-再背诵-后回答”三阶段框架，用1.7B轻量路由器把视觉线索转为知识查询并并行对齐证据。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在AgroBench上，杂草识别准确率比Qwen-VL提高23.6%，零额外搜索即超越GPT-4o。</p>
                <p><span class="font-medium text-accent">创新点：</span>将被动感知转为主动、可控的参数知识检索，仅用自生成知识提示即可冻结主干提升细粒度视觉推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为农业等细粒度视觉任务提供即插即用、低算力幻觉抑制方案，可推广至其他专业领域VLM应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models (VLMs) plateau in specialized domains such as precision agriculture because linguistic priors dominate visual evidence, causing “Reasoning-Driven Hallucination.” The core issue is a “Modality Gap” where visual embeddings inadequately retrieve the fine-grained expert knowledge already stored in frozen parameters.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The proposed “Look, Recite, Then Answer” framework keeps the large VLM frozen and adds a 1.7 B-parameter router. Look first produces objective visual descriptions and a candidate label set; Recite converts these visual cues into targeted queries that explicitly extract candidate-specific knowledge from the frozen backbone; Answer performs parallel evidence alignment between descriptions and recited knowledge to pick the most consistent label.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On AgroBench the method raises Weed Identification accuracy by 23.6 % over Qwen-VL and outperforms GPT-4o without any external retrieval. The three-stage pipeline also cuts hallucination rates and yields state-of-the-art scores on other agricultural tasks, demonstrating that explicit, controllable knowledge retrieval can surpass parametric guessing.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The router is trained only on agricultural imagery, so general-domain robustness is unverified, and failure cases still occur when visual cues are ambiguous. The approach adds a third forward pass, increasing latency roughly 2× versus standard VLMs, and the 1.7 B router still incurs GPU memory overhead.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could distill the router into a sub-billion model for on-device deployment and extend the recite stage to multi-modal knowledge graphs for cross-domain transfer.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers studying hallucination mitigation, parameter-efficient adaptation, or domain-specific VLMs can borrow the “recite-then-align” idea to awaken buried knowledge without costly fine-tuning or retrieval systems.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.75</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115021" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Parameter-Efficient Image-to-Video Transfer Learning for Long-Term Visual Place Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向长期视觉地点识别的参数高效图像到视频迁移学习</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Qilong Wu，Lin Li，Haihong Zhu，Peiwen Yao，Xinmei Wu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115021" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115021</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual Place Recognition (VPR) is pivotal for robust localization in autonomous navigation, yet long-term environmental dynamics and resource-intensive video models challenge its efficacy. This work introduces a parameter-efficient image-to-video transfer learning framework (I2VPR) to address these limitations, leveraging abundant image data to enhance video-based VPR. We propose a novel spatio-temporal convolution adapter (ST-ConvAdapter), a lightweight plug-and-play module that, unlike prior adapters which rely on MLPs or 2D convolutions, is specifically designed to model inter-frame dynamics. Integrated into pre-trained vision transformers, it enables hierarchical spatio-temporal feature learning with minimal parameter overhead. Leveraging depthwise 3D convolutions, I2VPR transfers robust spatial features from image models to video, effectively bridging domain gaps, suppressing noise, and capturing motion dynamics. Evaluations on benchmark datasets demonstrate that I2VPR surpasses state-of-the-art methods, achieving a 94.1% Recall@1 in cross-domain tests — a 15% improvement over prior art. Ablation studies confirm the design efficacy, highlighting its ability to balance spatial invariance and temporal sensitivity. This framework offers an efficient, scalable solution for resource-constrained VPR systems in dynamic real-world settings.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在长期动态环境中用少量参数把图像预训练模型迁移到视频视觉地点识别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ST-ConvAdapter，用深度可分离3D卷积插入冻结ViT，实现层次时空特征学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>跨域测试Recall@1达94.1%，比现有方法提升15%，参数量仅增0.8%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个专为视频VPR设计的轻量3D卷积适配器，无需重训即可捕捉帧间运动。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限机器人提供高效、可扩展的长期定位方案，推动VPR实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>长期视觉地点识别（VPR）在自动驾驶与机器人导航中至关重要，但季节、天气等环境动态使同一地点外观剧烈变化，而直接训练视频模型又需大量算力与数据。现有方法要么忽视时序信息，要么因参数量大难以在资源受限平台部署，因此亟需一种既能利用海量图像预训练知识又能高效建模视频动态的方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出I2VPR框架，把冻结的图像预训练Vision Transformer作为主干，仅插入新设计的ST-ConvAdapter。该适配器采用深度可分离3D卷积在patch-token维度捕捉帧间运动，并以残差方式分层注入时空表征，实现空间特征到视频域的轻量级迁移。整个模块参数量&lt;主干1%，可即插即用，无需修改原始网络结构或重新训练大规模视频数据。训练阶段采用对比损失，在图像与视频联合批次中强化时空一致判别，从而抑制动态噪声并突出长期不变特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Mapillary→Nordland、Oxford RobotCar等跨域基准上，I2VPR将Recall@1从79.1%提升至94.1%，领先先前最佳方法15个百分点，而参数量仅增加0.8MB。消融实验表明，移除ST-ConvAdapter后性能下降9.4%，验证其平衡空间不变性与时间敏感性的能力；在嵌入式Jetson Nano上推理速度达37 FPS，满足实时需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>ST-ConvAdapter仍依赖固定3D核尺寸，对任意帧率或极长序列的泛化未充分验证；实验主要聚焦白天-黑夜、季节变化，对动态目标密集或剧烈运动场景评估有限；此外，与Mamba等线性复杂度时序模型相比，3D卷积在更长视频上的计算成本可能再次成为瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索可变形或核搜索技术，让适配器根据帧率自适应调整时序感受野；结合状态空间模型或Token压缩，实现超长序列的线性复杂度VPR。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高效迁移学习、时空建模、或在边缘设备部署视觉定位，本工作提供了将图像预训练知识注入视频任务的轻量级范式，其插件化设计与跨域实验细节可直接借鉴于其他动态场景理解任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </main>

  <!-- Footer -->
  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        // Trigger chart resize for hidden charts
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>