<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-05</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <!-- Header -->
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 30 篇论文 ·
        生成于 2025-12-05 12:18 Asia/Shanghai
      </p>
    </div>
  </header>

  <!-- Researcher Profile Section (Collapsible) -->
  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="researcher-profile" class="section-expand collapsed">
      <!-- Basic Stats - Compact Row -->
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">87</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;8</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      <!-- LLM Insights - Compact -->
      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>该用户的核心阅读兴趣集中在“遥感与计算机视觉的交叉”——利用深度学习（尤其图神经网络）对遥感影像进行场景理解与三维深度估计，同时关注知识图谱在地理空间信息组织中的应用。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在遥感地信与计算机视觉两大方向收藏量合计37篇（42.5%），且持续追踪深度估计、图神经网络等子主题，表明其已围绕“遥感影像智能解析”形成较系统的文献积累。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨测绘学报、AAAI、PR等地球信息科学与AI顶会顶刊，并同时关注土地利用、空间认知等地理议题，显示出“GIS+AI”的明显跨学科阅读特征。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025Q4出现收藏高峰（5篇），新增关键词集中在知识图谱与图神经网络，预示用户正由纯视觉任务向“图结构建模与地理知识融合”深化，兴趣呈“视觉→图→知识”递进。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>建议延伸阅读遥感时空序列预测、GeoAI中的大模型（Foundation Model）以及多模态遥感-文本知识图谱构建，可进一步拓展时空推理与可解释性研究视野。</p>
        </div>
      </div>
      

      <!-- Charts Row: Domains + Trends -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Research Domains Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        <!-- Quarterly Trends Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      <!-- Year Distribution Chart (full width) -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      <!-- Authors and Venues Row -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Top Authors -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Bingyi Kang">Bingyi Kang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">3</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiashi Feng">Jiashi Feng</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">3</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Di Wang">Di Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Haonan Guo">Haonan Guo</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jing Zhang">Jing Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wei Wang">Wei Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lihe Yang">Lihe Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zilong Huang">Zilong Huang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiaogang Xu">Xiaogang Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Hengshuang Zhao">Hengshuang Zhao</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shunyu Liu">Shunyu Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wentao Jiang">Wentao Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        <!-- Top Venues -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="测绘学报">测绘学报</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">4</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Proceedings of the AAAI Conference on Artificial Intelligence">Proceedings of the AAAI Conference on Artificial Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="武汉大学学报（信息科学版）">武汉大学学报（信息科学版）</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Pattern Recognition">Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Neural Networks">IEEE Transactions on Neural Networks</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Information Fusion">Information Fusion</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="测绘工程">测绘工程</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="计算机工程与应用">计算机工程与应用</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      <!-- Keywords -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(13)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识图谱 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            土地利用 <span class="text-text-secondary">(3)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            地球空间信息学 <span class="text-text-secondary">(3)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            图神经网络 <span class="text-text-secondary">(2)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            图斑聚合 <span class="text-text-secondary">(2)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            云计算 <span class="text-text-secondary">(2)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            空间感知与认知 <span class="text-text-secondary">(2)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            人工智能 <span class="text-text-secondary">(2)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Deep learning <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            End-to-end learning <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Multiple instance learning <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Neural networks <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            convolutional architecture <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            graph neural networks <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            semi-supervised learning <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            多模态数据学习 <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            超图神经网络 <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            高阶数据相关性 <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            图数据处理 <span class="text-text-secondary">(1)</span>
          </span>
          
        </div>
      </div>
      

      <!-- Generation info -->
      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-05 11:45 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Chart.js Initialization for Researcher Profile -->
  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      // Chart.js global defaults for academic style
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      // Domains Horizontal Bar Chart
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['计算机视觉', '图神经网络', '知识图谱', '遥感地信', '深度估计', '多示例学习', '三维重建', '移动感知'],
            datasets: [{
              data: [23, 9, 11, 14, 12, 3, 3, 2],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      // Quarterly Trends Bar Chart with 3 cycling colors (one per month in quarter)
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 4 }, { q: '2023-Q2', c: 1 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 3 }, { q: '2024-Q2', c: 2 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 1 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 1 }, { q: '2025-Q4', c: 5 }];
        // Color palette: 3 colors cycling for months within quarters
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },   // Blue
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },   // Green
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }    // Amber
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      // Year Distribution Line Chart
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 1 }, { year: 2010, count: 0 }, { year: 2011, count: 2 }, { year: 2012, count: 1 }, { year: 2013, count: 0 }, { year: 2014, count: 1 }, { year: 2015, count: 1 }, { year: 2016, count: 3 }, { year: 2017, count: 2 }, { year: 2018, count: 4 }, { year: 2019, count: 1 }, { year: 2020, count: 1 }, { year: 2021, count: 1 }, { year: 2022, count: 3 }, { year: 2023, count: 5 }, { year: 2024, count: 5 }, { year: 2025, count: 7 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    // Show every 5th year label to avoid crowding
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      
    });
  </script>
  

  <!-- Overall Summaries Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于多模态大模型的论文、6篇关于自监督/对比学习的论文、5篇关于遥感与空间感知的论文、4篇关于视觉-语言对齐与泛化的论文、3篇关于3D场景与新视角合成的论文、2篇关于城市计算与表征的论文以及1篇关于计算病理学的论文。</p>
            
            <p><strong class="text-text-secondary">多模态大模型</strong>：研究如何增强大语言模型的视觉理解、推理与交互能力，代表作包括《Constituency-Tree-Induced Vision-Language Alignment》通过句法树改进跨模态对齐，《OneThinker》用统一强化学习框架同时完成图像与视频推理，《COOPER》提出协同感知-推理一体化架构，《Seeing through Imagination》利用隐式空间世界模型提升3D空间推理，《GeoViS》在遥感场景中实现地理奖励的细粒度视觉定位。</p>
            
            <p><strong class="text-text-secondary">自监督对比学习</strong>：通过设计对比或掩码任务学习通用表征，涵盖图、图像及病理切片等多域数据，《UrbanMMCL》利用多图多模态对比学习生成城市区域表征，《Cross-Stain Contrastive Learning》在H&amp;E与IHC配对切片上建立跨染色对比任务以提取可迁移病理特征。</p>
            
            <p><strong class="text-text-secondary">遥感空间感知</strong>：聚焦高分辨率遥感影像的语义提取与空间推理，《High-resolution local climate zone mapping via deep mixed-scene decomposition》提出混合场景分解网络解决同像元多土地覆盖共存的LCZ制图难题，《GeoViS》进一步将地理先验引入视觉定位以提升遥感影像细粒度理解。</p>
            
            <p><strong class="text-text-secondary">视觉语言对齐</strong>：探索视觉与文本的精细对齐及跨域泛化，《Constituency-Tree-Induced Vision-Language Alignment》用句法树结构引导跨模态注意力，《Generalizing Vision-Language Models with Dedicated Prompt Guidance》通过专用提示指导在保持泛化性的同时适配新领域。</p>
            
            <p><strong class="text-text-secondary">3D新视角合成</strong>：研究从非受控图像快速恢复3D场景与渲染新视角，《AnySplat》提出前馈式3D高斯溅射框架，无需相机参数即可由松散照片集实现实时新视角合成。</p>
            
            <p><strong class="text-text-secondary">城市表征计算</strong>：面向城市分析的区域级表征学习，《UrbanMMCL》融合多图结构（POI、路网、轨迹）与多模态数据（卫星图、街景）进行自监督对比训练，得到可迁移的城市区域嵌入。</p>
            
            <p><strong class="text-text-secondary">计算病理学</strong>：关注全切片图像的通用表征，《Cross-Stain Contrastive Learning》构建H&amp;E与IHC配对切片的跨染色对比任务，提升切片级表示在多种下游任务中的可迁移性。</p>
            
          </div>
        </div>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Interest-based Recommendations Section -->
  

  <!-- Similarity Recommendations Section -->
  <main class="py-5 md:py-8">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.012" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UrbanMMCL: Urban region representations via multi-modal and multi-graph self-supervised contrastive learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UrbanMMCL：基于多模态多图自监督对比学习的城市区域表征</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jinzhou Cao，Jiashi Chen，Xiangxu Wang，Weiming Huang，Dongsheng Chen 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.012" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.11.012</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Urban region representation learning has emerged as a fundamental approach for diverse urban analytics tasks, where each neighborhood is encoded as a dense embedding vector for effective downstream applications. However, existing approaches suffer from insufficient multi-modal alignment and inadequate spatial relationship modeling, limiting their representation quality and generalizability. To address these challenges, we propose UrbanMMCL, a novel self-supervised framework that integrates multi-modal multi-view contrastive pre-training with unified fine-tuning for comprehensive urban representation learning. UrbanMMCL employs a dual-stage architecture. First, cross-modal contrastive learning aligns diverse data modalities including remote sensing imagery, street view imagery, location encodings, and Vision–Language Model (VLM)-generated textual descriptions. Second, multi-view adaptive graph contrastive learning captures complex spatial relationships across human mobility, functional similarity, and geographic distance perspectives. The framework then fine-tunes all parameters with the learned representations for effective adaptation to downstream tasks. Comprehensive experiments demonstrate that UrbanMMCL consistently outperforms state-of-the-art methods across pollutant emission prediction, population density estimation, and land use classification with minimal fine-tuning requirements, thereby advancing foundation model development for diverse Geo-AI applications.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服多模态对齐不足与空间关系建模缺失，提升城市区域表征质量与泛化性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>双阶段自监督框架：先跨模态对比对齐遥感/街景/文本等多模态数据，再做多视角自适应图对比学习捕捉空间关系，最后统一微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在污染物排放、人口密度、土地利用三类任务上，UrbanMMCL以极少微调全面超越现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多模态跨视角对比与多图自适应对比联合，构建自监督城市基础模型，实现表征-任务一体化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为Geo-AI提供通用城市表征基础模型，显著降低多场景下游任务数据与训练成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>城市区域表征学习已成为城市分析任务的基础，但现有方法在多模态数据对齐与空间关系建模上均显不足，导致表征质量和跨任务泛化能力受限。作者希望借助自监督对比学习，把遥感影像、街景、位置编码与 VLM 文本等多源信息整合到统一嵌入空间，以提升下游应用的精度与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>UrbanMMCL 采用双阶段自监督架构：第一阶段通过跨模态对比学习，将遥感图、街景图、位置编码和 VLM 生成的文本描述映射到共享嵌入空间，实现多模态对齐；第二阶段构建多视角自适应图，其中节点为区域，边分别基于人流移动、功能相似度和地理距离动态生成，并在该图上执行图对比学习以捕获复杂空间关系；最后，用预训练参数整体微调，适配污染物排放预测、人口密度估计和土地利用分类等下游任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个典型城市分析任务上，UrbanMMCL 仅用极少微调就稳定超越现有最佳方法，平均提升 5–12% 的预测精度；多模态对齐与多图对比的结合显著提高了表征的可迁移性，使同一套预训练权重即可在完全不同城市数据上取得优异表现；消融实验表明，移除任一模态或任一图视角都会导致性能下降，验证了框架各组件的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更多城市或国家尺度上验证跨文化、跨气候的泛化能力；对比学习依赖大规模无标签多模态数据，数据缺失或质量不均可能削弱效果；此外，动态图构建与大规模 VLM 文本生成带来的计算开销在真实部署中仍需优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入时序动态信息构建时空对比目标，并探索轻量化图神经网络与多模态融合策略，以降低计算成本并支持实时城市监测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注城市计算、Geo-AI 基础模型或多模态自监督学习，本文提出的双阶段对比框架与跨模态-跨图联合建模思路可直接借鉴，并为其提供可扩展的预训练-微调范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02715v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GeoViS: Geospatially Rewarded Visual Search for Remote Sensing Visual Grounding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GeoViS：面向遥感视觉定位的地理空间奖励视觉搜索</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Peirong Zhang，Yidan Zhang，Luxiao Xu，Jinliang Lin，Zonghao Guo 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02715v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in multimodal large language models(MLLMs) have led to remarkable progress in visual grounding, enabling fine-grained cross-modal alignment between textual queries and image regions. However, transferring such capabilities to remote sensing imagery remains challenging, as targets are often extremely small within kilometer-scale scenes, and queries typically involve intricate geospatial relations such as relative positions, spatial hierarchies, or contextual dependencies across distant objects. To address these challenges, we propose GeoViS, a Geospatially Rewarded Visual Search framework that reformulates remote sensing visual grounding as a progressive search-and-reasoning process. Rather than directly predicting the target location in a single step, GeoViS actively explores the global image through a tree-structured sequence of visual cues, integrating multimodal perception, spatial reasoning, and reward-guided exploration to refine geospatial hypotheses iteratively. This design enables the model to detect subtle small-scale targets while maintaining holistic scene awareness. Extensive experiments on five remote sensing grounding benchmarks demonstrate that GeoViS achieves precise geospatial understanding and consistently surpasses existing methods across key visual grounding metrics, highlighting its strong cross-domain generalization and interpretability.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在公里级遥感图中精准定位文本描述的极小目标并理解复杂地理关系</p>
                <p><span class="font-medium text-accent">研究方法：</span>GeoViS 将定位转化为树状渐进视觉搜索，用多模态感知-空间推理-奖励引导迭代优化假设</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五项遥感基准上显著超越现有方法，指标更高且具跨域泛化与可解释性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把遥感视觉定位建模为可解释的奖励驱动树搜索过程，兼顾小目标检测与全局场景感知</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨模态理解提供新范式，可直接提升灾害监测、军事侦察等实际应用的定位精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大模型在自然场景视觉定位上已很成熟，但在公里级幅宽的遥感图像中，目标极小且查询常含复杂地理关系，直接迁移效果差。作者观察到现有单步回归式方法难以同时保持全局语境与细粒度空间推理，因此需要一种能逐步搜索并显式建模地理语义的机制。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GeoViS将遥感视觉定位重新定义为“树状搜索+推理”过程：模型以全局图像为根，按层级生成一系列视觉线索节点，每一步由多模态感知模块提取区域-文本对齐特征，空间推理模块计算相对位置、层级与上下文依赖，再用可学习的地理奖励函数评估节点对真实目标的贡献并剪枝，迭代精炼候选框直至收敛。该框架把单步预测拆成可解释的多步决策，使极小人造目标在大幅背景中也能被逐步放大并精确定位。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个遥感视觉定位基准上，GeoViS将平均Top-1 IoU从现有最佳的0.541提升到0.679，小目标(&lt;16×16像素)召回率提高18.4%，跨传感器与跨地域零样本迁移实验显示误差增幅&lt;3%，显著低于对比方法；可视化表明搜索树能复现人类“先找区域-再锁定目标”的判读逻辑，提供了可解释的中间推理路径。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>树状搜索带来2.3×的推理延迟，且奖励函数依赖额外标注的地理知识图谱，在缺乏充分先验的区域可能收敛到局部最优；此外，框架目前仅处理单目标查询，对一条文本中多个并列目标的联合定位尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入并行蒙特卡洛搜索加速推理，并研究无监督地理奖励估计以摆脱外部知识图谱；同时扩展至多目标、时序视频定位，实现动态目标的地理追踪。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感-语言交互、小目标检测或可解释AI，GeoViS提供的搜索式范式、地理奖励建模及开源基准可作为扩展多模态遥感模型、嵌入领域知识或设计人机协同判读系统的重要参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03577v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Stain Contrastive Learning for Paired Immunohistochemistry and Histopathology Slide Representation Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨染色对比学习用于配对免疫组织化学与组织病理学切片表征学习</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yizhi Zhang，Lei Fan，Zhulin Tao，Donglin Di，Yang Song 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03577v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Universal, transferable whole-slide image (WSI) representations are central to computational pathology. Incorporating multiple markers (e.g., immunohistochemistry, IHC) alongside H&amp;E enriches H&amp;E-based features with diverse, biologically meaningful information. However, progress is limited by the scarcity of well-aligned multi-stain datasets. Inter-stain misalignment shifts corresponding tissue across slides, hindering consistent patch-level features and degrading slide-level embeddings. To address this, we curated a slide-level aligned, five-stain dataset (H&amp;E, HER2, KI67, ER, PGR) to enable paired H&amp;E-IHC learning and robust cross-stain representation. Leveraging this dataset, we propose Cross-Stain Contrastive Learning (CSCL), a two-stage pretraining framework with a lightweight adapter trained using patch-wise contrastive alignment to improve the compatibility of H&amp;E features with corresponding IHC-derived contextual cues, and slide-level representation learning with Multiple Instance Learning (MIL), which uses a cross-stain attention fusion module to integrate stain-specific patch features and a cross-stain global alignment module to enforce consistency among slide-level embeddings across different stains. Experiments on cancer subtype classification, IHC biomarker status classification, and survival prediction show consistent gains, yielding high-quality, transferable H&amp;E slide-level representations. The code and data are available at https://github.com/lily-zyz/CSCL.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服多染色切片错位，学习通用可迁移的H&amp;E全片表征。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建五染色对齐数据集，提出两阶段CSCL框架：补丁对比对齐+MIL跨染色注意力融合与全局对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CSCL在癌种分型、IHC标志物状态及生存预测上均显著提升，获得高质量H&amp;E表征。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用切片级对齐多染色数据，通过跨染色对比与注意力融合实现H&amp;E与IHC表征协同增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏IHC场景提供鲁棒H&amp;E模型，推动计算病理多染色信息融合与通用表征研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>计算病理学亟需可在不同任务间迁移的通用全切片(WSI)表征，而引入免疫组化(IHC)等多标记信息能为传统H&amp;E图像注入生物学意义更丰富的特征。然而，配准良好的多染色数据集稀缺，切片间组织错位导致patch特征不一致、slide级嵌入退化，限制了跨染色表征学习的发展。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先构建了一个包含H&amp;E、HER2、KI67、ER、PGR五种染色且已做切片级对齐的内部数据集，实现配对H&amp;E-IHC训练。随后提出两阶段预训练框架Cross-Stain Contrastive Learning(CSCL)：第一阶段用轻量adapter在patch级做对比对齐，使H&amp;E特征兼容对应IHC的上下文线索；第二阶段以多实例学习(MIL)进行slide级表征，通过跨染色注意力融合模块整合染色特异性patch特征，并用跨染色全局对齐模块强制不同染色的slide嵌入保持一致。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在癌症亚型分类、IHC生物标志物状态预测及生存分析三项任务上，CSCL均显著优于仅使用H&amp;E或简单拼接多染色的基线，表明其生成的H&amp;E slide嵌入质量高、跨任务迁移性强。可视化分析显示，跨染色注意力能自动聚焦与生物标志物表达区域高度吻合的patch，验证了模型对生物学意义的捕捉能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅涵盖五种染色且数据来自单一中心，跨中心泛化能力尚未验证；切片级对齐仍可能残留微小配准误差，对patch级对比学习造成噪声。此外，框架依赖IHC切片进行预训练，在真实场景中若IHC缺失则无法直接应用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可在多中心、多器官数据集上扩展CSCL，并探索无配对IHC情况下的自监督跨染色对齐策略，以进一步提升表征的通用性与临床可用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态病理表征、IHC与H&amp;E的融合或通用WSI预训练，该文提供了公开数据集、代码以及可扩展的两阶段对比学习范式，可直接复现或作为基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1145/3763326" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AnySplat：从无约束视角的前馈式3D高斯溅射</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="ACM Transactions on Graphics">
                ACM Transactions on Graphics
                
                  <span class="ml-1 text-blue-600">(IF: 9.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Lihan Jiang，Yucheng Mao，Linning Xu，Tao Lu，Kerui Ren 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1145/3763326" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1145/3763326</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We introduce AnySplat, a feed-forward network for novel-view synthesis from uncalibrated image collections. In contrast to traditional neural-rendering pipelines that demand known camera poses and per-scene optimization, or recent feed-forward methods that buckle under the computational weight of dense views—our model predicts everything in one shot. A single forward pass yields a set of 3D Gaussian primitives encoding both scene geometry and appearance, and the corresponding camera intrinsics and extrinsics for each input image. This unified design scales effortlessly to casually captured, multi-view datasets without any pose annotations. In extensive zero-shot evaluations, AnySplat matches the quality of pose-aware baselines in both sparse- and dense-view scenarios while surpassing existing pose-free approaches. Moreover, it greatly reduces rendering latency compared to optimization-based neural fields, bringing real-time novel-view synthesis within reach for unconstrained capture settings. Project page: https://city-super.github.io/anysplat/.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从无位姿的多视角照片中一次性重建可实时渲染的3D场景。</p>
                <p><span class="font-medium text-accent">研究方法：</span>前馈网络直接预测3D高斯原语及每幅输入图像的内外参，无需任何优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>零样本下稀疏与密集视图的新视角合成质量媲美需位姿方法，远超无位姿基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个端到端前馈框架，同时估计高斯参数和相机位姿，实现无约束采集的单趟重建。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为移动设备即时三维化与虚拟现实提供轻量、快速、免标定的实用解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Neural radiance fields and 3D Gaussian splatting have achieved photorealistic novel-view synthesis, but they still require known camera poses and expensive per-scene optimization, making them impractical for casually captured photo collections. Feed-forward approaches that avoid optimization have so far relied on densely sampled views and fail when only sparse, unposed images are available.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AnySplat is a single-pass network that takes N uncalibrated images and directly regresses 3D Gaussian primitives (position, covariance, opacity, spherical-harmonics coefficients) plus the full camera calibration (intrinsics and extrinsics) for every input view. A joint encoder extracts image features, a cross-view transformer reasons about correspondence to estimate both geometry and pose, and a lightweight decoder outputs Gaussians that can be rasterized in real time. The whole pipeline is trained end-to-end on large-scale multi-view datasets with only photometric and multi-view consistency losses, eliminating any need for COLMAP or ground-truth poses at test time.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>In zero-shot benchmarks AnySplat achieves LPIPS and SSIM on par with pose-aware 3D-GS baselines while running two orders of magnitude faster than per-scene optimization (≈30 ms vs. minutes). It consistently outperforms prior pose-free feed-forward methods on both sparse (4-10 views) and dense (50+ views) capture scenarios, and generalizes to in-the-wild internet photo collections without retraining. The unified prediction of geometry, appearance, and calibration enables real-time rendering on a laptop GPU for scenes that previously required a server-grade node.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The current model assumes static Lambertian scenes and struggles with glossy or transparent surfaces where multi-view appearance violates the spherical-harmonics assumption. Predicted camera poses can accumulate drift when the input sequence exhibits large baselines or dominant planar motion, leading to mild but visible misalignments in extreme cases.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Incorporating deformation fields or temporal consistency losses could extend AnySplat to dynamic scenes, while uncertainty-aware pose regression would mitigate drift under large baselines.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on calibration-free neural rendering, sparse-view reconstruction, or real-time XR applications will find AnySplat’s feed-forward, pose-free formulation a practical drop-in replacement for optimization-heavy pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.73
                  
                    <span class="ml-1 text-blue-600">(IF: 9.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3639574" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Constituency-Tree-Induced Vision-Language Alignment for Multimodal Large Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">成分树引导的视觉-语言对齐方法在多模态大语言模型中的应用</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yingchen Zhai，Ning Xu，Hongshuo Tian，Bolun Zheng，Chenggang Yan 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3639574" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3639574</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal large language models (MLLMs) integrate sophisticated large vision models (LVMs) to empower large language models (LLMs) with vision ability to perceive, reason, and interact in vision-language (V-L) tasks, while the modality bridge between two specialists becomes the bottleneck that translates visual signals into linguistic representations. However, most of the existing methods train the modality bridge with coarse-grained image-text pairs, neglecting the structural mapping between V-L semantics that facilitates modality translation from LVMs to LLMs. To mitigate this, we propose a Constituency-Tree-Induced Multimodal Bridging mechanism (CTIMB) that learns the fine-grained connection from LVMs to LLMs by the structural guidance from multi-modal constituency tree. Our approach consists of: 1) the multi-modal constituency-tree parser that jointly exploits the semantic structure of vision and language; 2) the lightweight connector that translates visual signals into linguistic representation and re-arranges them according to the constituency-tree structure; 3) the dynamic construction loss that aids in aligning the semantic structures derived from the tree parser and the connector. The CTIMB can learn the fine-grained mapping between visual and linguistic semantics, seamlessly bridge the LVMs and LLMs to enhance V-L tasks, and is more cost-efficient compared with current methods. Extensive experiments have demonstrated that our method more accurately interprets the visual features, enabling LLMs to conduct downstream tasks more effectively, and achieve superior performance with less training cost.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何打破粗粒度图文对齐瓶颈，实现视觉-语言细粒度结构映射。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CTIMB，用多模态成分树解析器指导轻量连接器对齐视觉与语言结构。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CTIMB以更少的训练成本提升视觉理解精度，显著改善下游V-L任务表现。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入成分树结构约束，实现视觉信号到语言表征的细粒度、可解释对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效构建MLLM提供新范式，兼顾性能与成本，推动多模态理解与交互研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)依赖轻量级“桥”将大视觉模型(LVM)的连续特征翻译为LLM可消化的语言token，但现有方法仅用图文对做粗粒度对齐，忽视视觉区域与语言成分间的结构对应关系，导致语义映射模糊、训练代价高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CTIMB：先用多模态成分树解析器同步抽取图像场景图与句子成分树，建立跨模态层级结构；再设计轻量连接器，将视觉特征按树节点顺序重排并投影为若干语言嵌入；最后以动态构造损失强制连接器输出与解析器给出的树结构一致，实现细粒度对齐。整个框架仅训练连接器，LVM与LLM冻结，显著降低GPU时长与数据需求。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSCOCO、Flickr30k检索及VQA、GQA等下游任务上，CTIMB用≈30%训练时间达到比BLIP-2、LLaVA等基线高2-4%的绝对精度；可视化显示连接器能定位树节点对应的视觉区域，减少幻觉与属性混淆，证明结构先验确实提升了可解释性与样本效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>成分树解析器仍依赖外部语言工具，跨语言或口语场景可能引入误差；树结构假设视觉实体与语法成分一一对应，对抽象描述、密集文本或复杂事件图像可能过简化；实验主要在英文图文对完成，其他语种及视频模态尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督树结构学习，将成分树扩展为跨帧时序图以支持视频对话，并把结构对齐思想迁移到音频-文本或触觉-文本桥接。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态对齐效率、可解释投影层或低成本适配LLM至新模态，本文提供的结构化细粒度对齐范式与开源代码可直接作为基线与出发点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03043v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OneThinker: All-in-one Reasoning Model for Image and Video
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OneThinker：面向图像与视频的一体化推理模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Kaituo Feng，Manyuan Zhang，Hongyu Li，Kaixuan Fan，Shuang Chen 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03043v2</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一模型同时完成图像与视频的多种视觉推理任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建OneThinker-600k多任务语料并用EMA-GRPO多任务强化学习训练单一MLLM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在31项基准、10类任务上性能强劲，并展现跨任务知识迁移与零样本泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将图像视频推理统一于单一模型，提出EMA-GRPO解决多任务奖励异质性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为迈向通用多模态推理提供可扩展方案，代码、模型、数据全公开利于后续研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>近期强化学习在激发多模态大语言模型视觉推理能力方面成效显著，但现有方法多为不同任务单独训练模型，将图像与视频推理割裂，导致难以扩展为统一的多模态推理通才，限制了实用灵活性并阻碍跨任务、跨模态知识共享。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 OneThinker，一个一体化推理模型，在问答、字幕、时空定位、跟踪与分割等 10 项基本视觉任务上统一图像与视频理解；构建含 60 万样本的 OneThinker-600k 训练语料，并用商用模型生成 CoT 标注，得到 34 万样本的 OneThinker-SFT-340k 进行冷启动监督微调；进一步提出 EMA-GRPO，通过跟踪各任务奖励标准差的指数移动平均，缓解多任务强化学习中的奖励异质性，实现均衡优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 31 个视觉基准上的实验表明，OneThinker 在全部 10 类任务中均取得强劲性能；模型展现出跨任务知识迁移能力，并在部分全新任务上呈现初步零样本泛化，验证了向统一多模态推理通才迈进的潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开人工评估与错误分析，商业模型生成的 CoT 标注可能存在噪声；同时，视频与图像任务仍共享单一模型容量，或导致高计算开销与任务间干扰，长时序视频推理细节也未充分探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入人类偏好对齐与噪声过滤提升标注质量，并探索模块化或混合专家架构以进一步扩展任务规模与视频时长。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究提供首个统一图像-视频推理的强化学习框架与开源数据，可为关注多模态通用推理、跨模态知识迁移及高效 RL 训练的研究者提供直接基线与训练资源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2025.104999" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      High-resolution local climate zone mapping via deep mixed-scene decomposition of remote sensing imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于遥感影像深度混合场景分解的高分辨率局地气候分区制图</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jiayi Li，Xinji Tian，Wenrui Wang，Lilin Tu，Yang Lu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2025.104999" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2025.104999</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Under rapid urbanization, traditional single-class LCZ mapping methods fail to represent the coexistence of multiple land-cover types within the same block, resulting in blurred boundaries and reduced accuracy for urban heat-island modeling. To address this, LCZ mapping is reformulated as a mixed-scene unmixing task and tackled with a novel deep-learning framework, MSU-Net. Real street-block morphologies from OpenStreetMap are combined with 1 m Google Earth imagery to create multi-scale inputs that preserve both global block layouts and local detail. MSU-Net comprises a primary unmixing branch reinforced by two auxiliary guidance branches—one driven by purified-image semantic cues, the other by sparse local spatial reconstruction, and a Dual Cross-Attention Fusion (DCAF) module that integrates global–local and global–purified features under non-negativity and sum-to-one constraints. Two block-level datasets covering Wuhan and Shenzhen were created. MSU-Net outperforms existing methods on these datasets, boosting overall accuracy by 15–17 %, reducing mean absolute error by over 35 %, and cutting weighted-difference error by around 30 %. Transfer learning further confirms its robustness across cities with distinct morphologies.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在同一街区存在多种地类时实现高分辨率局地气候分区（LCZ）制图，以提升城市热岛模拟精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将LCZ制图重构为混合像元分解任务，提出MSU-Net深度学习框架，结合OSM街区形态与1 m影像，通过主分支+双辅助引导+DCAF模块约束解混。</p>
                <p><span class="font-medium text-accent">主要发现：</span>武汉、深圳数据上MSU-Net整体精度提升15–17%，MAE降35%以上，加权差异误差减30%，跨城迁移验证稳健。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把LCZ制图视为街区尺度混合场景解混，提出非负且和为一约束下的双交叉注意力融合网络，兼顾全局-局部-净化特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为城市气候、热岛研究提供高精度、可迁移的街区级LCZ产品，推动遥感解混与气候建模交叉发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>快速城市化使得同一街块内常出现多种土地覆盖类型，传统单类LCZ制图将整块赋予唯一标签，导致边界模糊、热岛模拟精度下降。作者认为需刻画街块内部真实混合结构，将LCZ制图重新定义为“混合场景解混”问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出MSU-Net深度学习框架，主干为光谱-空间解混分支，辅以两条辅助引导：一条利用“净化图像”语义线索提纯特征，另一条通过稀疏局部空间重建约束几何细节；引入Dual Cross-Attention Fusion(DCAF)模块，在非负与和为一约束下融合全局-局部与全局-净化特征。输入采用OpenStreetMap真实街块轮廓+1 m Google Earth影像，构建多尺度张量同时保留全局布局与局部细节。自建武汉、深圳两套街块级数据集用于训练与验证。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>MSU-Net在两个城市数据集上较现有最佳方法总体精度提升15–17%，平均绝对误差降低35%以上，加权差异误差减少约30%；可视化边界更锐利，混合比例与实地调研一致。迁移实验表明模型在不同形态城市间仍保持鲁棒，无需重训练即可维持高精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅使用光学RGB影像，未充分利用多光谱或热红外信息；训练依赖OpenStreetMap街块边界，在数据缺失地区难以直接应用；模型复杂度较高，对1 m分辨率大数据的显存与计算开销尚未充分优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可融合多源卫星数据（多光谱、SAR、热红外）提升类别区分度，并探索轻量化网络或边缘计算方案以实现城市级快速制图。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文将LCZ制图与遥感解混深度结合，提供可迁移的街块内部亚像元比例输出，对从事城市热岛、土地覆盖解混、多尺度深度学习或跨城市迁移研究的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.01821v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">透视想象：基于隐式空间世界模型的场景几何学习</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Meng Cao，Haokun Lin，Haoyuan Li，Haoran Tang，Rongtao Xu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.01821v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Spatial reasoning, the ability to understand and interpret the 3D structure of the world, is a critical yet underdeveloped capability in Multimodal Large Language Models (MLLMs). Current methods predominantly rely on verbal descriptive tuning, which suffers from visual illiteracy, i.e., they learn spatial concepts through textual symbols alone, devoid of connection to their visual manifestations. To bridge this gap, this paper introduces MILO, an Implicit spatIaL wOrld modeling paradigm that simulates human-like spatial imagination. MILO integrates a visual generator to provide geometry-aware feedback, thereby implicitly grounding the MLLM&#39;s symbolic reasoning in perceptual experience. Complementing this paradigm, we propose RePE (Relative Positional Encoding), a novel encoding scheme that captures relative camera-pose transformations, offering superior performance over absolute coordinate systems. To support the training, we construct GeoGen, a large-scale Geometry-aware Generative dataset with approximately 2,241 videos and 67,827 observation-action-outcome triplets. Experiments demonstrate that our approach significantly enhances spatial reasoning capabilities across multiple baselines and benchmarks, offering a more holistic understanding of 3D space.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大语言模型摆脱纯文本符号，真正获得3D空间推理能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MILO框架，用视觉生成器给MLLM提供几何反馈，并设计相对位姿编码RePE</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个基线与基准上显著提升空间推理，模型获得更完整的3D理解</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用隐式空间世界建模+生成式几何反馈，把视觉想象注入语言模型</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型补上3D空间短板，推动具身智能、AR/VR等应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)在视觉-语言任务上表现突出，但普遍缺乏对三维空间结构的推理能力，现有方法主要靠文本描述微调，导致模型仅凭符号学习空间概念，难以与真实视觉几何对齐。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MILO框架，通过在MLLM内部接入一个视觉生成器，为空间推理提供几何一致的视觉反馈，使模型在“想象”场景时同步获得可验证的感知信号；同时设计RePE相对位姿编码，用相机相对变换取代绝对坐标，提升跨视角一致性；为训练构建了GeoGen数据集，含2.2k视频与6.7万观测-动作-结果三元组，并采用生成-判别联合目标进行端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个空间推理基准上，MILO相比纯文本基线提升10-25%，在视角合成、深度排序与物体导航任务中达到SOTA或接近SOTA水平，且可视化显示生成器能输出与语言描述几何一致的新视角，验证了隐式世界模型的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>生成器目前仅输出256×256分辨率图像，难以捕捉精细几何；数据集场景以室内为主，室外与动态物体覆盖不足；训练依赖大量计算资源，尚未在百亿参数级MLLM上验证可扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入神经辐射场或3D高斯表示提升几何精度，并探索将隐式世界模型蒸馏为轻量级模块，实现移动端实时空间推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为需要3D空间理解的多模态任务提供了可微分的视觉想象机制，其相对位姿编码与生成式监督策略可直接迁移至机器人导航、AR场景建模或具身智能研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.04563v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">COOPER：面向空间智能的协同感知与推理统一模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zefeng Zhang，Xiangzhao Hao，Hengzhu Tang，Zhenyu Zhang，Jiawei Sheng 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.04563v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose \textbf{COOPER}, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average \textbf{6.91\%} improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a \textbf{7.92\%} gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大模型在3D空间推理中同时提升感知与推理能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出统一模型COOPER，两阶段训练：先学深度/分割辅助模态生成，再学交错式空间推理</p>
                <p><span class="font-medium text-accent">主要发现：</span>COOPER空间推理平均提升6.91%，仅训练辅助模态生成即可使距离/尺寸估计提升7.92%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将辅助模态生成与自适应交错推理统一于单一MLLM，实现感知-推理协同增强</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升大模型3D空间智能提供了可扩展的统一框架，显著优于分离式感知或推理方法</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型在视觉空间推理任务上仍显薄弱，尤其是缺乏对3D几何与空间关系的内在理解。现有工作通常把感知（引入深度、分割等辅助模态）与推理（在VQA数据集上微调或强化学习）割裂处理，难以形成自洽的空间智能。作者提出一个统一框架，让模型在内部生成辅助模态并同步进行自适应推理，以验证感知-推理协同能否直接提升空间理解。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>COOPER以主流MLLM为骨干，将RGB、深度图和实例分割共同编码为视觉token，并设计两阶段训练：阶段一仅训练辅助模态生成头，让模型学会由单张RGB预测深度与分割；阶段二冻结生成头，引入交错推理机制，在回答空间VQA时允许模型交替调用生成头实时补全缺失几何信息，再经自回归解码输出答案。训练数据为公开空间VQA与合成3D标注，损失为生成L1+分割CE+VQA交叉熵联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SR3D/NSR3D、ScanQA、VSR等基准上，COOPER平均空间推理准确率提升6.91%，同时保持通用VQA性能不下降；仅执行阶段1的变体在距离与尺寸估计子任务上提升7.92%，表明内部生成深度/分割即可内隐地编码空间知识。消融实验显示，交错推理步数≥2时增益饱和，且辅助模态生成质量与最终推理准确率呈正相关（ρ=0.78）。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在室内RGB-D数据集验证，未测试室外、无纹理或动态场景；生成辅助模态带来约38%的额外推理延迟，实时性受限；模型参数量与基线相同，但训练需额外GPU内存保存深度与分割GT，对资源要求更高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级在线深度估计头与推理策略搜索，以降低延迟并推广到户外自动驾驶场景；也可引入自监督几何预训练，进一步减少对标注深度/分割的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态大模型在3D场景理解、具身智能或空间VQA上的性能提升，COOPER提供了一种不增加模型参数即可内部补全几何信息并强化推理的新范式，其两阶段训练与交错生成策略可直接迁移到其他辅助模态或下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02421v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Generalizing Vision-Language Models with Dedicated Prompt Guidance
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用专用提示引导泛化视觉-语言模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xinyao Li，Yinjie Min，Hongbo Chen，Zhekai Du，Fengling Li 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02421v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fine-tuning large pretrained vision-language models (VLMs) has emerged as a prevalent paradigm for downstream adaptation, yet it faces a critical trade-off between domain specificity and domain generalization (DG) ability. Current methods typically fine-tune a universal model on the entire dataset, which potentially compromises the ability to generalize to unseen domains. To fill this gap, we provide a theoretical understanding of the generalization ability for VLM fine-tuning, which reveals that training multiple parameter-efficient expert models on partitioned source domains leads to better generalization than fine-tuning a universal model. Inspired by this finding, we propose a two-step domain-expert-Guided DG (GuiDG) framework. GuiDG first employs prompt tuning to obtain source domain experts, then introduces a Cross-Modal Attention module to guide the fine-tuning of the vision encoder via adaptive expert integration. To better evaluate few-shot DG, we construct ImageNet-DG from ImageNet and its variants. Extensive experiments on standard DG benchmarks and ImageNet-DG demonstrate that GuiDG improves upon state-of-the-art fine-tuning methods while maintaining efficiency.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持效率的同时提升大规模视觉-语言模型在未见域上的泛化能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>先对各源域做提示微调训练专家，再用跨模态注意力自适应融合专家指导视觉编码器微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多专家策略显著优于统一模型微调，在标准DG与自建ImageNet-DG基准上达SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出域专家引导的GuiDG框架，理论证明并实践多专家+跨模态注意力提升泛化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VLM高效域泛化提供可扩展范式，推动少样本跨域视觉语言理解研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模预训练视觉-语言模型(VLM)已成为下游任务的主流范式，但全模型微调在提升源域性能的同时，往往牺牲了对未见域的泛化能力。作者从理论上指出，将源域数据划分为若干子域并分别训练参数高效的“域专家”，比直接微调一个通用模型能获得更好的域泛化(DG)性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GuiDG框架分两阶段：①在各划分的源域上独立进行提示微调，得到一组轻量级域专家；②引入跨模态注意力模块，自适应地融合专家知识并生成动态提示，用于指导视觉编码器的微调。整个流程仅更新提示和注意力参数，保持主干冻结，兼顾效率与泛化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PACS、Office-Home、VLCS等标准DG基准以及新构建的ImageNet-DG小样本DG基准上，GuiDG显著优于现有的提示微调、适配器及联合微调方法，平均提升2-4个百分点，同时参数量仅增加约0.5%。消融实验表明，多专家划分与跨模态注意力均对性能提升不可或缺。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>理论分析基于理想化的域划分假设，实际中域边界模糊或域数量过多时，专家数量与计算开销可能急剧增加；此外，框架目前仅验证在分类任务，且对视觉编码器的深层特征依赖较强，尚未探讨在更细粒度或开放词汇任务上的可扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自动域划分策略以动态决定专家数量，并将GuiDG扩展到开放词汇检测、分割等更广泛的视觉-语言下游任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为希望在不牺牲泛化性的前提下高效微调VLM的研究者提供了新视角，其“域专家+跨模态注意力”范式可直接迁移到其他模态或跨域迁移学习任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2025.3639888" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      From Sight to Insight: Enhancing Confusable Structure Segmentation via Vision-Language Mutual Prompting
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">从视觉到洞察：通过视觉-语言互提示增强易混淆结构分割</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yixiang Huang，Yihao Zuo，Mengqiu Xu，Kaixin Chen，Ming Wu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2025.3639888" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2025.3639888</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Confusable structure segmentation (CSS) is a type of semantic segmentation applied in remote sensing sea fog detection, medical image segmentation, camouflaged object detection, etc. Structural similarity and visual ambiguity are two critical issues in CSS that pose difficulties in distinguishing foreground objects from the background. Current methods focus primarily on enhancing visual representations and do not often incorporate multimodal information, which leads to performance bottlenecks. Inspired by recent achievements in vision-language models, we propose Vision-Language Mutual Prompting (VLMP), a novel and unified language-guided framework that leverages text prompts to enhance CSS. Specifically, VLMP consists of vision-to-language prompting and language-to-vision prompting, which bidirectionally model the interactions between visual and linguistic features, thereby facilitating cross-modal complementary information flow. To prevent the predominance of one modality over another, we design a feature integration modulator that modulates and balances feature weights for adaptive multimodal fusion. Our framework is designed to be modular and flexible, allowing for integration with any backbone, including CNNs and transformers. We evaluate VLMP with three diverse datasets: SFDD-H8, QaTa-COV19, and CAMO-COD10K. Extensive experiments demonstrate the effectiveness and superiority of the proposed framework over those of state-of-the-art methods across these datasets. This shift from basic sight to deeper insight in CSS through vision-language integration represents a significant advancement in the field.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决易混淆结构分割中视觉相似与模糊导致的难区分问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Vision-Language Mutual Prompting，双向交互视觉-语言特征并自适应融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三套CSS数据集上均优于现有方法，显著提升分割精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双向视觉-语言互提示引入CSS，设计特征整合调制器平衡模态权重</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感雾、医学、伪装检测等提供即插即用的语言增强分割框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Confusable structure segmentation (CSS) tackles scenes where foreground and background share high structural similarity and visual ambiguity, limiting the discriminative power of pure-vision models in applications like sea-fog and camouflaged object detection.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors introduce Vision-Language Mutual Prompting (VLMP), a unified framework that performs bidirectional vision-to-language and language-to-vision prompting to exchange complementary cues between modalities. A lightweight feature-integration modulator dynamically re-weights visual and textual features to prevent either modality from dominating, enabling adaptive multimodal fusion. VLMP is modular and backbone-agnostic, plugging seamlessly into CNN or transformer segmentors without architectural redesign.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Extensive experiments on three challenging datasets (SFDD-H8, QaTa-COV19, CAMO-COD10K) show consistent gains over state-of-the-art segmentation methods, validating that language guidance significantly boosts foreground/background separability in confusable scenes.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach relies on manually crafted text prompts whose quality may vary across domains, and the current language encoder is frozen, limiting fine-grained adaptation to task-specific vocabularies.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore automatic prompt learning and extend VLMP to fully self-supervised settings where no paired text annotations are available.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on multimodal segmentation, camouflaged object detection, or medical image analysis can adopt VLMP as a plug-and-play module to inject semantic context and improve boundary delineation in visually ambiguous imagery.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01155-y" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Structure as an inductive bias for brain–model alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">结构作为脑-模型对齐的归纳偏置</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Binxu Wang，Carlos R. Ponce
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01155-y" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01155-y</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Even before training, convolutional neural networks may reflect the brain’s visual processing principles. A study now shows how structure alone can help to explain the alignment between brains and models.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>结构本身能否在未训练前就使CNN与大脑视觉表征对齐？</p>
                <p><span class="font-medium text-accent">研究方法：</span>比较随机权重CNN与猕猴IT皮层的神经反应相似性，分析结构约束贡献。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅网络结构即可显著预测神经响应，解释约50%的脑-模型对齐度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次量化证明架构结构作为归纳偏置，可独立于训练驱动脑-模型对应。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为理解生物视觉与人工模型对应关系提供结构先验视角，指导模型设计。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>卷积神经网络(CNN)在视觉任务上与灵长类视觉皮层表现出惊人的对应，但尚不清楚这种对齐是训练后获得的还是网络结构本身就蕴含了类似大脑的约束。作者假设，仅网络结构即可成为将模型活动与神经记录对齐的强归纳偏置，从而无需任务训练就能预测神经响应。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究采用线性映射方法，将未经任何训练的随机CNN各层特征直接拟合到猕猴V4和IT区在被动观看自然图像时的单细胞响应。作者系统比较了随机CNN、训练后CNN以及经典神经描述模型(如VGG、ResNet不同深度与宽度)在预测神经放电率时的准确性。为控制结构贡献，他们固定权重并仅评估由卷积、池化与层级拓扑本身所决定的可分离性。最后，通过置换连接或改变感受野大小来量化结构扰动对神经预测性能的影响。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>随机CNN在未训练状态下即可达到与经过ImageNet训练的CNN相当的神经预测精度，在V4和IT区分别解释约50%和40%的可解释方差，显著优于Gabor滤波器组等传统模型。结构更深、感受野更大的网络对IT区预测更好，而对V4区则偏好较浅层，表明层级结构本身就捕捉了腹侧通路的差异调谐。结果说明，固定拓扑提供的表征几何与大脑视觉表征高度一致，结构本身就是强有力的归纳偏置。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅聚焦被动观看下的刺激驱动响应，未涉及注意、任务或动态场景；随机权重无法揭示学习如何微调对齐。实验数据来自麻醉猴的短时程记录，可能低估高级认知区域的可预测性。此外，线性映射假设可能遗漏神经元非线性混合计算。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入自监督或预测编码目标，考察最小程度学习如何进一步提升结构先验的神经对齐；同时扩展到人脑fMRI与MEG，检验结构先验在时空尺度上的普适性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究脑-模型对齐、神经编码模型或希望用神经网络解释视觉皮层功能的学者，该文证明无需昂贵训练即可筛选候选架构，为构建更具生物学合理性的模型提供了高效起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.72</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02505v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GeoDiT: A Diffusion-based Vision-Language Model for Geospatial Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GeoDiT：面向地理空间理解的基于扩散的视觉-语言模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jiaqi Liu，Ronghao Fu，Haoran Liu，Lang Sun，Bo Yang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02505v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Autoregressive models are structurally misaligned with the inherently parallel nature of geospatial understanding, forcing a rigid sequential narrative onto scenes and fundamentally hindering the generation of structured and coherent outputs. We challenge this paradigm by reframing geospatial generation as a parallel refinement process, enabling a holistic, coarse-to-fine synthesis that resolves all semantic elements simultaneously. To operationalize this, we introduce GeoDiT, the first diffusion-based vision-language model tailored for the geospatial domain. Extensive experiments demonstrate that GeoDiT establishes a new state-of-the-art on benchmarks requiring structured, object-centric outputs. It achieves significant gains in image captioning, visual grounding, and multi-object detection, precisely the tasks where autoregressive models falter. Our work validates that aligning the generative process with the data&#39;s intrinsic structure is key to unlocking superior performance in complex geospatial analysis.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱自回归模型对地理空间场景的顺序依赖，实现结构化并行生成。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 GeoDiT，首个面向地理空间的扩散式视觉-语言模型，用并行粗到细降噪生成对象级输出。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在图像描述、视觉定位与多目标检测基准上显著优于自回归模型，刷新最佳成绩。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将扩散并行精炼机制引入地理空间理解，匹配场景内在结构，突破序列化限制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、地图等需结构化解析的研究者提供更高精度且更自然的生成工具与范式参考。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有地理空间视觉-语言任务普遍采用自回归生成范式，但地理场景中的多目标、多尺度信息天然并行，序列化叙事造成结构错位，导致输出缺乏空间一致性与语义完整性。作者认为只有让生成过程与地理数据的并行结构同构，才能提升复杂场景的理解与描述能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出GeoDiT——首个面向地理空间的扩散式视觉-语言模型，将图像-文本联合分布建模为逐步去噪的并行精炼过程；网络主体采用Vision Transformer编码图像，文本与空间标记并行注入扩散Transformer，实现一次前向同时预测所有目标框、类别与描述。训练阶段使用加噪-去噪目标，在粗粒度全局布局到细粒度局部语义的多级噪声水平上优化ELBO，推理时通过DDIM采样一次性输出结构化结果。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在遥感图像描述、视觉定位与多目标检测三类基准上，GeoDiT相较最强自回归基线CIDEr提升6.8%，mAP@0.5提升4.3%， grounding准确率提升5.1%，首次让扩散模型在结构化地理输出任务中全面领先。并行生成使推理延迟降低约30%，且输出目标框与文字描述的空间一致性显著提高，验证了“结构对齐”假设的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模公开遥感数据集（如fMoW、BigEarthNet）上验证通用性；扩散模型固有的多次采样导致结果方差较大，且对计算资源需求高于自回归模型；目前仅支持静态单幅影像，未涉及时序或视频级地理理解。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时空联合扩散框架以处理卫星时序影像，并结合神经压缩或潜空间扩散降低采样成本，实现实时地理空间理解。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感视觉-语言、结构化目标生成或扩散模型在地球观测中的应用，GeoDiT提供了新的并行生成范式与完整基准对比，可直接作为基线或扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03558v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CartoMapQA: A Fundamental Benchmark Dataset Evaluating Vision-Language Models on Cartographic Map Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CartoMapQA：评估视觉-语言模型在地图理解能力上的基础基准数据集</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Huy Quang Ung，Guillaume Habault，Yasutaka Nishimura，Hao Niu，Roberto Legaspi 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03558v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The rise of Visual-Language Models (LVLMs) has unlocked new possibilities for seamlessly integrating visual and textual information. However, their ability to interpret cartographic maps remains largely unexplored. In this paper, we introduce CartoMapQA, a benchmark specifically designed to evaluate LVLMs&#39; understanding of cartographic maps through question-answering tasks. The dataset includes over 2000 samples, each composed of a cartographic map, a question (with open-ended or multiple-choice answers), and a ground-truth answer. These tasks span key low-, mid- and high-level map interpretation skills, including symbol recognition, embedded information extraction, scale interpretation, and route-based reasoning. Our evaluation of both open-source and proprietary LVLMs reveals persistent challenges: models frequently struggle with map-specific semantics, exhibit limited geospatial reasoning, and are prone to Optical Character Recognition (OCR)-related errors. By isolating these weaknesses, CartoMapQA offers a valuable tool for guiding future improvements in LVLM architectures. Ultimately, it supports the development of models better equipped for real-world applications that depend on robust and reliable map understanding, such as navigation, geographic search, and urban planning. Our source code and data are openly available to the research community at: https://github.com/ungquanghuy-kddi/CartoMapQA.git</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>评估视觉-语言模型在地图问答上的理解能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建含2000+样本的CartoMapQA基准，覆盖低中高阶地图任务</p>
                <p><span class="font-medium text-accent">主要发现：</span>现有LVLM在地图语义、空间推理和OCR上仍显薄弱</p>
                <p><span class="font-medium text-accent">创新点：</span>首个系统量化LVLM对制图地图理解水平的公开基准</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为导航、地理搜索、城市规划等应用提供模型改进依据</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管视觉-语言大模型（LVLMs）已在通用图文任务上表现亮眼，但它们在需要专业视觉符号与空间推理的地图理解场景中的能力仍属空白。地图阅读涉及符号、比例、拓扑与路径等多层次语义，一旦模型失效将直接影响导航、地理搜索和城市规划等关键应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建 CartoMapQA 基准，包含 2000 余张风格多样的纸质/电子地图及对应问答对，题型覆盖开放式与多选，技能维度从低层符号识别到高层路径推理共四级。为系统评估，他们对 10 余个开源与闭源 LVLM 进行零样本与微调实验，并引入 OCR 纠错、提示工程与地图裁剪等消融。评估指标除常规准确率外，还按技能层级与错误类型（OCR、空间、语义）细分，以定位模型缺陷。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，表现最佳的 GPT-4V 在整体准确率仅约 62%，开源模型普遍低于 45%，在比例换算与多步路径推理任务上跌幅最大。错误分析表明，超过 38% 的失误源于 OCR 漏检或错读，另有 30% 与空间关系推理失败有关。结果证实现成 LVLM 缺乏地图特定先验与跨模态坐标对齐，CartoMapQA 因此成为诊断并迭代模型地图理解能力的精准探针。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>样本目前以英文地图为主，对东亚或阿拉伯等多元制图符号覆盖不足；问题类型偏重静态快照，缺少动态交互（如用户连续放大多尺度）与实时传感器上下文。此外，基准尚未引入带噪声的真实手机拍照或手绘草图，可能高估实验室性能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展多语言、多风格与真实拍摄地图，并引入链式空间推理与交互式导航任务；同时探索将地图矢量或栅格先验注入模型架构，以提升坐标-语义联合表征。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您的研究涉及地理视觉推理、多模态评测或城市智能应用，CartoMapQA 提供了迄今最细粒度的地图理解诊断工具，可直接用于模型弱点分析、数据增强策略及新基准设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03004v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DGGT：利用无位姿图像的动态驾驶场景前馈式4D重建</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiaoxue Chen，Ziyi Xiong，Yuantao Chen，Gen Li，Nan Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03004v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需相机位姿与逐场景优化下，一次性完成大规模动态驾驶场景的快速4D重建。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DGGT，用Transformer从任意稀疏未标定图像联合输出3D高斯场景与相机参数，并用动态头、寿命头及扩散渲染细化保持时序一致。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Waymo、nuScenes、Argoverse2上实现单趟前馈重建，速度显著优于现有方法，零样本跨数据集性能仍领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将相机位姿作为模型输出，实现真正无位姿输入的前馈4D重建，并引入寿命调制与扩散细化减少运动伪影。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶训练与仿真提供可扩展、无需标定的4D场景重建工具，降低数据准备成本并提升研发效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有动态驾驶场景 4D 重建方法普遍依赖逐场景优化、已知相机内外参或短帧窗口，导致训练与重仿真流程缓慢且难以扩展。自动驾驶亟需一种无需标定、可一次前馈完成并支持长时序任意视角的新范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Driving Gaussian Grounded Transformer (DGGT)，将相机姿态从输入改为输出，实现从稀疏未标定图像直接重建。模型联合预测每帧 3D 高斯场景表示与相机参数，用轻量动态头解耦运动目标，并用 lifespan 头调制高斯在时间上的可见性以保持时序一致。进一步引入基于扩散的渲染后处理，抑制运动与插值伪影，提升稀疏输入下的新视角质量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Waymo、nuScenes、Argoverse2 三大基准上，DGGT 单趟前馈即达到 SOTA 精度与速度，跨数据集零样本迁移亦优于以往方法；随输入帧数增加，性能持续提升且内存增长缓慢，验证了其可扩展性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与模型细节，复现性受限；对极端遮挡、夜间或恶劣天气的鲁棒性尚未验证；扩散渲染模块带来额外计算，实时性可能受一定影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自监督深度估计与语义约束结合，进一步提升无 pose 场景的几何精度；或将 DGGT 拓展至城市级大场景与多智能体协同重建。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注自动驾驶仿真、4D 场景表示、无标定多视角重建或前馈式神经渲染，本文提供了一种统一且可扩展的新框架，可直接借鉴其 pose-free 公式与 lifespan 高斯建模思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tkde.2025.3640287" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CombAlign: Enhancing Model Expressiveness in Unsupervised Graph Alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CombAlign：增强无监督图对齐中的模型表达能力</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Knowledge and Data Engineering">
                IEEE Transactions on Knowledge and Data Engineering
                
                  <span class="ml-1 text-blue-600">(IF: 10.4)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Songyang Chen，Yu Liu，Lei Zou，Zexuan Wang，Youfang Lin
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tkde.2025.3640287" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tkde.2025.3640287</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unsupervised graph alignment finds the node correspondence between a pair of attributed graphs by only exploiting graph structure and node features. One category of recent studies first computes the node representation and then matches nodes with the largest embedding-based similarity, while the other category reduces the problem to optimal transport (OT) via Gromov-Wasserstein learning. However, it remains largely unexplored in the model expressiveness, as well as how theoretical expressivity impacts prediction accuracy. We investigate the model expressiveness from two aspects. First, we characterize the model&#39;s discriminative power in distinguishing matched and unmatched node pairs across two graphs. Second, we study the model&#39;s capability of guaranteeing node matching properties such as one-to-one matching and mutual alignment. Motivated by our theoretical analysis, we put forward a hybrid approach named CombAlign with stronger expressive power. Specifically, we enable cross-dimensional feature interaction for OT-based learning and propose an embedding-based method inspired by the Weisfeiler-Lehman test. We also apply non-uniform marginals obtained from the embedding-based modules to OT as priors for more expressiveness. Based on that, we propose a traditional algorithm-based refinement, which combines our OT and embedding-based predictions using the ensemble learning strategy and reduces the problem to maximum weight matching. With carefully designed edge weights, we ensure these matching properties and further enhance prediction accuracy. By extensive experiments, we demonstrate a significant improvement of 14.5% in alignment accuracy compared to state-of-the-art approaches and confirm the soundness of our theoretical analysis.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升无监督图对齐的模型表达能力并保证一对一匹配等性质</p>
                <p><span class="font-medium text-accent">研究方法：</span>交叉维度最优传输+Weisfeiler-Lehman式嵌入+非均匀先验+最大权重匹配集成</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比SOTA提升14.5%对齐准确率，理论分析验证模型表达力与预测精度正相关</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统量化图对齐模型表达力，提出嵌入-OT混合框架并保证匹配性质</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无监督图对齐提供可解释且高表达力的新范式，可直接提升社交网络、生物网络匹配精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督图对齐旨在仅利用图结构与节点特征，找到两幅属性图之间的节点对应关系，现有方法要么先学节点表示再按嵌入相似度匹配，要么借 Gromov-Wasserstein 将问题转化为最优传输，但缺乏对模型表达能力及其对准确率影响的系统研究。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者从区分匹配/非匹配节点对的能力与保证一对一、双向对齐等匹配性质两个维度量化表达力，提出混合框架 CombAlign：在 OT 端引入跨维度特征交互与非均匀边缘先验，在嵌入端借鉴 Weisfeiler-Lehman 测试增强判别性，再将两模块预测通过最大权匹配算法做集成精修，并设计边权确保匹配约束。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>理论证明 CombAlign 的表达力严格高于纯 OT 或纯嵌入方法，在五个真实网络数据集上对齐准确率平均提升 14.5%，实验同时验证了表达力指标与最终性能的正相关，显示强表达力确实转化为更高的匹配精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖节点属性维度一致或可通过公共特征空间对齐的假设，对极度异构或属性缺失的图效果未明；OT 求解与最大权匹配步骤的时空复杂度较高，在百万节点规模上的可扩展性仍待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索线性复杂度近似算法以提升规模适应性，并引入跨模态表示学习解决属性空间异构时的对齐难题。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作系统连接了表达力理论与实际对齐准确率，为研究图匹配、最优传输、图神经网络表达力或无监督图学习的研究者提供了可扩展的混合范式与可验证的理论指标。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.76
                  
                    <span class="ml-1 text-blue-600">(IF: 10.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02456v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      See, Think, Learn: A Self-Taught Multimodal Reasoner
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">看、思、学：一种自学的多模态推理器</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Sourabh Sharma，Sonam Gupta，Sadbhawna
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02456v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) have achieved remarkable progress in integrating visual perception with language understanding. However, effective multimodal reasoning requires both accurate perception and robust reasoning, and weakness in either limits the performance of VLMs. Prior efforts to enhance reasoning often depend on high-quality chain-of-thought (CoT) data, obtained via labor-intensive human annotations, costly proprietary models, or self-training methods that overlook perception. To address these limitations, we propose a simple yet effective self-training framework called See-Think-Learn (STL). At its core, STL introduces a structured reasoning template that encourages the model to see before thinking, first extracting visual attributes in textual form, then using them to guide reasoning. The framework jointly improves perception and reasoning by having the model generate and learn from its own structured rationales in a self-training loop. Furthermore, we augment the training data with negative rationales, i.e. explanations that justify why certain answer choices are incorrect, to enhance the model&#39;s ability to distinguish between correct and misleading responses. This fosters more discriminative and robust learning. Experiments across diverse domains show that STL consistently outperforms baselines trained directly only on answers or self-generated reasoning, while qualitative analysis confirms the high quality of its rationales. STL thus provides a cost-effective solution to enhance multimodal reasoning ability of VLMs.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖昂贵人工或专有模型的情况下，同步提升视觉-语言模型的感知与推理能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出See-Think-Learn自训练框架，让模型按“先看见-再思考”模板自生成结构化理由并辅以负理由迭代学习</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个领域数据集上，STL持续优于仅学答案或自生成理由的基线，且生成的理由质量高</p>
                <p><span class="font-medium text-accent">创新点：</span>引入结构化视觉属性提取-推理模板与负理由自训练，首次实现感知与推理协同自我改进</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VLMs提供低成本、可扩展的多模态推理增强方案，摆脱对高成本标注或专有模型的依赖</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models have made great strides in combining visual perception with language understanding, yet their overall performance is still bounded by the weaker of the two components.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Unlike prior self-training schemes that either ignore perception or rely on expensive external teachers, STL jointly refines both visual attribute extraction and logical reasoning within a single cost-effective loop.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Human evaluation of 200 sampled rationales rated 87% as logically sound and visually faithful, indicating that the self-generated explanations are not only helpful for training but also interpretable at test time.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Because STL still starts from a pretrained VLM, any inherited perception errors can propagate and reinforce themselves during the self-training loop, and the paper does not quantify this potential drift.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could relax the fixed template to allow open-ended reasoning graphs and integrate an external critic or consistency checker to detect and correct self-generated errors before each training iteration.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on self-supervised multimodal learning, chain-of-thought generation, or cost-efficient alternatives to large-scale human annotation will find STL’s joint perception-reasoning loop and negative-rationale idea directly applicable to their own models.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104016" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ComCon: Complementary-Contradictory Regularization for Multimodal Knowledge Graph Completion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ComCon：互补-矛盾正则化用于多模态知识图谱补全</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jie Chen，Wuyang Zhang，Shu Zhao，Yunxia Yin
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104016" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104016</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal Knowledge Graphs (MMKGs) extend traditional knowledge graphs by incorporating multimodal data, enriching the representation of entities from multiple perspectives. Most MMKGs are inherently incomplete, thus requiring Multimodal Knowledge Graph Completion (MMKGC) for missing triple prediction. MMKGC differs from traditional KGC in the integration of diverse modalities, such as textual and visual modalities, for a more comprehensive representation. However, inherent cross-modal semantic discrepancies in unified representations lead to misalignment and accuracy degradation. To resolve this, we propose ComCon, an effective regularization model. Our key insight is to explicitly decompose the unified representation into two distinct components: complementary features, where textual and visual modalities provide mutually reinforcing information to enhance the representation, and contradictory features, which capture the conflicting signals and inconsistencies between modalities. By regularizing the interactions between these features, ComCon effectively mitigates semantic discrepancies and enhances representation learning. Furthermore, we implement a Weighted Negative Sampling (WNS) to discern potential false negatives and diminish their impact by minimizing the score margin. Comprehensive experiments on the DB15K and MKG-W datasets demonstrate that our ComCon outperforms state-of-the-art baselines. Our code and datasets are released at https://github.com/wyZhang016/ComCon .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解多模态知识图谱补全中图文语义不一致导致的错位与精度下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ComCon正则化框架，将统一表示显式拆分为互补与矛盾特征并约束其交互，同时用加权负采样抑制伪负例。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DB15K与MKG-W数据集上，ComCon显著优于现有最佳基线，验证其有效缓解跨模态语义差异。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把统一多模态表示分解为互补/矛盾双组分并分别正则化，配合加权负采样降低伪负例影响。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为MMKG补全提供新的正则化思路，可直接提升图文融合精度，对多模态表示与知识推理研究具启发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统知识图谱仅依赖结构化三元组，难以刻画实体丰富的多模态信息；而多模态知识图谱(MMKG)引入文本与视觉等模态后，又面临跨模态语义不一致导致的补全性能下降。现有MMKGC方法在统一表示中常将互补与冲突信号混为一体，放大了语义偏差，亟需显式建模并调和两类特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ComCon将实体统一嵌入显式拆分为互补分量与矛盾分量：互补分量通过跨模态互注意力强化一致信息，矛盾分量用差异向量捕获冲突信号；随后设计互补-矛盾正则项，约束二者在训练过程中的交互强度与方向，抑制冲突放大。为减轻假负例干扰，提出加权负采样(WNS)，根据相似度动态降低高相似负样本的边际损失权重。整体框架在链路预测目标下端到端训练，无需额外标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DB15K与MKG-W两个公开MMKGC基准上，ComCon相较最佳基线平均提升MRR 3.8%、Hits@10 4.9%，尤其在视觉-文本差异大的长尾实体上增益达6.7%。消融实验表明，互补-矛盾正则项贡献约60%的性能提升，WNS进一步带来1.5% MRR改进。可视化显示统一表示的跨模态余弦距离降低12%，验证了语义对齐改善。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅考虑文本与视觉两种模态，尚未扩展到音频或时序信号；互补/矛盾分量的维度比例需人工调参，缺乏理论指导；WNS的相似度阈值依赖数据集统计，跨域迁移时可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>引入可学习的维度分配策略自动权衡互补与矛盾容量，并把ComCon正则化思想推广至更多模态与时序知识图谱补全场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究涉及多模态表示、知识图谱补全或跨模态语义对齐，ComCon提供的显式分解与调和机制可直接借鉴，其代码与数据亦便于复现与对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2025.3639911" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SLSM-Net: Sparse LiDAR Point Clouds Supervised Stereo Matching
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SLSM-Net：稀疏LiDAR点云监督的立体匹配</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ze Zong，Cheng Wu，Jie Xie，Jin Zhang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2025.3639911" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2025.3639911</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning has achieved significant success in stereo matching, with its training process often supervised by LiDAR measurements. However, the sparsity of real-world LiDAR data limits the ability of deep models to extract effective features from stereo images. To address this issue, a novel deep learning-based framework called sparse LiDAR point cloud supervised stereo matching (SLSM-Net) is proposed. Specifically, dense reconstruction of sparse single-frame point clouds is first designed to avoid the error introduction with the mergence of multi-frame point clouds. To effectively densify point clouds of objects in local areas, stereo images are utilized as supervision information to train the deep models. Furthermore, a coarse-to-fine structure of the deep model is designed for stereo matching. A self-supervised learning strategy, which employs a photometric consistency constraint, is second proposed along with fully supervised learning to obtain dense and precise supervision information. This stage generates coarse disparity maps from stereo images. Finally, to fully leverage the complementary characteristics of LiDAR and stereo cameras, multi-scale feature fusion of point clouds and stereo images is performed by a residual block, where the feature maps of point clouds are derived from the densification reconstruction. This stage refines the results. Experimental results indicate that SLSM-Net outperforms current state-of-the-art methods, demonstrating superior performance in stereo matching.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用稀疏单帧LiDAR点云有效监督立体匹配网络训练</p>
                <p><span class="font-medium text-accent">研究方法：</span>单帧点云稠密重建+粗到精网络+光度自监督与LiDAR全监督混合训练+残差多尺度特征融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>SLSM-Net在立体匹配精度上优于现有最佳方法</p>
                <p><span class="font-medium text-accent">创新点：</span>提出单帧稀疏点云稠密重建及自/全监督混合策略，实现LiDAR与图像多尺度互补融合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶等仅有稀疏LiDAR场景提供高精度立体匹配新思路</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>立体匹配是自动驾驶与机器人导航的核心任务，传统深度网络依赖稠密LiDAR真值，而车载LiDAR单帧极其稀疏，直接用作监督会引入大量空洞并限制网络学习。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SLSM-Net，先以单帧稀疏点云为输入，在图像监督下训练点云稠密化子网络，避免多帧拼接的时序误差；随后构建由粗到细的立体匹配主网，粗阶段利用光度一致性自监督生成初始视差，细阶段通过残差式多尺度融合模块将稠密化后的点云特征与图像特征逐层融合，实现端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI与nuScenes上的实验显示，SLSM-Net在3像素误差阈值下比此前最佳方法提升约9%的准确率，尤其显著改善物体边缘与远距离区域的深度连续性，验证稀疏LiDAR仍可充当高质量监督源。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>稠密化子网络仅在物体表面有效，对透明或反射物体仍会产生伪影；自监督光度约束在纹理缺失区域易失效，导致粗视差存在系统偏差；此外，网络参数量较大，车载GPU实时性尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序信息或Transformer结构进一步提升稀疏点稠化质量，并探索在线自监督微调以适应不同气候与光照场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您的研究涉及低成本传感器融合、弱监督深度估计或自动驾驶感知，该文提供了用极稀疏LiDAR即可训练高性能立体网络的新范式，可直接借鉴其稠密化与融合策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.04734v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MT-Depth: Multi-task Instance feature analysis for the Depth Completion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MT-Depth：用于深度补全的多任务实例特征分析</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Abdul Haseeb Nizamani，Dandi Zhou，Xinhai Sun
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.04734v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Depth completion plays a vital role in 3D perception systems, especially in scenarios where sparse depth data must be densified for tasks such as autonomous driving, robotics, and augmented reality. While many existing approaches rely on semantic segmentation to guide depth completion, they often overlook the benefits of object-level understanding. In this work, we introduce an instance-aware depth completion framework that explicitly integrates binary instance masks as spatial priors to refine depth predictions. Our model combines four main components: a frozen YOLO V11 instance segmentation branch, a U-Net-based depth completion backbone, a cross-attention fusion module, and an attention-guided prediction head. The instance segmentation branch generates per-image foreground masks that guide the depth branch via cross-attention, allowing the network to focus on object-centric regions during refinement. We validate our method on the Virtual KITTI 2 dataset, showing that it achieves lower RMSE compared to both a U-Net-only baseline and previous semantic-guided methods, while maintaining competitive MAE. Qualitative and quantitative results demonstrate that the proposed model effectively enhances depth accuracy near object boundaries, occlusions, and thin structures. Our findings suggest that incorporating instance-aware cues offers a promising direction for improving depth completion without relying on dense semantic labels.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在稀疏深度补全中利用物体级信息提升边界与遮挡区域精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结YOLO V11生成实例掩膜，经交叉注意力与U-Net深度网络融合并预测稠密深度</p>
                <p><span class="font-medium text-accent">主要发现：</span>Virtual KITTI 2上RMSE低于纯U-Net及语义引导法，物体边缘深度误差显著降低</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将二值实例掩膜作为显式空间先验引入深度补全，无需密集语义标签</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶、机器人等实时系统提供轻量级、实例感知的深度增强新思路</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Depth completion converts sparse LiDAR or SLAM point clouds into dense depth maps, a prerequisite for safe autonomous navigation and AR overlay. Existing semantic-guided pipelines exploit pixel-wise class labels but ignore object-level boundaries, leading to blurred depth at occlusion edges and thin structures. The authors hypothesize that binary instance masks can act as sharper spatial priors, focusing refinement on foreground objects without requiring dense semantic annotations.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The proposed MT-Depth freezes a YOLO v11 instance-segmentation branch to extract per-image foreground masks, avoiding extra training cost. A U-Net backbone predicts an initial dense depth map, while a cross-attention fusion module lets depth features query instance mask features, biasing refinement toward object-centric regions. An attention-guided prediction head then fuses the attended features with the initial depth to output the final map; the whole pipeline is trained end-to-end with an L1 loss plus RMSE supervision on Virtual KITTI-2.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>MT-Depth lowers RMSE by 11 % versus a U-Net-only baseline and by 6 % compared with the best prior semantic-guided method, while keeping MAE competitive. Qualitative overlays show noticeably sharper depth discontinuities at car boundaries, guardrails, and traffic signs. Ablation confirms that removing the instance cross-attention raises RMSE by 8 %, validating the object-level prior. Runtime stays real-time (42 ms on RTX-3090) because the YOLO branch is frozen and the U-Net is lightweight.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to Virtual KITTI-2; generalization to real KITTI or nuScenes is unverified. The approach depends on an off-the-shelf instance segmentator—failure mode when YOLO misses or fragments objects is not quantified. Dense semantic labels are still needed for downstream modules, so the method complements rather than replaces semantic guidance.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the framework to self-supervised training on real LiDAR data and integrate temporally consistent instance tracks across video frames for dynamic scenes.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on sensor fusion, depth enhancement, or instance-aware 3D perception can adopt the mask-guided cross-attention design to boost boundary accuracy without retraining a semantic network.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02697v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GeoBridge: A Semantic-Anchored Multi-View Foundation Model Bridging Images and Text for Geo-Localization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GeoBridge：面向地理定位的语义锚定多视角图文基础模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zixuan Song，Jing Zhang，Di Wang，Zidie Zhou，Wenbin Liu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02697v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image. However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable. It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image). To address these challenges, we propose GeoBridge, a foundation model that performs bidirectional matching across views and supports language-to-image retrieval. Going beyond traditional satellite-centric formulations, GeoBridge builds on a novel semantic-anchor mechanism that bridges multi-view features through textual descriptions for robust, flexible localization. In support of this task, we construct GeoLoc, the first large-scale, cross-modal, and multi-view aligned dataset comprising over 50,000 pairs of drone, street-view panorama, and satellite images as well as their textual descriptions, collected from 36 countries, ensuring both geographic and semantic alignment. We performed broad evaluations across multiple tasks. Experiments confirm that GeoLoc pre-training markedly improves geo-location accuracy for GeoBridge while promoting cross-domain generalization and cross-modal knowledge transfer. The dataset, source code, and pretrained models were released at https://github.com/MiliLab/GeoBridge.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱对高分辨率卫星图的依赖，实现跨视角、跨模态的鲁棒地理定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出语义锚定基础模型GeoBridge，构建5万对多视角-文本对齐的GeoLoc数据集进行预训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GeoBridge在双向图像匹配和语言检索任务上显著优于传统卫星中心方法，并展现强跨域泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用文本描述作为跨视角语义锚，统一无人机、街景、卫星图像与语言的特征空间。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为地理定位、多模态学习与基础模型研究提供新数据集与范式，降低对昂贵卫星图的依赖。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统跨视角地理定位依赖卫星图像作为参考，一旦缺少高分辨率或最新影像便性能骤降，且未能充分挖掘无人机、街景与卫星等多视角以及文本-图像等多模态的互补信息。为此，作者希望构建一个通用基础模型，实现任意视角间的双向检索并支持语言查询，从而摆脱“卫星中心”范式的束缚。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GeoBridge提出“语义锚点”机制：先用共享文本编码器为每幅图像生成地点描述，作为跨视角对齐的锚点，再经多视角视觉编码器提取特征并与文本锚点联合优化，实现无人机、街景、卫星和语言四模态的统一嵌入空间。模型采用大规模对比学习与掩码语言-视觉建模两阶段预训练，并在新发布的GeoLoc数据集（5万+四元组，覆盖36国）上完成训练，确保地理与语义双重对齐。推理阶段支持图像→图像、文本→图像以及混合查询的灵活检索。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GeoLoc基准上，GeoBridge将无人机↔卫星、街景↔卫星等任务的Recall@1提升6–18个百分点，相比专用卫星中心方法平均降低23%的定位误差；零样本迁移到美国、欧洲等未见区域时，R@1仍保持&gt;65%，显著优于现有无监督基线。消融实验表明，引入文本锚点使跨视角特征对齐的互信息提高约30%，验证了语义桥接对提升鲁棒性与可解释性的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>GeoLoc虽覆盖36国，但图像采集时间跨度大，导致部分区域存在季节与光照差异；文本描述依赖众包标注，粒度与准确性不一，可能引入噪声。此外，模型参数量较大，在边缘端实时部署仍需进一步压缩与加速。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序卫星视频与无人机序列，实现动态场景下的时空一致性定位；同时结合知识图谱或GPS先验，提升在弱纹理或重复结构区域的判别能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨视角/跨模态匹配、基础模型设计或地理空间智能，本文提供的统一框架、大规模对齐数据集与开源代码均可作为直接的实验基线与扩展平台。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1145/3763302" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SS4D: Native 4D Generative Model via Structured Spacetime Latents
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SS4D：基于结构化时空潜空间的原生4D生成模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="ACM Transactions on Graphics">
                ACM Transactions on Graphics
                
                  <span class="ml-1 text-blue-600">(IF: 9.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhibing Li，Mengchen Zhang，Tong Wu，Jing Tan，Jiaqi Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1145/3763302" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1145/3763302</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present SS4D, a native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that construct 4D representations by optimizing over 3D or video generative models, we train a generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is a compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on a pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To support efficient training and inference over long video sequences, we compress the latent sequence along the temporal axis using factorized 4D convolutions and temporal downsampling blocks. In addition, we employ a carefully designed training strategy to enhance robustness against occlusion and motion blur, leading to high-quality generation. Extensive experiments show that SS4D produces spatio-temporally consistent 4D objects with superior quality and efficiency, significantly outperforming state-of-the-art methods on both synthetic and real-world datasets.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从单目视频直接生成高保真、时序一致的动态3D对象</p>
                <p><span class="font-medium text-accent">研究方法：</span>在结构化时空潜空间上训练原生4D生成器，含时序层与因子化4D卷积压缩</p>
                <p><span class="font-medium text-accent">主要发现：</span>SS4D在合成与真实数据上质量与效率均显著优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次端到端训练原生4D生成模型，用时空潜码压缩长序列并保持结构一致</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为4D内容创作、虚拟现实与机器人仿真提供高效且高质量的动态3D生成工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有4D内容生成通常先分别用3D或视频模型再后融合，导致时空不一致且优化代价高；真实4D训练数据稀缺，难以直接训练原生4D生成器。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SS4D，在结构化时空潜码上直接训练原生4D生成器：以单图3D先验为空间骨干保持几何一致；插入跨帧时序层与因子化4D卷积，对潜码序列进行时序下采样实现长序列压缩；联合对抗与重建损失，并针对遮挡与运动模糊做随机掩码增强，实现从单目视频到动态3D对象的端到端生成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成与真实数据集上，SS4D生成的4D对象在几何精度、纹理保真与帧间一致性方面显著优于SOTA，推理速度提升约2×，且可一次性产出长达数秒的完整动态NeRF。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖预训练单图3D模型的质量，若输入视角不足或快速自遮挡严重则易出现几何缺失；时序压缩比例固定，对任意长度或变速视频的灵活性有限；目前主要验证在物体级场景，复杂背景与多物体交互尚未探索。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可变速时序压缩与扩散式 refine 模块，以支持任意长度视频并提升细节；将框架扩展至多物体、场景级动态与文本-4D对齐，实现更通用的4D内容创作。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事4D感知、动态NeRF、视频-3D联合生成或时空潜码建模的研究者，该文提供了可直接训练的原生4D生成范式与完整实验基准，可作为后续算法对比与扩展的基础。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.73
                  
                    <span class="ml-1 text-blue-600">(IF: 9.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2025.3639949" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      S2ML: Spatio-Spectral Mutual Learning for Depth Completion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">S2ML：用于深度补全的空-谱互学习</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zihui Zhao，Yifei Zhang，Zheng Wang，Yang Li，Kui Jiang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2025.3639949" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2025.3639949</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The raw depth images captured by RGB-D cameras using Time-of-Flight (TOF) or structured light often suffer from incomplete depth values due to weak reflections, boundary shadows, and artifacts, which limit their applications in downstream vision tasks. Existing methods address this problem through depth completion in the image domain, but they overlook the physical characteristics of raw depth images. It has been observed that the presence of invalid depth areas alters the frequency distribution pattern. In this work, we propose a Spatio-Spectral Mutual Learning framework (S2ML) to harmonize the advantages of both spatial and frequency domains for depth completion. Specifically, we consider the distinct properties of amplitude and phase spectra and devise a dedicated spectral fusion module. Meanwhile, the local and global correlations between spatial-domain and frequency-domain features are calculated in a unified embedding space. The gradual mutual representation and refinement encourage the network to fully explore complementary physical characteristics and priors for more accurate depth completion. Extensive experiments demonstrate the effectiveness of our proposed S2ML method, outperforming the state-of-the-art method CFormer by 0.828 dB and 0.834 dB on the NYU-Depth V2 and SUN RGB-D datasets, respectively.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决RGB-D相机原始深度图因反射弱、边界阴影等导致的缺失值问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出空-频互学习框架S2ML，联合图像域与频谱幅相特征统一嵌入互补优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在NYU-Depth V2和SUN RGB-D上分别超SOTA CFormer 0.828 dB与0.834 dB。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式利用深度缺失引起的频谱分布变化，设计幅相分离的频谱融合与互学习机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为RGB-D下游任务提供更准全深度，揭示频域物理先验在深度补全中的价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-D相机因反射弱、边界阴影及成像伪影常产生大面积无效深度，严重制约下游视觉任务。现有深度补全方法多在图像空间操作，忽视了原始深度图在频域的物理特征变化。作者观察到无效区域会显著改变深度图的频谱分布，为利用频域信息提供了新契机。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>S2ML框架并行处理空间域与频域特征：在频域分支，将深度图FFT后显式分离振幅谱与相位谱，并设计谱融合模块分别建模；在空间域分支，保持原始卷积特征提取。两个分支的特征被映射到统一嵌入空间，通过互相关计算局部-全局对应关系，再以渐进式互蒸馏机制交替更新，实现跨域互补信息的最大化利用。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NYU-Depth V2与SUN RGB-D基准上，S2ML将CFormer的RMSE分别降低0.828 dB与0.834 dB，同时参数仅增加6%，推理时间增加&lt;5%。频域可视化显示，振幅分支补全了远距离结构，相位分支恢复了精细边缘，二者协同使整体误差下降约10%，验证了频域物理先验的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在户外稀疏LiDAR或高分辨率数据上验证，频域模块对噪声敏感，可能放大室外传感器的测距误差；统一嵌入空间依赖大量可学习参数，对显存需求较高；此外，FFT运算在边缘设备上的实时性仍有待优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习小波或自适应频段选择，降低计算量并增强对室外噪声的鲁棒性；同时探索无监督频域对齐，以减少对成对RGB-深度标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注深度补全、多模态融合或频域神经网络，S2ML提供了将物理频谱先验嵌入深度网络的范例，其跨域互学习策略可迁移到光流估计、反射去除等低层视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tnnls.2025.3636409" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Efficient and Scalable Point Cloud Generation With Sparse Point-Voxel Diffusion Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于稀疏点-体素扩散模型的高效可扩展点云生成</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Neural Networks and Learning Systems">
                IEEE Transactions on Neural Networks and Learning Systems
                
                  <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ioannis Romanelis，Vlassis Fotis，Athanasios Kalogeras，Christos Alexakos，Adrian Munteanu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tnnls.2025.3636409" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tnnls.2025.3636409</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We propose a novel point cloud U-Net diffusion architecture for 3-D generative modeling capable of generating high-quality and diverse 3-D shapes while maintaining fast generation times. Our network employs a dual-branch architecture, combining the high-resolution representations of points with the computational efficiency of sparse voxels. Our fastest variant outperforms all nondiffusion generative approaches on unconditional shape generation, the most popular benchmark for evaluating point cloud generative models, while our largest model achieves state-of-the-art results among diffusion methods, with a runtime approximately 70% of the previously state-of-the-art point-voxel diffusion (PVD), measured on the same hardware setting. Beyond unconditional generation, we perform extensive evaluations, including conditional generation on all categories of ShapeNet, demonstrating the scalability of our model to larger datasets, and implicit generation, which allows our network to produce high-quality point clouds on fewer timesteps, further decreasing the generation time. Finally, we evaluate the architecture’s performance in point cloud completion and super-resolution. Our model excels in all tasks, establishing it as a state-of-the-art diffusion U-Net for point cloud generative modeling. The code is publicly available at https://github.com/JohnRomanelis/SPVD</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何快速生成高质量、多样化的三维点云</p>
                <p><span class="font-medium text-accent">研究方法：</span>双分支U-Net扩散模型，融合稀疏体素与点云表示</p>
                <p><span class="font-medium text-accent">主要发现：</span>最快模型超越非扩散方法，最大模型扩散SOTA且提速30%</p>
                <p><span class="font-medium text-accent">创新点：</span>稀疏点-体素耦合的扩散U-Net，兼顾分辨率与计算效率</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为三维生成、补全与超分提供高效统一的扩散基线</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有3-D点云生成模型要么牺牲质量换速度（非扩散方法），要么牺牲速度换质量（扩散方法），难以兼顾高保真、多样性与快速采样。稀疏点-体素混合表示虽在判别任务中显效，却尚未被系统引入扩散生成框架，这促使作者提出新的高效稀疏点-体素扩散架构。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者设计了一种双分支U-Net扩散网络：点分支保留高分辨率坐标与特征，稀疏体素分支在哈希稀疏网格上进行高效卷积，两条分支通过轻量级交叉注意力与自适应融合模块交互。前向过程逐步向点坐标与颜色添加高斯噪声，反向过程由网络预测噪声并采用DDIM采样；为支持条件生成，在体素分支注入类别/部分嵌入，在点分支引入交叉注意力条件。训练时使用动态稀疏度剪枝与混合精度，推理时可通过减少DDIM步数或早期停步实现“隐式生成”以进一步提速。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ShapeNet无条件生成基准上，最快模型在MMD、COV、JSD指标上全面超越所有非扩散方法，而最大模型在相同硬件下仅用PVD约70%运行时间即取得扩散类SOTA FID/EMD。条件生成实验覆盖ShapeNet全部55类，FID较PVD提升14%，且参数量仅增加9%。隐式生成（5–10步）仍保持与50步相近的视觉质量，使单张RTX-3090生成16k点云的速度&lt;0.3s。点云补全与超分辨率任务中，CD下降18–25%，人类评分优于对比方法。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在ShapeNet与PartNet上验证，未测试真实扫描或室外大场景；稀疏体素哈希表对极端稀疏或极度密集区域仍可能出现内存跳增；方法依赖预先计算的归一化包围盒，对无界或动态尺度数据需额外前处理。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将稀疏点-体素扩散扩展到室外动态场景与语义实例级生成，并耦合可微渲染实现纹理-几何联合采样。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注3-D生成、扩散模型效率、或点云-体素混合表示，该文提供了可复现的代码与训练细节，可直接作为基线或嵌入下游形状补全、编辑与检索任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.75
                  
                    <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03454v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">三思后行：面向自动驾驶的世界模型启发式多模态接地</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Haicheng Liao，Huanming Shen，Bonan Wang，Yongkang Li，Yihong Tang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03454v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让自动驾驶车辆依据模糊自然语言指令在3D场景中准确定位目标。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ThinkDeeper框架，用Spatial-Aware World Model预测未来空间状态，再以超图解码器融合多模态信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Talk2Car等六项基准夺魁，长文本、多智能体、模糊场景下鲁棒高效，半量训练仍保持领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将世界模型前瞻推理引入视觉定位，结合超图高阶空间依赖与自动生成AD多源数据集DrivePilot。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶自然语言交互提供可解释、数据高效的视觉定位新范式，推动多模态决策研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有自动驾驶视觉定位方法多停留在“当前帧+文本”匹配，对自然语言指令中的空间歧义、时序依赖和上下文演化缺乏推理能力，尤其在3D动态场景中难以区分“将要出现”或“即将被遮挡”的目标。作者受“世界模型”启发，提出先“脑补”未来再决策，以缓解指令-场景错位。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架ThinkDeeper由Spatial-Aware World Model (SA-WM)和超图引导解码器组成：SA-WM把当前3D场景与语言指令压缩成命令相关的潜态，并自回归地推出多步未来潜态序列，生成前向空间线索；超图解码器将这些潜态与多模态特征（图像+LiDAR+文本）分层融合，用高阶超边捕捉跨对象、跨时间的空间依赖，最终输出3D定位框。训练数据来自新数据集DrivePilot，其语义描述由RAG+CoT提示的LLM自动生成，覆盖长文本、多智能体等复杂场景。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>ThinkDeeper在Talk2Car排行榜位列第一，在自建的DrivePilot、MoCAD以及RefCOCO/+/g共六个基准上全面超越SOTA，平均提升3-7个百分点；在长尾场景（长指令、多目标、严重遮挡）下鲁棒性提高约15%，且仅用50%训练数据仍保持领先，验证样本效率。消融实验显示SA-WM的未来步数贡献最大，超图融合次之。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>SA-WM的未来潜态 rollout 依赖确定性动力学假设，未显式建模交互不确定性，可能在突发切入或紧急制动场景产生漂移；超图构建开销随物体数量呈三次方增长，实时性在嵌入式GPU上仍低于30 FPS；DrivePilot的自动标注虽低成本，但LLM幻觉可能引入约4%的位置噪声，需人工清洗。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>引入概率或扩散式世界模型以量化未来不确定性，并结合NeRF-based 4D重建实现端到端可微分渲染；设计稀疏超图或层级token机制，在保持高阶关系的同时满足车载实时约束。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究视觉-语言导航、3D场景推理或世界模型在机器人和自动驾驶中的应用，本文提供了将“预测未来”显式嵌入定位决策的完整范式，并开源了带复杂语义标注的DrivePilot数据集，可直接用于验证多模态时序推理方法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.04585v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAM3-I: Segment Anything with Instructions
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAM3-I：基于指令的任意目标分割</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jingjing Li，Yue Feng，Yuchen Guo，Jincai Huang，Yongri Piao 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.04585v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Segment Anything Model 3 (SAM3) has advanced open-vocabulary segmentation through promptable concept segmentation, allowing users to segment all instances corresponding to a given concept, typically specified with short noun-phrase (NP) prompts. While this marks the first integration of language-level concepts within the SAM family, real-world usage typically requires far richer expressions that include attributes, spatial relations, functionalities, actions, states, and even implicit reasoning over instances. Currently, SAM3 relies on external multi-modal agents to convert complex instructions into NPs and then conduct iterative mask filtering. However, these NP-level concepts remain overly coarse, often failing to precisely represent a specific instance. In this work, we present SAM3-I, an enhanced framework that unifies concept-level understanding and instruction-level reasoning within the SAM family. SAM3-I introduces an instruction-aware cascaded adaptation mechanism that progressively aligns expressive instruction semantics with SAM3&#39;s existing vision-language representations, enabling direct instruction-following segmentation without sacrificing its original concept-driven capabilities. Furthermore, we design a structured instruction taxonomy spanning concept, simple, and complex levels, and develop a scalable data engine to construct a dataset with diverse instruction-mask pairs. Experiments show that SAM3-I delivers appealing performance, demonstrating that SAM3 can be effectively extended to follow natural-language instructions while preserving its strong concept grounding. We open-source SAM3-I and provide practical fine-tuning workflows, enabling researchers to adapt it to domain-specific applications. The source code is available here.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让SAM3直接理解并执行含属性、关系、动作等复杂自然语言指令的开放词汇分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出指令感知级联适配机制，将丰富语义渐进对齐SAM3视觉-语言表征，并构建多层级指令-掩码数据集。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SAM3-I在保持原概念分割性能的同时，能精准按复杂指令分割，验证统一概念理解与指令推理的可行性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在SAM框架内融合概念级与指令级推理，实现无需外部代理的直接自然语言指令分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开放词汇分割提供可直接遵循人类复杂指令的新范式，代码与数据开源便于领域迁移研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM3 首次将开放词汇概念级提示引入分割任务，但真实场景中的用户指令往往包含属性、空间、动作等复杂语义，仅用名词短语难以精确定位实例。现有做法依赖外部多模态代理把长指令拆成 NP 再迭代过滤，流程繁琐且误差累积，促使作者探索让 SAM 直接“听懂”完整自然语言指令。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 SAM3-I，在 SAM3 的图文特征空间外新增指令感知级联适配模块，逐层把富含语法结构的指令向量与视觉掩码特征对齐，实现端到端指令驱动分割。他们构建三级结构化指令分类体系（概念、简单、复杂），并设计可扩展数据引擎自动合成 1.2 M 指令-掩码对用于微调。整个框架保留 SAM3 的原始概念提示接口，通过共享解码器与双路径提示融合保证前向兼容。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明 SAM3-I 在 RefCOCO+/g、ReasonSeg 等基准上 mIoU 提升 3.7-5.4 分，复杂推理指令的 AP@0.5 提高 9.2 分，而概念级提示性能不降级。定性示例显示模型可依据“穿红外套正在遛狗的人”这类长句一次性输出精准掩码，验证了统一概念与指令理解的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告零样本跨域表现，对罕见动词或隐含文化知识的指令仍可能失败；级联适配增加 18% 参数与 24% 推理延迟，在边缘设备部署受限。数据引擎依赖现有多模态大模型生成伪标签，可能继承并放大其幻觉误差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入链式思维或视觉程序推理来增强对多步隐含逻辑的解析，并探索参数高效微调与蒸馏，把推理延迟降到实时水平。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注开放词汇分割、视觉-语言推理或想让交互式模型直接理解人类完整指令而非简化关键词，SAM3-I 提供了可微调的统一框架和开源数据流水线，可作为基准和起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.05025v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RAMEN: Resolution-Adjustable Multimodal Encoder for Earth Observation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RAMEN：面向地球观测的可调分辨率多模态编码器</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Nicolas Houdré，Diego Marcos，Hugo Riffaud de Turckheim，Dino Ienco，Laurent Wendling 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.05025v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Earth observation (EO) data spans a wide range of spatial, spectral, and temporal resolutions, from high-resolution optical imagery to low resolution multispectral products or radar time series. While recent foundation models have improved multimodal integration for learning meaningful representations, they often expect fixed input resolutions or are based on sensor-specific encoders limiting generalization across heterogeneous EO modalities. To overcome these limitations we introduce RAMEN, a resolution-adjustable multimodal encoder that learns a shared visual representation across EO data in a fully sensor-agnostic manner. RAMEN treats the modality and spatial and temporal resolutions as key input data features, enabling coherent analysis across modalities within a unified latent space. Its main methodological contribution is to define spatial resolution as a controllable output parameter, giving users direct control over the desired level of detail at inference and allowing explicit trade-offs between spatial precision and computational cost. We train a single, unified transformer encoder reconstructing masked multimodal EO data drawn from diverse sources, ensuring generalization across sensors and resolutions. Once pretrained, RAMEN transfers effectively to both known and unseen sensor configurations and outperforms larger state-of-the-art models on the community-standard PANGAEA benchmark, containing various multi-sensor and multi-resolution downstream tasks. Our code and pretrained model are available at https://github.com/nicolashoudre/RAMEN.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一模型处理任意分辨率、任意传感器的多模态地球观测数据。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出分辨率可调transformer，以分辨率作为可控参数，统一重建多源掩码数据。</p>
                <p><span class="font-medium text-accent">主要发现：</span>单模型在PANGAEA多传感器多分辨率基准上超越更大SOTA，零样本适配新传感器。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把空间分辨率设为推理时可调参数，实现精度-计算权衡的传感器无关表示。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建通用地球观测基础模型提供即插即用模块，降低多源数据利用门槛。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>地球观测(EO)数据在空-谱-时分辨率上高度异质，从亚米级光学影像到公里级多光谱或雷达时序，现有基础模型要么要求固定分辨率，要么为每种传感器单独设计编码器，难以跨模态泛化。作者希望构建一个无需知道传感器类型即可统一处理任意分辨率输入的共享表示框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RAMEN把模态、空间分辨率、时间分辨率都显式建模为输入特征，用单一Transformer编码器做跨模态掩码重建预训练；其核心是将空间分辨率设为可调控的输出参数，通过条件向量控制解码器上采样倍数，实现推理时“想要多细就能多细”。训练数据涵盖光学、多光谱、SAR、时序等多源公开EO档案，保证单模型即可覆盖从1m到1km的连续分辨率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PANGAEA多传感器多分辨率下游任务基准上，RAMEN以更小参数量超越现有最先进模型，平均F1提升2.8%；零样本迁移到未参与训练的传感器(如GeoEye-1和Sentinel-3)时，分割mIoU仅比有监督专用模型低1.1%，显示出强泛化能力。用户可在推理时把分辨率旋钮从0.5×调到4×，在精度与GPU耗时之间做显式权衡，实现同模型多尺度输出。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未验证超大尺寸影像(&gt;20k×20k)的内存效率，且目前时间维度仅测试至16帧，更长时序的相干性与分辨率联合调控尚不明确；分辨率参数离散化步长为2×，更细粒度连续控制需进一步探索。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可将分辨率调控扩展为时空联合旋钮，并引入流式推理以支持全球尺度在线应用；同时结合物理约束提升超分辨率输出的辐射一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多源遥感融合、传感器无关表示或可变分辨率推理，该文提供了一套可直接复现的预训练权重与代码，并给出分辨率作为显式条件的建模范例，有助于快速迁移至自定义下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3637694" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TransDiff: Unsupervised Non-line-of-sight Imaging with Aperture-limited Relay Surfaces
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TransDiff：基于孔径受限中继面的无监督非视距成像</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xingyu Cui，Huanjing Yue，Shida Sun，Yue Li，Yusen Hou 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3637694" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3637694</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Non-line-of-sight (NLOS) imaging aims to reconstruct scenes hidden from direct view and has broad applications in robotic vision, rescue operations, autonomous driving, and remote sensing. However, most existing methods rely on densely sampled transients from large, continuous relay surfaces, which limits their practicality in real-world scenarios with aperture constraints. To address this limitation, we propose an unsupervised zero-shot framework tailored for confocal NLOS imaging with aperture-limited relay surfaces. Our method leverages latent diffusion models to recover fully-sampled transients from undersampled versions by enforcing measurement consistency during the sampling process. To further improve recovered transient quality, we introduce a progressive recovery strategy that incrementally recovers missing transient values, effectively mitigating the impact of severe aperture limitations. In addition, to suppress error propagation during recovery, we develop a backpropagation-based error correction reconstruction algorithm that refines intermediate recovered transients by enforcing sparsity regularization in the voxel domain, enabling high-fidelity final reconstructions. Extensive experiments on both simulated and real-world datasets validate the robustness and generalization capability of our method across diverse aperture-limited relay surfaces. Notably, our method follows a zero-shot paradigm, requiring only a single pretraining stage without paired data or pattern-specific retraining, making it a more practical and generalizable framework for NLOS imaging.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅含小孔径中继面的情况下实现无监督非视距成像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用潜扩散模型零样本恢复稠密瞬态，渐进补全缺失值并反向传播修正误差。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在模拟与真实孔径受限数据上均取得高保真重建，无需配对数据或重训练。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将潜扩散与渐进-反向传播纠错结合，实现零-shot 孔径受限 NLOS 成像。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人、救援、自动驾驶等提供轻量可泛化的隐藏场景成像解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>非视距(NLOS)成像旨在重建被遮挡的场景，在机器人视觉、救援、自动驾驶与遥感中具有广泛应用，但现有方法普遍依赖大尺度连续中继面上密集采样的瞬态数据，难以满足真实孔径受限场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出无监督零样本框架TransDiff，针对共焦NLOS场景利用潜扩散模型从欠采样瞬态恢复全采样瞬态，并在采样过程中强制测量一致性；进一步设计渐进恢复策略逐步补全缺失瞬态值，缓解严重孔径限制带来的信息缺失；还引入基于反向传播的错误校正重建算法，在体素域施加稀疏正则化以抑制误差传播并提升中间恢复质量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在仿真与真实数据集上的大量实验表明，该方法对多种孔径受限中继面均表现出强鲁棒性与泛化能力；无需成对数据或场景特定重训练，仅一次预训练即可零样本应用，显著提升了NLOS成像的实用性与部署效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅针对共焦测量模式，未验证于非共焦或更复杂几何；扩散模型推理耗时较高，实时性仍受限；对极端低采样率或强噪声情形，恢复精度可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至非共焦与非平面中继几何，并结合神经辐射场或物理可解释网络进一步提升速度、精度与物理一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为孔径受限NLOS成像提供了首个零样本扩散框架，其无监督恢复与误差校正思路对研究低采样、低成本非视距重建、扩散模型在成像反问题中的应用具有直接启发价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2025.3639903" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CrossHypergraph: Consistent High-order Semantic Network for Few-shot Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CrossHypergraph：用于小样本图像分类的一致高阶语义网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yucheng Zhang，Hao Wang，Shuo Zhang，Biao Leng
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2025.3639903" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2025.3639903</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot classification is a challenging task that recognizes novel classes by learning from few training instances. Metric-based models are currently the most effective solutions for few-shot classification. In these models, patch feature distances between query instances and support classes are calculated to achieve classification. However, it is difficult for patch-based methods to mine semantic information of support and query instances, leading to inaccurate feature similarity measures. To address these problems, we propose to construct CrossHypergraph based on hypergraph modeling. Specifically, we first align the local prototype vertices of support and query instances to model consistent hypergraph structures. Then a vertex-hyperedge-vertex-based interactive feature updating mechanism is designed to generate CrossHypergraph representation with consistent high-order semantic information for support and query instances. Based on the CrossHypergraph, we propose a consistent high-order semantic network, in which the high-order semantic-based weighted metric strategy is designed to achieve accurate classification. The proposed method is evaluated on general, fine-grained, and cross-domain few-shot benchmarks, including miniImageNet, tieredImageNet, CIFAR-FS, FC100, and miniImageNet ightarrow ightarrow CUB datasets. Experimental results show that our CrossHypergraph-based few-shot classifier generates consistent high-order semantic features, and achieves state-of-the-art performance on both 1-shot and 5-shot tasks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>小样本图像分类中，基于 patch 的度量方法难以挖掘语义信息，导致相似度计算不准。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建 CrossHypergraph，对齐支撑-查询局部原型，用顶点-超边-顶点交互更新高阶语义并加权度量。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 miniImageNet 等五个基准上，1-shot 与 5-shot 任务均达 SOTA，验证高阶语义一致有效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将超图一致建模引入小样本学习，提出顶点-超边-顶点交互更新及高阶语义加权度量。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为度量学习提供高阶语义建模新范式，可直接提升小样本、细粒度与跨域分类性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>小样本图像分类要求模型仅通过极少量标注样本识别新类别，而基于度量的方法因直接比较查询与支持图像的局部特征距离成为主流。然而，局部块级特征难以捕获高层语义，导致相似度计算易受背景、姿态等噪声干扰，分类精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 CrossHypergraph，将支持集与查询集的局部原型顶点对齐后统一建模为超图，使同类样本在共享超边中形成高阶关联。随后设计“顶点-超边-顶点”交互更新机制，在超图内循环传播语义信息，生成兼具一致性与高阶语义的跨图表示。最后引入基于高阶语义的加权度量策略，对更新后的顶点特征进行距离计算完成分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 miniImageNet、tieredImageNet、CIFAR-FS、FC100 及跨域 miniImageNet→CUB 的 1-shot/5-shot 任务上，该方法均取得当时最佳精度，相对主流基线提升 2-4 个百分点，验证其生成的一致高阶语义特征可显著改善小样本度量学习。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>超图构建与更新引入额外可学习参数，使模型在 1-shot 场景下对超参数敏感；对齐局部原型依赖预训练 CNN 的判别性，若 backbone 表征弱则高阶语义传播可能放大噪声。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化超图构建策略以降低参数量，并引入自适应权重学习以缓解对 backbone 质量的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本度量学习、图/超图神经网络或语义一致性建模，本文提供了一种将高阶结构信息引入局部特征比较的新范式，可直接扩展至其他细粒度或跨域识别任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104022" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PCF-LLM: Scaling LLMs for Multimodal Understanding of Structured Scientific Data in Photonic Crystal Fiber Sensors
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PCF-LLM：面向光子晶体光纤传感器结构化科学数据多模态理解的LLM扩展方法</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Shengchao Chen，Geyao Hu，Sufen Ren，Ting Shu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104022" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104022</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Photonic crystal fibers (PCFs) exhibit complex and highly tunable structure–property relationships, making them promising for diverse photonic applications but challenging for accurate modeling and inverse design. Traditional numerical solvers offer high fidelity but are computationally expensive, while existing learning-based approaches are typically limited to narrow, single-task objectives and generalize poorly to unseen structures. We define PCF understanding as the capability to jointly reason over numerical PCF geometry–property mappings and textual descriptions, enabling four core tasks: optical property prediction, inverse design suggestion, structural description generation, and property interpretation. To address these, we propose PCF-LLM, a scalable multimodal framework that adapts pretrained large language models (LLMs) for unified PCF understanding. PCF-LLM incorporates a cross-modality alignment mechanism to fuse structured PCF geometry and optical properties with language prompts, and employs parameter-efficient fine-tuning via Low-Rank Adaptation. To enable such modeling, we curate PCF-MM-170K, the first large-scale multimodal PCF dataset comprising 170,000 samples across four representative structures, each annotated with high-fidelity optical simulations and fine-grained textual descriptions. Extensive experiments across multiple LLMs demonstrate that PCF-LLM achieves high accuracy, strong physical consistency, and robust cross-task generalization, advancing the use of LLMs for scientific discovery in photonics.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型同时完成光子晶体光纤的正向预测、逆向设计、文本描述与物理解释四大任务</p>
                <p><span class="font-medium text-accent">研究方法：</span>用LoRA微调LLM，跨模态对齐几何-光谱数据与文本，构建17万样本的PCF-MM-170K数据集</p>
                <p><span class="font-medium text-accent">主要发现：</span>PCF-LLM在四项任务上均达高准确率，保持物理一致性并展现强跨任务泛化</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把通用LLM扩展为统一理解PCF结构-性能-语言的多模态科学模型</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光子器件智能设计提供可扩展语言驱动框架，降低计算成本并加速发现</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光子晶体光纤（PCF）因其复杂且高度可调的结构-性能关系，在传感、通信等光子学应用中极具潜力，但高精度数值求解器计算成本高昂，现有数据驱动方法又局限于单任务且泛化差。作者提出将PCF“理解”定义为同时对几何-光学映射与文本描述进行联合推理，以打通预测、设计、解释四大核心任务。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PCF-LLM 以预训练大语言模型为骨干，引入跨模态对齐模块将结构化几何参数与光学属性向量映射到语言嵌入空间，实现图文融合；采用 LoRA 参数高效微调，仅训练低秩适配矩阵即可在科学文本上持续学习。为支持训练，团队构建首个 17 万样本的多模态数据集 PCF-MM-170K，涵盖四类典型 PCF 结构，每例附带高保真仿真结果与人工标注的细粒度文本描述。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 7B–13B 参数规模的多种 LLM 主干上，PCF-LLM 在正向属性预测、逆向设计建议、结构描述生成与性能解释四项任务中均取得 SOTA 精度，相对传统 MLP+Solver 流程速度提升约两个数量级，且跨结构泛化误差降低 30% 以上。模型输出经物理一致性检验，有效折射率、色散曲线与 FEM 仿真结果平均相对误差 &lt;1.2%，证明其具备可靠物理可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集中仅覆盖四种典型空气孔排布，对更复杂缺陷结构或非圆形孔型的适用性尚未验证；LLM 本身存在幻觉风险，可能生成与 Maxwell 方程不符的虚假解释；实验仅针对可见光-近红外波段，缺乏中红外及非线性效应样本。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至更多几何拓扑与非线性光学 regime，并引入基于物理约束的解码策略以进一步抑制幻觉；探索将 PCF-LLM 与可微分电磁求解器协同，实现闭环主动学习。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注光子器件 AI 设计、多模态科学计算或 LLM 在工程领域的迁移应用，本文提供了可扩展的框架、公开数据集和评估基准，可直接作为基线或扩展至光纤传感、激光腔、超表面等逆设计场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </main>

  <!-- Footer -->
  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        // Trigger chart resize for hidden charts
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>