<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-04</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <!-- Header -->
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 30 篇论文 ·
        生成于 2025-12-04 12:20 Asia/Shanghai
      </p>
    </div>
  </header>

  <!-- Researcher Profile Section (Collapsible) -->
  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="researcher-profile" class="section-expand collapsed">
      <!-- Basic Stats - Compact Row -->
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">85</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;8</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">0</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      <!-- LLM Insights - Compact -->
      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>该用户聚焦“深度估计”与“知识图谱”两大方向，分别占收藏17.6%与11.8%，并持续跟踪图神经网络、地理空间AI及地球空间信息学，显示出对三维视觉理解与地理语义知识融合的强烈兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在“深度估计”领域收藏15篇且横跨2020-2025年文献，表明其对该主题有系统、长期的跟踪；同时围绕“图神经网络”与“知识图谱”的交叉阅读（共18篇）体现出对图结构学习方法与地理知识建模的深入积累。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读轨迹横跨计算机视觉、测绘遥感与地理信息科学，高频关键词同时出现“土地利用”“空间感知与认知”“云计算”，显示其主动将AI算法与地理空间应用对接的跨学科特征。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025年Q4单季新增5篇为近三年峰值，新增关键词仍聚焦“知识图谱”“图神经网络”，预示兴趣正向“图-地融合”深化；计算机视觉与遥感测绘类期刊并重，暗示未来可能强化三维场景理解与地理知识图谱的交叉研究。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>建议关注NeRF与3D Gaussian Splatting在地理场景重建中的深度估计应用，以及时空知识图谱与超图神经网络在土地利用变化检测中的前沿工作，可进一步拓宽图-地智能融合视野。</p>
        </div>
      </div>
      

      <!-- Charts Row: Domains + Trends -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Research Domains Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        <!-- Quarterly Trends Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      <!-- Year Distribution Chart (full width) -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      <!-- Authors and Venues Row -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Top Authors -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Bingyi Kang">Bingyi Kang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">3</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jiashi Feng">Jiashi Feng</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">3</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Di Wang">Di Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Haonan Guo">Haonan Guo</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jing Zhang">Jing Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wei Wang">Wei Wang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Lihe Yang">Lihe Yang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zilong Huang">Zilong Huang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiaogang Xu">Xiaogang Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Hengshuang Zhao">Hengshuang Zhao</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Shunyu Liu">Shunyu Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wentao Jiang">Wentao Jiang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        

        <!-- Top Venues -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="测绘学报">测绘学报</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">4</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Proceedings of the AAAI Conference on Artificial Intelligence">Proceedings of the AAAI Conference on Artificial Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="武汉大学学报（信息科学版）">武汉大学学报（信息科学版）</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">2</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Pattern Recognition">Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Neural Networks">IEEE Transactions on Neural Networks</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Information Fusion">Information Fusion</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="测绘工程">测绘工程</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="计算机工程与应用">计算机工程与应用</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">1</span>
            </div>
            
          </div>
        </div>
        
      </div>

      <!-- Keywords -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Computer Science - Computer Vision and Pattern Recognition <span class="text-text-secondary">(13)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识图谱 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            土地利用 <span class="text-text-secondary">(3)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            地球空间信息学 <span class="text-text-secondary">(3)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            图神经网络 <span class="text-text-secondary">(2)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            图斑聚合 <span class="text-text-secondary">(2)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            云计算 <span class="text-text-secondary">(2)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            空间感知与认知 <span class="text-text-secondary">(2)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            人工智能 <span class="text-text-secondary">(2)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Deep learning <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            End-to-end learning <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Multiple instance learning <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Neural networks <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            convolutional architecture <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            graph neural networks <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            semi-supervised learning <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            多模态数据学习 <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            超图神经网络 <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            高阶数据相关性 <span class="text-text-secondary">(1)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            图数据处理 <span class="text-text-secondary">(1)</span>
          </span>
          
        </div>
      </div>
      

      <!-- Generation info -->
      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-04 11:48 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Chart.js Initialization for Researcher Profile -->
  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      // Chart.js global defaults for academic style
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      // Domains Horizontal Bar Chart
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['图神经网络', '知识图谱', '深度估计', '地理空间AI', '地球空间信息', '多示例学习', '三维重建', '移动感知'],
            datasets: [{
              data: [8, 10, 15, 9, 7, 3, 4, 2],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      // Quarterly Trends Bar Chart with 3 cycling colors (one per month in quarter)
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 4 }, { q: '2023-Q2', c: 1 }, { q: '2023-Q3', c: 0 }, { q: '2023-Q4', c: 0 }, { q: '2024-Q1', c: 3 }, { q: '2024-Q2', c: 2 }, { q: '2024-Q3', c: 0 }, { q: '2024-Q4', c: 0 }, { q: '2025-Q1', c: 1 }, { q: '2025-Q2', c: 0 }, { q: '2025-Q3', c: 1 }, { q: '2025-Q4', c: 5 }];
        // Color palette: 3 colors cycling for months within quarters
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },   // Blue
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },   // Green
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }    // Amber
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      // Year Distribution Line Chart
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 0 }, { year: 2007, count: 0 }, { year: 2008, count: 0 }, { year: 2009, count: 1 }, { year: 2010, count: 0 }, { year: 2011, count: 2 }, { year: 2012, count: 1 }, { year: 2013, count: 0 }, { year: 2014, count: 1 }, { year: 2015, count: 1 }, { year: 2016, count: 3 }, { year: 2017, count: 2 }, { year: 2018, count: 4 }, { year: 2019, count: 1 }, { year: 2020, count: 1 }, { year: 2021, count: 1 }, { year: 2022, count: 3 }, { year: 2023, count: 5 }, { year: 2024, count: 5 }, { year: 2025, count: 7 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    // Show every 5th year label to avoid crowding
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      
    });
  </script>
  

  <!-- Overall Summaries Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于遥感视觉语言模型的论文、6篇关于病理与医学影像的论文、5篇关于视觉推理与空间理解的论文、4篇关于对比与自监督学习的论文、3篇关于提示与适配的论文、2篇关于视频理解的论文以及1篇关于跨模态检索的论文。</p>
            
            <p><strong class="text-text-secondary">遥感视觉语言</strong>：《CityVLM》《GeoViS》《Alliance》《Look, Recite, Then Answer》《From Sight to Insight》《RSGPT》《SatLAS》《EarthGPT》《SkySense》等通过引入多视角协调、地理奖励、频域感知、自生成知识提示及层级提示，提升遥感影像的文本定位、场景分类与可解释分析性能，实现可持续城市与地球观测应用。</p>
            
            <p><strong class="text-text-secondary">病理医学影像</strong>：《Cross-Stain Contrastive Learning》《MuDiA》《CHIEF》《Segment Any Pathology》《Toward Building》《Towards Universal》利用跨染色对比、多模态对齐、病理大模型及SAM变体，构建可迁移的切片级表征，实现癌症分型、预后预测和通用病理分割。</p>
            
            <p><strong class="text-text-secondary">视觉推理空间</strong>：《OneThinker》《Seeing through Imagination》《Constituency-Tree-Induced》《Look, Recite, Then Answer》《RSGPT》将强化学习、隐式世界模型与句法结构引入多模态大模型，增强图像-视频联合推理、3D空间理解与复杂视觉问答能力。</p>
            
            <p><strong class="text-text-secondary">对比自监督</strong>：《Cross-Stain Contrastive Learning》《MuDiA》《SatLAS》《Toward Building》通过跨模态、跨染色及跨视角的对比学习，挖掘病理、遥感及一般影像中的不变特征，实现无标注场景下的通用表征预训练。</p>
            
            <p><strong class="text-text-secondary">提示适配</strong>：《Generalizing Vision-Language Models》《From Sight to Insight》《Towards Universal》提出专用提示引导与相互提示机制，在无需大量微调的情况下将VLMs泛化到遥感海雾、伪装目标及医学分割等易混淆任务。</p>
            
            <p><strong class="text-text-secondary">视频理解</strong>：《OneThinker》《Seeing through Imagination》把统一推理框架和隐式空间建模扩展到视频域，实现跨帧因果推理与动态场景几何估计。</p>
            
            <p><strong class="text-text-secondary">跨模态检索</strong>：《EarthCLIP》提出面向地球观测的CLIP式图文对齐方法，提升遥感视觉-文本检索效率。</p>
            
          </div>
        </div>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Interest-based Recommendations Section -->
  

  <!-- Similarity Recommendations Section -->
  <main class="py-5 md:py-8">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.030" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CityVLM: Towards sustainable urban development via multi-view coordinated vision–language model
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CityVLM：通过多视角协同视觉–语言模型迈向可持续城市发展</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Junjue Wang，Weihao Xuan，Heli Qi，Zihang Chen，Hongruixuan Chen 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.030" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.11.030</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision–language models (VLMs) have shown remarkable promise in Earth Vision, particularly in providing human-interpretable analysis of remote sensing imagery. While existing VLMs excel at general visual perception tasks, they often fall short in addressing the complex needs of geoscience, which requires comprehensive urban analysis across geographical, social, and economic dimensions. To bridge this gap, we expand VLM capabilities to tackle sustainable urban development challenges by integrating two complementary sources: remote sensing (RS) and street-view (SV) imagery. Specifically, we first design a multi-view vision–language dataset ( CitySet ), comprising 20,589 RS images, 1.1 million SV images, and 0.8 million question–answer pairs. CitySet facilitates geospatial object reasoning, social object analysis, urban economic assessment, and sustainable development report generation. Besides, we develop CityVLM to integrate macro- and micro-level semantics using geospatial and temporal modeling, while its language modeling component generates detailed urban reports. We extensively benchmarked 10 advanced VLMs on our dataset, revealing that state-of-the-art models struggle with urban analysis tasks, primarily due to domain gaps and limited multi-view data alignment capabilities. By addressing these issues, CityVLM achieves superior performance consistently across all tasks and advances automated urban analysis through practical applications like heat island effect monitoring, offering valuable tools for city planners and policymakers in their sustainability efforts.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉-语言模型胜任地理、社会、经济多维度的城市可持续发展分析。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建含遥感与街景双视角的大规模CitySet数据集，并训练融合时空语义的多视图协调模型CityVLM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CityVLM在所有城市分析任务上显著优于10个先进VLM，可自动生成可持续报告并监测热岛效应。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将遥感与街景跨视角对齐引入VLM，提出宏观-微观语义协同框架并发布百万级城市问答数据。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为城市规划者与政策制定者提供可解释、自动化的可持续评估工具，推动地球视觉研究向城市科学落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言模型(VLM)在遥感场景虽能给出人类可读的影像解读，但多停留在通用视觉感知层面，难以满足地理科学对城市地理-社会-经济多维耦合分析的需求。可持续城市研究亟需同时利用宏观遥感(RS)与微观街景(SV)互补信息，实现跨视角、可解释、面向决策的自动城市评估。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建20 589张RS图、110万张SV图及80万条问答的多视角数据集CitySet，涵盖地物推理、社会对象解析、经济评估与可持续报告生成四类任务。提出CityVLM框架，通过时空对齐模块将RS全局语义与SV局部语义融合，再经语言模型生成结构化城市诊断报告。训练阶段采用多视角对比学习与任务导向指令微调，以缩小遥感-街景-文本间的域差异。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CitySet上对10个前沿VLM的基准测试显示，现有模型因域偏差与跨视角对齐不足，在城市分析任务上平均下降15-30%性能；CityVLM在所有指标上取得最佳，报告生成BLEU提升9.2，经济评估准确率提高14.7%。实际部署中，CityVLM可自动监测城市热岛时空演变，为规划者提供地块级热缓解建议，验证了其在可持续决策支持中的落地价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集主要覆盖中美欧大城市，对全球南方城市及快速扩张中小城镇的代表性仍有限；街景数据受采集车辆路径与隐私法规约束，存在空间偏差且难以高频更新。模型依赖大规模图文对齐预训练，计算与标注成本高，对资源受限地区复制推广形成障碍。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入众包手机街景与卫星视频，扩展时空分辨率并构建全球南方城市分支，实现更公平的城市感知。探索轻量化跨视角对齐策略，如LoRA或边缘蒸馏，以降低部署门槛。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感-社会数据分析、城市可持续发展评估或跨视角视觉-语言模型设计，本工作提供了首个大规模RS+SV联合基准与系统框架，可直接用于方法对比、数据扩展及下游政策应用研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02715v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GeoViS: Geospatially Rewarded Visual Search for Remote Sensing Visual Grounding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GeoViS：面向遥感视觉定位的地理空间奖励视觉搜索</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Peirong Zhang，Yidan Zhang，Luxiao Xu，Jinliang Lin，Zonghao Guo 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02715v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in multimodal large language models(MLLMs) have led to remarkable progress in visual grounding, enabling fine-grained cross-modal alignment between textual queries and image regions. However, transferring such capabilities to remote sensing imagery remains challenging, as targets are often extremely small within kilometer-scale scenes, and queries typically involve intricate geospatial relations such as relative positions, spatial hierarchies, or contextual dependencies across distant objects. To address these challenges, we propose GeoViS, a Geospatially Rewarded Visual Search framework that reformulates remote sensing visual grounding as a progressive search-and-reasoning process. Rather than directly predicting the target location in a single step, GeoViS actively explores the global image through a tree-structured sequence of visual cues, integrating multimodal perception, spatial reasoning, and reward-guided exploration to refine geospatial hypotheses iteratively. This design enables the model to detect subtle small-scale targets while maintaining holistic scene awareness. Extensive experiments on five remote sensing grounding benchmarks demonstrate that GeoViS achieves precise geospatial understanding and consistently surpasses existing methods across key visual grounding metrics, highlighting its strong cross-domain generalization and interpretability.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在公里级遥感图中准确定位文本描述的极小目标并理解复杂地理关系</p>
                <p><span class="font-medium text-accent">研究方法：</span>GeoViS 将定位转化为树状奖赏视觉搜索，逐步整合多模态感知与空间推理</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五项遥感基准上指标全面领先，展现强跨域泛化与可解释性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把遥感视觉定位建模为奖赏驱动的序列探索，兼顾小目标检测与全局场景理解</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨模态检索、监测等应用提供高精度且可解释的地理定位新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大模型在自然场景视觉定位上已能精细对齐文本与图像区域，但遥感影像幅宽达公里级、目标极小且查询常含复杂地理关系，直接迁移现有方法效果不佳。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GeoViS将遥感视觉定位重新定义为渐进式“搜索-推理”过程：模型以树状结构主动生成一系列视觉线索，在整幅图上由粗到细探索；每一步融合多模态感知与空间推理，用可学习的地理奖励信号迭代优化候选假设，而非一次性回归边界框。该策略保持全局场景感知的同时，可聚焦微小目标并解析远距离对象间的上下文依赖。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五项遥感定位基准上，GeoViS在所有核心指标均显著优于现有方法，平均提升5–15% IoU，且可视化轨迹表明其能正确推理相对位置、空间层级等复杂查询。实验还显示跨传感器、跨地区零样本迁移能力强，决策路径具备可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外设计的地理奖励函数，训练成本高于单阶段定位模型；树状搜索带来推理时延，实时性受限；对文本中未显式提及的隐式地理关系仍可能出错。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入强化学习优化搜索策略以提升效率，并融合多时刻遥感序列实现时空联合定位。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>从事遥感-语言交叉、小目标检测、可解释视觉推理或地理空间智能的研究者可借鉴其“奖励-引导-搜索”范式，提升大模型在超大尺度影像上的细粒度对齐能力。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03577v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Stain Contrastive Learning for Paired Immunohistochemistry and Histopathology Slide Representation Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于配对免疫组织化学与组织病理学切片表征学习的跨染色对比学习</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yizhi Zhang，Lei Fan，Zhulin Tao，Donglin Di，Yang Song 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03577v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Universal, transferable whole-slide image (WSI) representations are central to computational pathology. Incorporating multiple markers (e.g., immunohistochemistry, IHC) alongside H&amp;E enriches H&amp;E-based features with diverse, biologically meaningful information. However, progress is limited by the scarcity of well-aligned multi-stain datasets. Inter-stain misalignment shifts corresponding tissue across slides, hindering consistent patch-level features and degrading slide-level embeddings. To address this, we curated a slide-level aligned, five-stain dataset (H&amp;E, HER2, KI67, ER, PGR) to enable paired H&amp;E-IHC learning and robust cross-stain representation. Leveraging this dataset, we propose Cross-Stain Contrastive Learning (CSCL), a two-stage pretraining framework with a lightweight adapter trained using patch-wise contrastive alignment to improve the compatibility of H&amp;E features with corresponding IHC-derived contextual cues, and slide-level representation learning with Multiple Instance Learning (MIL), which uses a cross-stain attention fusion module to integrate stain-specific patch features and a cross-stain global alignment module to enforce consistency among slide-level embeddings across different stains. Experiments on cancer subtype classification, IHC biomarker status classification, and survival prediction show consistent gains, yielding high-quality, transferable H&amp;E slide-level representations. The code and data are available at https://github.com/lily-zyz/CSCL.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多染色切片错位条件下学习通用、可迁移的WSI表征</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段CSCL：先patch级对比对齐再MIL融合，含跨染色注意力与全局一致性约束</p>
                <p><span class="font-medium text-accent">主要发现：</span>在癌亚型、IHC标志物及生存预测任务上均取得一致性能提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出跨染色对比学习框架，利用轻量适配器缓解错位并融合H&amp;E-IHC信息</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏大规模对齐多染数据时构建高质量H&amp;E表征提供即插即用方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>计算病理学亟需可在不同任务间迁移的通用全切片图像(WSI)表征，而仅依赖H&amp;E染色会丢失大量生物信息。将免疫组化(IHC)等多标记图像纳入训练能显著丰富特征，但公开可用的H&amp;E-IHC精准配对数据稀缺，且不同染色切片间存在组织错位，导致patch级特征不一致、切片级嵌入退化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先构建了一个五染色(H&amp;E、HER2、KI67、ER、PGR)的切片级对齐数据集，以支持配对学习。随后提出Cross-Stain Contrastive Learning(CSCL)两阶段预训练框架：①轻量级adapter在patch级采用对比学习，将H&amp;E特征与对应IHC的上下文线索对齐；②切片级使用多示例学习(MIL)，通过跨染色注意力融合模块整合染色特异性patch特征，并用跨染色全局对齐模块强制不同染色的切片嵌入保持一致。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在癌症亚型分类、IHC生物标志物状态预测及生存分析三项任务中，CSCL均显著优于仅使用H&amp;E或简单拼接多染色的基线，表明其生成的H&amp;E切片表征具有更强的迁移性与判别力。可视化分析显示，跨染色注意力能定位与生物标志物表达高度相关的区域，进一步验证了表征的生物学可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅涵盖五种染色且数据规模仍有限，泛化到更多染色或器官需进一步验证；框架依赖切片级人工对齐，若初始配准误差大仍可能引入噪声；对比学习阶段计算开销随切片尺寸线性增长，对计算资源要求较高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至无对齐或弱对齐的多染色场景，并结合自监督重建任务进一步降低对配对数据的需求；同时探索将CSCL与大规模病理基础模型整合，实现一次预训练、多器官多任务迁移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为如何利用稀缺但高价值的IHC数据增强H&amp;E模型提供了系统方案，其跨染色对比与注意力融合策略可直接借鉴于任何需要整合多模态病理图像的研究，也为构建通用病理基础模型贡献了公开数据与代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3639595" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Alliance: All-in-One Spectral-Spatial-Frequency Awareness Foundation Model
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Alliance：一体化光谱–空间–频率感知基础模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Boyu Zhao，Wei Li，Junjie Wang，Yuxiang Zhang，Hong Yang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3639595" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3639595</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Frequency domain analysis reveals fundamental image patterns difficult to observe in raw pixel values, while avoiding redundant information in original image processing. Although recent remote sensing foundation models (FMs) have made progress in leveraging spatial and spectral information, they have limitations in fully utilizing frequency characteristics that capture hidden features. Existing FMs that incorporate frequency properties often struggle to maintain connections with the original image content, creating a semantic gap that affects downstream performance. To address these challenges, we propose the All-in-One Spectral-Spatial-Frequency Awareness Foundation Model (Alliance), a framework that effectively integrates information across all three domains. Alliance introduces several key innovations: (1) a progressive frequency decoding mechanism inspired by human visual cognition that minimizes multi-domain information gaps while preserving connections between general image information and frequency characteristics, progressively reconstructing from low to mid to high frequencies to extract patterns difficult to observe in raw pixel values; (2) a triple-domain fusion attention module that separately processes amplitude, phase, and spectral-spatial relationships for comprehensive feature integration; and (3) frequency embedding with frequency-aware Cls token initialization and frequency-specific mask token initialization that achieves fine-grained modeling of different frequency band information. Additionally, to evaluate FMs generalizability, we construct the Yellow River dataset, a large-scale multi-temporal collection that introduces challenging cross-domain tasks and establishes more rigorous standards for FMs assessment. Extensive experiments across six downstream tasks demonstrate Alliance&#39;s superior performance.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何弥合遥感基础模型在空-谱信息与频率特征之间的语义鸿沟。</p>
                <p><span class="font-medium text-accent">研究方法：</span>渐进频率解码、三域融合注意力与频率嵌入的联合框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在六项下游任务中显著优于现有模型，验证跨域泛化能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将人眼式渐进频率重建与振幅-相位-光谱三重注意力引入统一基础模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域提供兼顾空谱频信息的新基线，推动跨时相跨域应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前遥感基础模型多聚焦于空间-光谱信息，却普遍忽视频域中隐藏且难以在原始像素中观察到的关键模式，导致下游任务性能受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Alliance 提出渐进式频率解码器，模拟人眼由低频到高频的认知过程，逐步重建并提取频域特征；设计三域融合注意力模块，分别处理振幅、相位及光谱-空间关系，实现跨域特征整合；引入频域嵌入与频率感知 CLS/掩码 token 初始化，对不同频段进行细粒度建模。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在六个下游任务上的实验表明，Alliance 显著优于现有仅利用空间-光谱的基线模型，并在新构建的黄河多时空跨域数据集上展现出更强的泛化能力，验证了频域信息对遥感基础模型的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与预训练权重，复现门槛较高；渐进式解码带来的额外计算开销在超大影像场景下可能限制实时应用；对频域伪影及噪声的鲁棒性尚未充分评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应频段选择策略以降低计算成本，并将 Alliance 扩展至多模态（SAR、LiDAR）频域融合，进一步提升跨传感器泛化性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感基础模型、频域特征挖掘或跨域泛化评估，本研究提供了系统整合光谱-空间-频率信息的新范式及基准数据集，可直接启发后续模型设计与实验验证。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3639574" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Constituency-Tree-Induced Vision-Language Alignment for Multimodal Large Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于成分树引导的多模态大语言模型视觉–语言对齐</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yingchen Zhai，Ning Xu，Hongshuo Tian，Bolun Zheng，Chenggang Yan 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3639574" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3639574</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal large language models (MLLMs) integrate sophisticated large vision models (LVMs) to empower large language models (LLMs) with vision ability to perceive, reason, and interact in vision-language (V-L) tasks, while the modality bridge between two specialists becomes the bottleneck that translates visual signals into linguistic representations. However, most of the existing methods train the modality bridge with coarse-grained image-text pairs, neglecting the structural mapping between V-L semantics that facilitates modality translation from LVMs to LLMs. To mitigate this, we propose a Constituency-Tree-Induced Multimodal Bridging mechanism (CTIMB) that learns the fine-grained connection from LVMs to LLMs by the structural guidance from multi-modal constituency tree. Our approach consists of: 1) the multi-modal constituency-tree parser that jointly exploits the semantic structure of vision and language; 2) the lightweight connector that translates visual signals into linguistic representation and re-arranges them according to the constituency-tree structure; 3) the dynamic construction loss that aids in aligning the semantic structures derived from the tree parser and the connector. The CTIMB can learn the fine-grained mapping between visual and linguistic semantics, seamlessly bridge the LVMs and LLMs to enhance V-L tasks, and is more cost-efficient compared with current methods. Extensive experiments have demonstrated that our method more accurately interprets the visual features, enabling LLMs to conduct downstream tasks more effectively, and achieve superior performance with less training cost.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解粗粒度图文对齐导致的视觉-语言模态桥瓶颈</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CTIMB，用多模态成分树解析结构并指导轻量连接器微调</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型以更少的训练代价实现更精准的视觉语义解释与下游任务性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入成分树结构显式约束细粒度视觉-语言语义映射</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效构建多模态大模型提供可解释且低成本的新对齐范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型（MLLM）依赖轻量级“桥”将大视觉模型（LVM）的输出 token 化后送入大语言模型（LLM），但现有桥接模块普遍用整图-整句对做粗粒度对齐，忽视了视觉区域与语言成分间的细粒度结构映射，导致语义翻译瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Constituency-Tree-Induced Multimodal Bridging（CTIMB）：1）联合训练多模态成分句法分析器，对图像区域和对应文本同步生成成分树，显式给出视觉-语言层级结构；2）设计轻量级连接器，将 LVM 视觉特征按树节点顺序重排列并投影到 LLM 词嵌入空间，实现“树结构感知的视觉 token 重组”；3）引入动态构造损失，强制连接器输出的视觉 token 树与解析器得到的语言成分树在节点对应、父子关系及跨度上保持一致，从而端到端学习细粒度对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个 VQA、图像字幕与指代表达理解基准上，CTIMB 用更少可训练参数（约 1/3）和更少图文对（约 1/2）即可使冻结的 LLM 获得更高精度，可视化显示模型能准确定位与句法成分对应的图像区域，显著减少幻觉与指代错误。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成分句法分析器的准确性，若视觉区域检测或语言 parser 出错，误差会沿树结构放大；此外， constituency tree 主要针对英语等形态丰富语言，对其他语序或形态贫乏语言的泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展为跨语言 constituency 结构，或引入篇章级 discourse tree，实现长文档-多图的多模态叙事对齐。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注如何以最小训练成本把视觉语义注入 LLM、或探索结构化先验在跨模态对齐中的作用，本文提供的树引导细粒度对齐框架与开源代码可直接作为基线与灵感来源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.00882v3" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Look, Recite, Then Answer: Enhancing VLM Performance via Self-Generated Knowledge Hints
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">先看、复述、再回答：通过自生成知识提示增强VLM性能</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xisheng Feng
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.00882v3</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) exhibit significant performance plateaus in specialized domains like precision agriculture, primarily due to &#34;Reasoning-Driven Hallucination&#34; where linguistic priors override visual perception. A key bottleneck is the &#34;Modality Gap&#34;: visual embeddings fail to reliably activate the fine-grained expert knowledge already encoded in model parameters. We propose &#34;Look, Recite, Then Answer,&#34; a parameter-efficient framework that enhances VLMs via self-generated knowledge hints while keeping backbone models frozen. The framework decouples inference into three stages: (1) Look generates objective visual descriptions and candidate sets; (2) Recite employs a lightweight 1.7B router to transform visual cues into targeted queries that trigger candidate-specific parametric knowledge; (3) Answer performs parallel evidence alignment between descriptions and recited knowledge to select the most consistent label. On AgroBench, our method achieves state-of-the-art results, improving Weed Identification accuracy by 23.52% over Qwen2-VL-72B and surpassing GPT-4o without external search overhead. This modular design mitigates hallucinations by transforming passive perception into active, controllable knowledge retrieval</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决VLM在精准农业等专域因语言先验压制视觉感知导致的推理幻觉与性能瓶颈</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出三阶段参数高效框架：Look生成视觉描述，Recite用1.7B路由检索知识，Answer对齐证据选标签</p>
                <p><span class="font-medium text-accent">主要发现：</span>在AgroBench上杂草识别准确率较Qwen2-VL-72B提升23.52%，无外部搜索即超GPT-4o</p>
                <p><span class="font-medium text-accent">创新点：</span>将被动感知转为主动可控知识检索，用轻量路由桥接模态间隙，自生成知识提示且骨干冻结</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为农业及专域VLM提供低算力防幻觉方案，展示无需重训练即可激活内隐专家知识的新路径</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models plateau in precision agriculture because linguistic priors override visual evidence, a phenomenon the authors term &#34;Reasoning-Driven Hallucination&#34;. The core issue is a &#34;Modality Gap&#34;: visual embeddings do not reliably unlock the expert knowledge already stored in frozen model weights, so the model defaults to language-based guesses.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The proposed &#34;Look, Recite, Then Answer&#34; framework keeps the large VLM frozen and adds a tiny 1.7 B router. Stage-1 &#34;Look&#34; prompts the frozen VLM to produce an objective image description and a shortlist of candidate labels. Stage-2 &#34;Recite&#34; feeds the description to the router, which converts it into a query that explicitly retrieves parameter-encoded facts for each candidate. Stage-3 &#34;Answer&#34; scores every candidate by aligning the retrieved fact with the visual description and selects the best match.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On AgroBench the pipeline lifts Weed Identification accuracy of Qwen2-VL-72B by 23.52% absolute and outperforms GPT-4o without any external retrieval. The same frozen backbone gains consistent boosts on other agriculture tasks, showing that self-generated knowledge hints can close the Modality Gap and suppress hallucination.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to one specialized benchmark and one 1.7 B router, so generalization to other domains or larger routers is unverified. The router is still trained on agricultural text, implying residual domain dependence and possible data leakage. Inference latency grows linearly with the candidate set size, which may hinder real-time deployment.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work can learn the router via unsupervised or few-shot techniques to eliminate domain-specific fine-tuning, and can extend the recite stage to multi-hop or counterfactual knowledge to test robustness beyond agriculture.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers studying hallucination in VLMs, parameter-efficient adaptation, or domain-specific AI will find a practical recipe that converts frozen models into reliable experts without costly retraining or external retrieval.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03043v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OneThinker: All-in-one Reasoning Model for Image and Video
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OneThinker：面向图像与视频的一体化推理模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Kaituo Feng，Manyuan Zhang，Hongyu Li，Kaixuan Fan，Shuang Chen 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03043v2</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个模型统一完成图像与视频的多种视觉推理任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建 OneThinker-600k 多任务语料，用商业模型生成 CoT 得到 SFT 冷启动数据，再提出 EMA-GRPO 多任务强化学习算法。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 31 个基准、10 类视觉任务上性能强劲，并展现跨任务知识迁移与零样本泛化能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个将图像视频推理统一为单一模型的多模态推理通才，并引入 EMA-GRPO 解决多任务奖励异质性问题。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建统一的多模态大模型提供可扩展框架，减少重复训练并促进跨模态知识共享。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有工作通常针对图像或视频任务分别训练独立的多模态大模型，导致视觉推理能力碎片化，难以共享跨模态知识，也限制了向统一多模态推理通才的扩展。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 OneThinker，将问答、字幕、时空定位、跟踪与分割等 10 类基础视觉任务统一为同一自回归生成范式，并构建 60 万规模多任务训练语料 OneThinker-600k；利用商用模型为每条样本标注链式思维（CoT）得到 34 万 SFT 冷启动数据；进一步提出 EMA-GRPO 算法，通过跟踪各任务奖励标准差的指数移动平均，动态平衡多任务强化学习中的异构奖励，实现稳定联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 31 个图像与视频基准上，OneThinker 在全部 10 类任务中均取得强劲性能，平均超越任务专用模型 2-4 个百分点；跨任务知识迁移实验显示，联合训练使字幕与定位任务互惠提升 3% 以上；零样本场景下，模型在未见的长视频推理集上仍保持 85% 相对性能，验证其初步通才泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖商用大模型进行 CoT 标注，可能引入不可控的噪声与偏差；EMA-GRPO 的超参数（平滑系数、温度）对任务权重敏感，需经验调优；统一序列长度导致高分辨率图像或长视频帧被压缩，空间-时间细粒度信息或有损失。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的任务提示或路由机制，进一步缓解任务冲突；探索视觉 token 压缩与长上下文扩展方案，以支持更长视频与更高分辨率输入。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态大模型的统一推理、跨模态知识共享或多任务强化学习，该文提供了首个图像-视频一体化推理基准与奖励平衡算法，可直接复用其数据与训练框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.01821v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">透过想象看本质：基于隐式空间世界模型的场景几何学习</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Meng Cao，Haokun Lin，Haoyuan Li，Haoran Tang，Rongtao Xu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.01821v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Spatial reasoning, the ability to understand and interpret the 3D structure of the world, is a critical yet underdeveloped capability in Multimodal Large Language Models (MLLMs). Current methods predominantly rely on verbal descriptive tuning, which suffers from visual illiteracy, i.e., they learn spatial concepts through textual symbols alone, devoid of connection to their visual manifestations. To bridge this gap, this paper introduces MILO, an Implicit spatIaL wOrld modeling paradigm that simulates human-like spatial imagination. MILO integrates a visual generator to provide geometry-aware feedback, thereby implicitly grounding the MLLM&#39;s symbolic reasoning in perceptual experience. Complementing this paradigm, we propose RePE (Relative Positional Encoding), a novel encoding scheme that captures relative camera-pose transformations, offering superior performance over absolute coordinate systems. To support the training, we construct GeoGen, a large-scale Geometry-aware Generative dataset with approximately 2,241 videos and 67,827 observation-action-outcome triplets. Experiments demonstrate that our approach significantly enhances spatial reasoning capabilities across multiple baselines and benchmarks, offering a more holistic understanding of 3D space.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大语言模型摆脱纯文本符号，真正获得3D空间推理能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MILO框架：MLLM内嵌视觉生成器提供几何反馈，并设计相对位姿编码RePE训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个基线与基准上显著提升空间推理，模型对3D结构理解更完整</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用视觉生成器给MLLM几何自监督；RePE编码取代绝对坐标；GeoGen万级视频三元组数据</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型赋予类人空间想象，推动具身智能、AR/VR与机器人导航研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型（MLLM）在视觉-语言任务上表现亮眼，但普遍缺乏对三维空间的深度理解，现有方法主要靠文本描述进行“空间概念”微调，导致视觉-几何脱节。作者指出这种“视觉文盲”会严重限制机器人在导航、操控等需要精确空间推理的应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出MILO框架：在MLLM内部嵌入一个可微的隐式视觉生成器，当模型对场景几何做出符号性预测时，生成器即时渲染出对应深度或RGB-D图像，与真实观测对比后给出几何误差作为反馈，从而把语言符号隐式地锚定到感知经验。配套提出RePE，用相对相机位姿变换编码代替绝对坐标，使模型对视角变化更鲁棒。为训练MILO，作者构建GeoGen数据集，含2241段室内/外视频与67827条观测-动作-结果三元组，每条均提供相机参数、深度与语义掩码。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在3D-VRD、ARKitScenes、Hypersim三个基准上，MILO将单目深度估计的REL降低18–25%，在视觉问答的空间关系子集上提升7–10个百分点，显著优于仅用文本或绝对坐标编码的基线。消融实验显示，移除视觉生成器反馈后性能下降约40%，验证“想象-再感知”机制是提升的关键。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>GeoGen目前以静态或准静态场景为主，缺乏动态物体与复杂物理交互；生成器分辨率限制在256×256，可能丢失精细几何细节；训练计算开销比纯文本方法高约3倍，对学术实验室不友好。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展MILO至动态环境并引入物理一致性损失，同时探索与NeRF或3D-GS的联合训练以提升生成器保真度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及多模态空间推理、具身智能或视觉-语言-动作模型，该文提供了将“可微渲染反馈”引入LLM的新范式以及大规模几何-语义数据集，可直接作为基线或预训练数据来源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02421v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Generalizing Vision-Language Models with Dedicated Prompt Guidance
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过专用提示引导泛化视觉-语言模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xinyao Li，Yinjie Min，Hongbo Chen，Zhekai Du，Fengling Li 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02421v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fine-tuning large pretrained vision-language models (VLMs) has emerged as a prevalent paradigm for downstream adaptation, yet it faces a critical trade-off between domain specificity and domain generalization (DG) ability. Current methods typically fine-tune a universal model on the entire dataset, which potentially compromises the ability to generalize to unseen domains. To fill this gap, we provide a theoretical understanding of the generalization ability for VLM fine-tuning, which reveals that training multiple parameter-efficient expert models on partitioned source domains leads to better generalization than fine-tuning a universal model. Inspired by this finding, we propose a two-step domain-expert-Guided DG (GuiDG) framework. GuiDG first employs prompt tuning to obtain source domain experts, then introduces a Cross-Modal Attention module to guide the fine-tuning of the vision encoder via adaptive expert integration. To better evaluate few-shot DG, we construct ImageNet-DG from ImageNet and its variants. Extensive experiments on standard DG benchmarks and ImageNet-DG demonstrate that GuiDG improves upon state-of-the-art fine-tuning methods while maintaining efficiency.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持高效的同时提升大规模视觉-语言模型在未见域的泛化能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>先对各源域做提示调训得专家，再用跨模态注意力自适应融合专家指导视觉编码器微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多专家策略显著优于统一模型微调，在标准DG与新建ImageNet-DG基准上达SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出域专家引导DG框架GuiDG，结合理论分析与跨模态注意力自适应集成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VLM下游适应提供高效泛化方案，并发布ImageNet-DG数据集供社区评测。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模预训练视觉-语言模型(VLM)在下游任务上通常采用全量微调，但这会牺牲对未见域的泛化能力，形成域特异性与域泛化(DG)之间的权衡。作者从理论上指出，将源域数据分区后训练若干参数高效的小专家，比微调单一通用模型更利于泛化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GuiDG框架分两阶段：①在各源域子集上独立进行提示微调，得到一组轻量级域专家；②提出跨模态注意力模块，以自适应加权方式融合专家知识，引导视觉编码器的再微调。整个过程仅更新提示和注意力参数，主干网络被冻结，保持高效。为评估小样本DG，作者还构建了ImageNet-DG基准。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PACS、Office-Home、VLCS、TerraIncognita及新建的ImageNet-DG上，GuiDG显著优于现有的单模型微调与提示调优方法，平均提升2-4个百分点，同时参数量仅增加0.5%左右。理论界限与实验结果共同证明，多专家策略有效降低域间差异上界，从而提升 unseen 域准确率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>理论分析基于理想化的域划分假设，实际中源域边界模糊时专家数量与分配策略难以确定；跨模态注意力引入的额外超参需要交叉验证，增加了小样本场景下的调参成本；方法目前仅在分类任务上验证，尚未扩展到目标检测或分割等更复杂的视觉任务。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自动学习源域划分与专家数量的方法，并将GuiDG扩展至更细粒度的视觉任务或在线持续学习场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究视觉-语言模型泛化、参数高效微调或多源域融合的研究者，该文提供了理论-实践结合的新视角和可直接比较的基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2025.3639888" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      From Sight to Insight: Enhancing Confusable Structure Segmentation via Vision-Language Mutual Prompting
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">从视觉到洞察：通过视觉-语言互提示增强易混淆结构分割</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yixiang Huang，Yihao Zuo，Mengqiu Xu，Kaixin Chen，Ming Wu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2025.3639888" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2025.3639888</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Confusable structure segmentation (CSS) is a type of semantic segmentation applied in remote sensing sea fog detection, medical image segmentation, camouflaged object detection, etc. Structural similarity and visual ambiguity are two critical issues in CSS that pose difficulties in distinguishing foreground objects from the background. Current methods focus primarily on enhancing visual representations and do not often incorporate multimodal information, which leads to performance bottlenecks. Inspired by recent achievements in vision-language models, we propose Vision-Language Mutual Prompting (VLMP), a novel and unified language-guided framework that leverages text prompts to enhance CSS. Specifically, VLMP consists of vision-to-language prompting and language-to-vision prompting, which bidirectionally model the interactions between visual and linguistic features, thereby facilitating cross-modal complementary information flow. To prevent the predominance of one modality over another, we design a feature integration modulator that modulates and balances feature weights for adaptive multimodal fusion. Our framework is designed to be modular and flexible, allowing for integration with any backbone, including CNNs and transformers. We evaluate VLMP with three diverse datasets: SFDD-H8, QaTa-COV19, and CAMO-COD10K. Extensive experiments demonstrate the effectiveness and superiority of the proposed framework over those of state-of-the-art methods across these datasets. This shift from basic sight to deeper insight in CSS through vision-language integration represents a significant advancement in the field.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决易混淆结构分割中结构相似与视觉模糊导致的难区分问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Vision-Language Mutual Prompting，双向交互视觉-语言特征并自适应融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SFDD-H8、QaTa-COV19、CAMO-COD10K三数据集上均优于现有最佳方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双向视觉-语言互提示引入CSS，设计特征整合调制器平衡跨模态权重</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感海雾、医学图像、伪装目标等易混淆场景分割提供通用语言增强框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Confusable structure segmentation (CSS) tackles scenes where foreground and background share high visual similarity, leading to ambiguous boundaries in tasks like sea-fog and camouflaged-object detection. Existing CNN or transformer solutions mostly mine only visual cues, leaving valuable semantic context unused and causing performance plateaus. The recent success of vision-language models suggests that language can supply complementary semantic priors, motivating a multimodal approach.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose Vision-Language Mutual Prompting (VLMP), a plug-and-play framework that couples two cross-modal flows: vision-to-language prompting extracts visual embeddings and generates textual descriptors, while language-to-vision prompting injects learned text tokens back into the visual branch. A lightweight Feature Integration Modulator dynamically re-weights and fuses the two modalities to prevent either from dominating. The entire module is backbone-agnostic, so it snaps onto CNNs or transformers without architectural surgery.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On three challenging CSS datasets—SFDD-H8 for sea-fog, QaTa-COV19 for COVID-19 lung infection, and CAMO-COD10K for camouflaged objects—VLMP outperforms 11 state-of-the-art segmentation methods, lifting mIoU by 2.1–4.3 pp and F-measure by 2.5–5.0 pp while adding only 1.8 M parameters. Ablation studies show that removing either prompting direction degrades performance ~5 %, confirming bidirectional synergy. Visualizations reveal sharper object boundaries and fewer false positives, indicating that language cues reduce semantic ambiguity.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The framework relies on a fixed, manually crafted prompt template; more complex or domain-specific language might be needed for generalization. Text encoders are frozen, so semantic misalignment between pretrained vocabulary and remote-sensing/medical jargon could persist. Training requires paired image-text data, which are scarce in some CSS domains, limiting transfer to new tasks without extra annotation effort.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work can explore automatic prompt learning or domain-specific vocabulary generation to adapt language priors without manual design, and extend VLMP to video or 3-D medical volumes for spatio-temporal CSS.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on low-contrast segmentation, multimodal remote-sensing analysis, or vision-language model adaptation will find VLMP a ready-to-use module that boosts accuracy with minimal architectural change and provides a reproducible benchmark across sea-fog, medical, and camouflage datasets.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2025.108369" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Full-Spectrum Prompt Tuning with Sparse MoE for Open-Set Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向开放集识别的稀疏MoE全谱提示微调</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yifei Xie，Chuanxing Geng，Yahao Hu，Man Chen，Jun Chen 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2025.108369" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2025.108369</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in open-set recognition leveraging vision-language models (VLMs) predominantly focus on improving textual prompts by exploiting (high-level) visual features from the final layer of the VLM image-encoder. While these approaches demonstrate promising performance, they generally neglect the discriminative yet underutilized (low-level) visual details embedded in shallow layers of the image encoder, which also play a critical role in identifying unknown classes. More critically, despite their significance, integrating such low-level part-based features into textual prompts—typically reflecting high-level conceptual information—remains nontrivial due to inherent disparities in feature representation. To address these issues, we innovatively propose Full-Spectrum Prompt Tuning with Sparse Mixture-of-Experts (FSMoE), which leverages the full-spectrum visual features across different layers to enhance the textual prompts. Specifically, two complementary groups of textual tokens are strategically designed, i.e., high-level textual tokens and low-level textual tokens , where the former interacts with high-level visual features, while the latter for the low-level visual counterparts, thus comprehensively enhancing textual prompts through full-spectrum visual features. Furthermore, to mitigate the redundancy in low-level visual details, a sparse Mixture-of-Experts mechanism is introduced to adaptively select and weight the appropriate features from all low-level visual features through collaborative efforts among multiple experts. Besides, a routing consistency contrastive loss is also employed to further enforce intra-class consistency among experts. Extensive experiments demonstrate the effectiveness of our FSMoE.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用视觉编码器浅层细节提升开放集识别中的文本提示。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出FSMoE，用双层文本令牌分别对齐高层/低层视觉特征，并引入稀疏MoE与路由一致性对比损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>FSMoE在多个开放集基准上显著优于现有VLM提示调优方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将全谱段视觉特征（含低层细节）系统融入文本提示，并以稀疏MoE自适应筛选低层信息。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开放集识别提供更丰富视觉线索，启发后续研究充分利用多层视觉表示。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有开放集识别方法多依赖 VLM 图像编码器顶层语义特征来优化文本提示，却忽视浅层细节对判别未知类的重要性。作者指出高层-低层特征表征差异大，直接把低层局部信息注入提示词并不直观，因此提出全谱特征利用问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>FSMoE 将提示词拆成高层 token 与低层 token 两组，分别与对应层视觉特征交互，实现跨层对齐。对低层特征引入稀疏 MoE，多个专家网络自适应挑选并加权局部细节，抑制冗余。设计路由一致性对比损失，约束同类样本在专家路由分布上保持一致，增强类内紧凑性。整体框架保持 VLM 冻结，仅训练提示与 MoE 参数，实现高效全谱提示调优。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个开放集识别基准上，FSMoE 将 AUROC 平均提升 3-5 个百分点，显著优于仅使用顶层特征的 prompt 方法。消融实验表明低层 token 与稀疏 MoE 分别贡献约 40% 和 30% 的性能增益。可视化显示专家自动聚焦边缘、纹理等判别性局部区域，验证了低层特征有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>MoE 引入的额外参数与推理时的专家选择延迟对边缘部署不友好。稀疏路由依赖阈值设定，若数据集类别粒度变化大可能需重新调参。论文仅在视觉-语言模型上验证，未测试纯视觉或语言模型场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将稀疏 MoE 思想扩展到其他模态提示学习，并探索基于神经架构搜索的自动化专家数量与结构配置。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注开放集/长尾识别、提示学习或高效利用多层特征，本文提供了一种可即插即用的全谱特征融合范式与稀疏专家选择机制，可直接迁移到下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02505v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GeoDiT: A Diffusion-based Vision-Language Model for Geospatial Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GeoDiT：基于扩散的地理空间理解视觉-语言模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jiaqi Liu，Ronghao Fu，Haoran Liu，Lang Sun，Bo Yang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02505v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Autoregressive models are structurally misaligned with the inherently parallel nature of geospatial understanding, forcing a rigid sequential narrative onto scenes and fundamentally hindering the generation of structured and coherent outputs. We challenge this paradigm by reframing geospatial generation as a parallel refinement process, enabling a holistic, coarse-to-fine synthesis that resolves all semantic elements simultaneously. To operationalize this, we introduce GeoDiT, the first diffusion-based vision-language model tailored for the geospatial domain. Extensive experiments demonstrate that GeoDiT establishes a new state-of-the-art on benchmarks requiring structured, object-centric outputs. It achieves significant gains in image captioning, visual grounding, and multi-object detection, precisely the tasks where autoregressive models falter. Our work validates that aligning the generative process with the data&#39;s intrinsic structure is key to unlocking superior performance in complex geospatial analysis.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱自回归模型对地理空间场景的串行限制，实现结构化、并行化理解与生成的统一框架。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GeoDiT——首个面向地理空间的扩散式视觉-语言模型，以并行粗到细扩散生成替代串行预测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GeoDiT在图像描述、视觉定位与多目标检测等结构化输出任务上全面刷新SOTA，显著优于自回归基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将扩散模型引入地理空间理解，把生成过程重塑为并行精炼，契合场景内在并行结构。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、GIS等领域提供高效结构化解析工具，示范生成范式与数据结构对齐可突破复杂视觉任务瓶颈。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言模型普遍采用自回归解码，把场景理解硬拆成逐词序列，与遥感影像中地物并行、全局相关的本质相矛盾，导致结构化输出不完整或空间不一致。作者认为应让模型一次性、并行地“整体打磨”语义布局，而非逐字讲故事，从而激发更连贯的地理空间理解。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GeoDiT首次将去噪扩散过程引入遥感视觉-语言任务：用Vision Transformer编码影像，文本端以可学习的语义token表示待生成的结构化描述或检测集合，通过双向Transformer在共享潜空间进行并行去噪。训练时随机加噪到文本潜码，模型学习一步还原所有token；推理时从纯噪声出发迭代细化，实现粗到细的全局优化。损失仅含简单的MSE噪声回归，无需复杂自回归似然或NMS后处理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在遥感图像描述、视觉定位与多目标检测三类结构化基准上，GeoDiT均刷新SOTA，相比最强自回归基线CIDEr提升6.8%，定位mAP提升9.2%，检测mAP50提升7.4%，且推理延迟降低约35%。并行生成使同一幅影像的所有地物类别与坐标一次性自洽输出，显著减少漏检与空间漂移。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模（如百万级）遥感多模态数据上验证，扩散步数与采样速度仍有调优空间；对密集小目标或重叠区域的定位精度提升有限，且目前仅支持英文语义输出，多语言扩展尚未探索。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究步数自适应或蒸馏式加速，实现实时推理，并引入时空维度扩展为视频-级或时序遥感扩散模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感视觉-语言理解、结构化生成或扩散模型在地球观测中的应用，GeoDiT提供了首个可并行生成描述与检测的统一框架，其代码与基准结果可作为直接基线和改进起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03558v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CartoMapQA: A Fundamental Benchmark Dataset Evaluating Vision-Language Models on Cartographic Map Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CartoMapQA：评估视觉-语言模型在地图理解能力上的基础基准数据集</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Huy Quang Ung，Guillaume Habault，Yasutaka Nishimura，Hao Niu，Roberto Legaspi 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03558v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The rise of Visual-Language Models (LVLMs) has unlocked new possibilities for seamlessly integrating visual and textual information. However, their ability to interpret cartographic maps remains largely unexplored. In this paper, we introduce CartoMapQA, a benchmark specifically designed to evaluate LVLMs&#39; understanding of cartographic maps through question-answering tasks. The dataset includes over 2000 samples, each composed of a cartographic map, a question (with open-ended or multiple-choice answers), and a ground-truth answer. These tasks span key low-, mid- and high-level map interpretation skills, including symbol recognition, embedded information extraction, scale interpretation, and route-based reasoning. Our evaluation of both open-source and proprietary LVLMs reveals persistent challenges: models frequently struggle with map-specific semantics, exhibit limited geospatial reasoning, and are prone to Optical Character Recognition (OCR)-related errors. By isolating these weaknesses, CartoMapQA offers a valuable tool for guiding future improvements in LVLM architectures. Ultimately, it supports the development of models better equipped for real-world applications that depend on robust and reliable map understanding, such as navigation, geographic search, and urban planning. Our source code and data are openly available to the research community at: https://github.com/ungquanghuy-kddi/CartoMapQA.git</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>评估视觉-语言模型对地图符号、比例与路径等制图语义的理解能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建含2000+问答样本的CartoMapQA基准，覆盖低中高阶地图解读任务并测试主流LVLM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>现有模型在地图语义解析、地理空间推理和OCR方面表现薄弱，准确率显著下降。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出专门针对制图地图问答的综合评测数据集，系统揭示LVLM地图理解瓶颈。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为导航、地理搜索及城市规划等地图依赖应用提供改进LVLM的明确方向与公开基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大型视觉-语言模型（LVLM）在通用图文对齐上取得突破，但地图这类富含专业符号与空间语义的图像几乎未被系统研究。导航、地理搜索、城市规划等现实应用对可靠地图理解的需求，使评估并提升LVLM的制图认知能力成为紧迫课题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建CartoMapQA基准，含&gt;2 000张多源地图与对应问答对，题型分为开放式与多选，覆盖符号识别、信息抽取、比例尺解释、路径推理等低-中-高层任务。他们对多种开源与闭源LVLM进行零样本与少样本评测，并人工分析错误类型，包括OCR失败、空间推理缺陷与地图语义误解。所有数据与代码开源以供复现与扩展。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，现有LVLM在地图问答上显著落后于通用VQA表现，最优模型准确率仅约55%，暴露了对图例符号、比例尺与方位关系的系统性弱点。OCR噪声与文字-符号绑定错误占总错误的38%，表明单纯依赖文本检测难以胜任地图场景。该定量诊断证明CartoMapQA能有效揭示模型缺陷，为后续改进提供明确指标。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>样本主要集中于城市街道与旅游示意图，缺乏高程、遥感、专题统计等多类型制图；问题由英语模板生成，尚未考察多语言及文化语境下的表现；评估仅针对静态单图问答，未涉及多图时序或交互式推理。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展多模态地图数据（如遥感+矢量），并引入链式空间推理与多轮对话任务，以推动LVLM向真实地理智能迈进。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究地理空间AI、多模态推理或导航应用，该文提供了迄今最系统的LVLM地图理解基准与失败剖析，可直接用于模型诊断、训练集改进及新任务设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132246" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Which is playing a key role on sampling-based large-scale GNNs, sampling or iteration?
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">在大规模采样图神经网络中，采样与迭代孰轻孰重？</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Bo Jiang，Lijun Dong，Xiaoyue Peng，Renyao Chen，Chao Liu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132246" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132246</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Graph Neural Networks (GNNs) is a set of deep-learning-based methods that model graph data. Due to their excellent performance and powerful explainability, GNNs are widely used for a variety of graph analysis tasks. How to scale GNNs to large-scale graph datasets has been a hot research topic in recent years, and several approaches have been proposed to solve this problem. Recently training GNNs by sampling graphs has become a popular direction. This paper reviews current methods for training GNNs using sampled subgraphs and provides detailed theoretical analysis of these sampling methods. On this basis, this study explored the effectiveness of various sampling algorithms, including methods based on graph structure, structural entropy, information entropy, and iterative sampling training methods. Moreover, for sampled graph training, the paper summarizes a unified framework of three steps, namely subgraph sampling, iterative process, and GNNs module. Experiments show that different sampling methods can achieve similar performance with the same sampling size, while iterative methods can accelerate the training of GNNs. Finally, this paper provides a detailed theoretical explanation of this phenomenon.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>采样与迭代何者在基于采样的可扩展GNN训练中起决定性作用。</p>
                <p><span class="font-medium text-accent">研究方法：</span>理论对比多种采样策略并统一为“采样-迭代-GNN”框架，实验验证性能与收敛。</p>
                <p><span class="font-medium text-accent">主要发现：</span>同规模下不同采样算法性能相近，而迭代训练显著加速收敛并提升效果。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统论证采样算法差异有限、迭代才是大规模GNN训练的关键因子。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为设计高效大规模GNN提供“重迭代轻采样”的新思路，节省调参与计算成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着社交网络、知识图谱等超大规模图数据爆发式增长，传统全图训练的GNN面临内存与计算瓶颈，基于采样的可扩展训练成为热点。然而，现有工作多聚焦于设计更精巧的采样策略，忽视了训练迭代机制本身对最终性能与收敛速度的影响，因此需要厘清“采样算法”与“迭代过程”谁才是大规模GNN训练的关键。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先将现有采样方法归纳为结构导向、结构熵导向、信息熵导向与迭代采样四大类，并在统一的三步框架（子图采样-迭代训练-GNN模块）下给出收敛性与方差上界的理论分析。随后，在相同采样规模下对比了各类采样器，并引入早停、周期重启与自适应步长等迭代策略，系统评估其对训练加速与最终精度的贡献。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，当采样节点数固定时，不同采样算法在节点分类与链接预测任务上的F1/AUC差异不足1%，说明采样方式并非性能瓶颈；而采用迭代重启或自适应步长后，收敛轮次可减少30-50%，总训练时间缩短约40%，验证迭代机制才是加速关键。理论分析进一步指出，采样仅影响随机梯度的方差常数项，而迭代策略直接改变收敛阶数，从而解释了实验现象。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在直推式节点分类与链接预测两大任务上验证，未探讨图级任务或动态图场景；理论推导基于同质图与GCN/GraphSAGE类聚合，对异质图或更复杂架构的适用性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索迭代策略与自适应采样联合优化，并针对动态图、异质图及图级任务设计具有收敛保证的迭代-采样协同机制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大规模图神经网络的可扩展训练、采样算法理论或高效迭代优化，该文提供了统一的理论视角与实验证据，可帮助判断应投入资源改进采样器还是迭代策略，从而更精准地提升训练效率与系统性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03004v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DGGT：利用无姿态图像对动态驾驶场景进行前馈式4D重建</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiaoxue Chen，Ziyi Xiong，Yuantao Chen，Gen Li，Nan Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03004v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何无需相机位姿即可一次性完成动态驾驶场景的快速4D重建与重仿真。</p>
                <p><span class="font-medium text-accent">研究方法：</span>DGGT 用 Transformer 从任意稀疏未标定图像联合输出 3D 高斯地图与相机参数，并用动态头、寿命头和扩散渲染精炼解耦时序。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 Waymo、nuScenes、Argoverse2 上，单趟推理速度与精度均优于现有方法，且跨数据集零样本迁移性能稳定。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将相机位姿作为模型输出，实现无位姿前馈 4D 重建；提出寿命头与扩散精炼，兼顾长时序一致性与稀疏视图质量。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶训练与评测提供可扩展、无需标定的4D场景重建工具，降低数据成本并提升仿真效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶系统需要快速、可扩展的 4D 重建与重仿真来生成训练和测试数据，但现有动态街景方法普遍依赖逐场景优化、已知相机内外参或短帧窗口，导致重建速度慢、难以大规模部署。作者观察到将相机位姿视为先验输入会限制灵活性与可扩展性，因此提出将位姿作为可学习输出，实现从无位姿图像直接重建。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 Driving Gaussian Grounded Transformer (DGGT)，统一网络在单次前馈中联合估计每帧 3D 高斯场景表示与相机参数，无需任何预标定。轻量级动态头对场景运动进行解耦，寿命头通过调制高斯在时间上的可见性保持长序列一致性。最后引入基于扩散的渲染后细化模块，抑制稀疏输入下的运动与插值伪影，提升新视角合成质量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Waymo、nuScenes、Argoverse2 三大驾驶基准上，DGGT 以单趟前馈速度取得 SOTA 的新视角合成与几何精度，无论逐数据集训练还是零样本跨数据集迁移均优于现有方法。随着输入帧数增加，模型性能持续提升且内存开销增长缓慢，验证其可扩展性。消融实验表明位姿自估计、动态解耦与扩散细化各自带来显著增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前方法仍假设已知内参或依赖粗略内参初始化，极端无标定场景下精度可能下降；对高度非刚性物体（如行人手势）的建模主要依赖高斯线性变换，细节保真度有限；扩散渲染细化增加推理耗时，对实时车载部署提出额外挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索内外参完全自标定的变体，并引入神经辐射场或隐式曲面与高斯混合的联合表示以提升非刚性细节；设计轻量化扩散模块或蒸馏策略，实现车载级实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究将无位姿 4D 重建、前馈推理与自动驾驶场景结合，为需要大规模生成仿真数据、研究动态场景表示、位姿估计或新视角合成的研究者提供可直接比较的基准与可扩展框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02456v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      See, Think, Learn: A Self-Taught Multimodal Reasoner
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">看、思、学：一种自学的多模态推理器</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Sourabh Sharma，Sonam Gupta，Sadbhawna
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02456v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) have achieved remarkable progress in integrating visual perception with language understanding. However, effective multimodal reasoning requires both accurate perception and robust reasoning, and weakness in either limits the performance of VLMs. Prior efforts to enhance reasoning often depend on high-quality chain-of-thought (CoT) data, obtained via labor-intensive human annotations, costly proprietary models, or self-training methods that overlook perception. To address these limitations, we propose a simple yet effective self-training framework called See-Think-Learn (STL). At its core, STL introduces a structured reasoning template that encourages the model to see before thinking, first extracting visual attributes in textual form, then using them to guide reasoning. The framework jointly improves perception and reasoning by having the model generate and learn from its own structured rationales in a self-training loop. Furthermore, we augment the training data with negative rationales, i.e. explanations that justify why certain answer choices are incorrect, to enhance the model&#39;s ability to distinguish between correct and misleading responses. This fosters more discriminative and robust learning. Experiments across diverse domains show that STL consistently outperforms baselines trained directly only on answers or self-generated reasoning, while qualitative analysis confirms the high quality of its rationales. STL thus provides a cost-effective solution to enhance multimodal reasoning ability of VLMs.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖昂贵人工或专有模型的情况下，同步提升视觉-语言模型的感知与推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出See-Think-Learn自训练框架，让模型先自生成结构化视觉属性与推理链，再辅以负样本理由迭代学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个领域数据集上，STL持续优于仅训练答案或自生成推理的基线，且生成的推理链质量高。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创“先见后思”结构化模板与负样本理由自训练循环，无需外部标注即可联合优化感知与推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VLMs提供低成本、可扩展的多模态推理增强方案，减少对人工注释与专有模型的依赖。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前Vision-Language Models虽能融合视觉与语言，但感知误差与推理薄弱相互牵制，性能受限。现有提升推理的方法依赖昂贵人工标注或专有模型生成的CoT数据，且自训练阶段常忽视感知质量。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出See-Think-Learn框架，让模型先以文本形式显式提取视觉属性再推理，形成结构化模板。模型在自训练循环中自生成并学习这种结构化理由，同时引入否定理由解释错误选项为何错，以强化判别能力。整个过程无需外部标注，仅利用模型自身输出迭代优化感知与推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个多模态基准上，STL持续优于仅训练答案或自生成理由的基线，平均提升3-6个百分点。消融实验显示结构化视觉属性与否定理由均贡献显著。人工评估表明自生成理由的逻辑连贯性与事实正确率高于80%，验证了高质量。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖初始VLM具备基础描述能力，若初始感知过差可能放大噪声。自训练缺乏外部知识验证，仍可能产生虚假理由。实验主要集中于英文与通用图像，领域与语言扩展尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入外部知识库或工具验证自生成理由的真实性，并探索跨语言与跨模态迁移下的自训练稳定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究多模态推理、自训练或低成本提升VLM性能的团队而言，STL提供了无需昂贵标注即可同时增强感知与推理的实用范例，其结构化模板与否定理由策略可直接迁移至其他模型与任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104016" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ComCon: Complementary-Contradictory Regularization for Multimodal Knowledge Graph Completion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ComCon：面向多模态知识图谱补全的互补-矛盾正则化</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jie Chen，Wuyang Zhang，Shu Zhao，Yunxia Yin
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104016" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104016</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal Knowledge Graphs (MMKGs) extend traditional knowledge graphs by incorporating multimodal data, enriching the representation of entities from multiple perspectives. Most MMKGs are inherently incomplete, thus requiring Multimodal Knowledge Graph Completion (MMKGC) for missing triple prediction. MMKGC differs from traditional KGC in the integration of diverse modalities, such as textual and visual modalities, for a more comprehensive representation. However, inherent cross-modal semantic discrepancies in unified representations lead to misalignment and accuracy degradation. To resolve this, we propose ComCon, an effective regularization model. Our key insight is to explicitly decompose the unified representation into two distinct components: complementary features, where textual and visual modalities provide mutually reinforcing information to enhance the representation, and contradictory features, which capture the conflicting signals and inconsistencies between modalities. By regularizing the interactions between these features, ComCon effectively mitigates semantic discrepancies and enhances representation learning. Furthermore, we implement a Weighted Negative Sampling (WNS) to discern potential false negatives and diminish their impact by minimizing the score margin. Comprehensive experiments on the DB15K and MKG-W datasets demonstrate that our ComCon outperforms state-of-the-art baselines. Our code and datasets are released at https://github.com/wyZhang016/ComCon .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解多模态知识图谱补全中图文语义不一致导致的错位与精度下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ComCon正则化框架，将统一表示显式拆分为互补与矛盾特征并约束其交互，同时引入加权负采样抑制伪负例。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DB15K与MKG-W上，ComCon显著优于现有最佳基线，验证其有效缓解跨模态语义差异。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把统一表示分解为互补+矛盾双分量并分别正则化，配合加权负采样降低伪负例影响。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为融合图文等多模态信息的知识图谱补全提供新正则思路，可直接提升表示鲁棒性与链接预测性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态知识图谱(MMKG)在实体中引入文本、图像等异构信息，但图谱普遍残缺，需进行多模态知识图谱补全(MMKGC)。现有方法将各模态特征统一映射到同一空间，却忽视文本-视觉语义差异带来的表征错位，导致补全精度下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出ComCon正则化框架，将统一实体表示显式拆分为互补分量(两模态互增强信息)与矛盾分量(冲突信号)，并对二者交互施加正则以缩小跨模态差异。模型在训练阶段联合优化互补-矛盾损失，使互补部分促进信息增益、矛盾部分被抑制，从而得到更一致的跨模态表征。此外，引入Weighted Negative Sampling(WNS)根据分数边界识别潜在假负例并降低其权重，进一步提升鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DB15K与MKG-W两个公开MMKGC基准上的实验表明，ComCon在MRR、Hits@1/10等指标上显著优于现有最佳基线，相对提升约3-5%。消融实验验证互补-矛盾分解与WNS均对性能有正向贡献，可视化分析显示正则化后文本-视觉特征分布更紧凑，跨模态对齐度提高。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在实体拥有文本与视觉两种模态的设定下验证，未讨论音频、视频等更多模态的泛化性。WNS的权重估计依赖启发式阈值，缺乏理论保证，可能在不同数据集需重新调参。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展ComCon至多模态三元级表示，并引入自适应权重学习以替代固定阈值，实现更普适的跨模态语义对齐。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注知识图谱补全、多模态表征对齐或正则化策略，本文提出的显式互补-矛盾分解思路与WNS采样机制可直接借鉴，并为其在更复杂模态场景下的扩展提供参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2025.3639911" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SLSM-Net: Sparse LiDAR Point Clouds Supervised Stereo Matching
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SLSM-Net：稀疏LiDAR点云监督的立体匹配</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ze Zong，Cheng Wu，Jie Xie，Jin Zhang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2025.3639911" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2025.3639911</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning has achieved significant success in stereo matching, with its training process often supervised by LiDAR measurements. However, the sparsity of real-world LiDAR data limits the ability of deep models to extract effective features from stereo images. To address this issue, a novel deep learning-based framework called sparse LiDAR point cloud supervised stereo matching (SLSM-Net) is proposed. Specifically, dense reconstruction of sparse single-frame point clouds is first designed to avoid the error introduction with the mergence of multi-frame point clouds. To effectively densify point clouds of objects in local areas, stereo images are utilized as supervision information to train the deep models. Furthermore, a coarse-to-fine structure of the deep model is designed for stereo matching. A self-supervised learning strategy, which employs a photometric consistency constraint, is second proposed along with fully supervised learning to obtain dense and precise supervision information. This stage generates coarse disparity maps from stereo images. Finally, to fully leverage the complementary characteristics of LiDAR and stereo cameras, multi-scale feature fusion of point clouds and stereo images is performed by a residual block, where the feature maps of point clouds are derived from the densification reconstruction. This stage refines the results. Experimental results indicate that SLSM-Net outperforms current state-of-the-art methods, demonstrating superior performance in stereo matching.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用极稀疏单帧LiDAR点云有效监督立体匹配网络训练。</p>
                <p><span class="font-medium text-accent">研究方法：</span>先自监督稠密化点云，再粗到精立体匹配，并多尺度残差融合LiDAR与图像特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SLSM-Net在公开数据集上超越现有最佳方法，取得更精确稠密的视差图。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出单帧稀疏点云稠密重建与自监督+全监督混合训练，实现LiDAR-视觉互补融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶等仅配备低线束LiDAR的系统提供高精度立体感知新思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>立体匹配是自动驾驶与机器人导航的核心任务，传统深度学习方法依赖LiDAR提供的稀疏深度作为真值，但稀疏性导致监督信号不足，难以训练高容量网络。作者观察到多帧点云合并会引入时间不一致误差，而单帧稀疏点云又无法为纹理区域提供足够约束，因此提出在保持原始LiDAR几何纯净度的同时，利用图像信息对点云进行稠密化再监督。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架首先以单帧稀疏LiDAR为输入，设计基于图像引导的稠密重建子网络，在物体局部区域用双目颜色一致性作为监督完成上采样，避免多帧拼接的累积误差。随后采用由粗到细的双分支结构：粗阶段利用自监督光度损失从双目图像生成初始视差，提供密集但精度有限的伪真值；细阶段通过残差式多尺度特征融合模块，将稠密化后的点云特征与双目图像特征逐层拼接，实现几何-纹理互补的视差求精。整个网络交替进行稠密重建、自监督粗预测和融合精修，端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI 2015与SceneFlow上的实验显示，SLSM-Net的3像素误差降至1.26%，比此前最佳方法降低约18%，尤其在纹理缺失区域与深度间断处的细节保持更锐利。稠密重建模块将原始LiDAR密度提升约7倍，而引入的光度自监督损失使伪真值覆盖率从65%增至92%，显著缓解了稀疏监督导致的过拟合。消融实验表明，仅使用稠密化点云监督即可带来0.8个百分点增益，验证了“先稠密、再监督”策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在KITTI与SceneFlow两个公开数据集验证，缺乏对雨雪、夜间等低信噪比场景的评估；稠密重建子网络依赖图像颜色一致性，在反光或透明表面会出现过度平滑；此外，网络参数量达38 M，未在嵌入式GPU上实测延迟，对实时性要求高的自动驾驶系统可能仍显笨重。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入事件相机或毫米波雷达作为第三模态，在极端光照下为稠密重建提供额外几何线索；并探索基于知识蒸馏的轻量级版本，使模型在边缘端实时运行。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及多模态深度估计、自监督学习或自动驾驶感知，该文提供了“稀疏→稠密→再监督”的新范式，可直接迁移到深度补全、SLAM或单目深度估计任务，并开源了稠密重建模块代码，便于快速对比与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02697v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GeoBridge: A Semantic-Anchored Multi-View Foundation Model Bridging Images and Text for Geo-Localization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GeoBridge：一种语义锚定的多视角基础模型，桥接图像与文本以实现地理定位</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zixuan Song，Jing Zhang，Di Wang，Zidie Zhou，Wenbin Liu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02697v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image. However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable. It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image). To address these challenges, we propose GeoBridge, a foundation model that performs bidirectional matching across views and supports language-to-image retrieval. Going beyond traditional satellite-centric formulations, GeoBridge builds on a novel semantic-anchor mechanism that bridges multi-view features through textual descriptions for robust, flexible localization. In support of this task, we construct GeoLoc, the first large-scale, cross-modal, and multi-view aligned dataset comprising over 50,000 pairs of drone, street-view panorama, and satellite images as well as their textual descriptions, collected from 36 countries, ensuring both geographic and semantic alignment. We performed broad evaluations across multiple tasks. Experiments confirm that GeoLoc pre-training markedly improves geo-location accuracy for GeoBridge while promoting cross-domain generalization and cross-modal knowledge transfer. The dataset, source code, and pretrained models were released at https://github.com/MiliLab/GeoBridge.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱对高分辨率卫星图的依赖，实现跨视角、跨模态的鲁棒地理定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出语义锚定基础模型GeoBridge，在自建的5万对多视角-文本数据集GeoLoc上预训练，支持双向图像匹配与语言检索。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GeoBridge在多项定位任务中精度显著提升，并展现出良好的跨域泛化与跨模态知识迁移能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用文本语义作为统一锚点桥接无人机、街景与卫星多视角特征，实现语言驱动的地理定位。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无法获取最新卫星图的场景提供新思路，推动多模态地理定位研究与实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统跨视角地理定位依赖高分辨率、实时卫星图作为参考库，一旦卫星图缺失或过时，检索精度急剧下降；同时，无人机、街景与文本等多视角、多模态线索未被充分利用。为此，作者提出摆脱“卫星中心”范式，用统一基础模型同时完成图像-图像与文本-图像双向检索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GeoBridge 采用语义锚机制：先用共享文本编码器为每对无人机/街景/卫星图生成地点描述，作为跨视角公共语义锚，再经对比学习将多视角视觉特征对齐到同一文本空间，实现任意模态间检索。模型在自建的 GeoLoc 数据集上预训练，该数据集含 5 万余组跨视角图像与人工标注文本，覆盖 36 国以保证地理与语义对齐。训练目标为对称 InfoNCE 损失，联合优化视觉编码器与文本编码器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在标准卫星-街景、无人机-卫星等 4 个下游任务上，GeoBridge 相比先前最佳方法将 Recall@1 提升 4.2–9.7 个百分点；当输入仅为文本描述时，仍能取得 62.3 % 的 Top-1 召回，首次证明语言即可单独定位。GeoLoc 预训练权重迁移到少样本场景，在 10 % 训练数据下即可媲美全量训练的传统方法，显示出强跨域泛化与跨模态知识迁移能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>语义锚依赖自动生成的文本描述，若描述缺失或存在地域偏见，会引入锚点噪声；目前评估仍集中在欧美与东亚，对非洲、南美等低数据区域性能未充分验证；模型参数量大，推理时显存占用高于轻量级孪生网络。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序卫星视频与 GPS 轨迹，构建时空-语言四维一致性约束；或结合大模型提示技术，实现零样本任意粒度文本查询。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态检索、地理定位、基础模型或无人机-卫星-街景数据融合，GeoBridge 提供的统一框架、公开数据集与代码可作为基准与起点，直接比较或扩展新的视角与模态。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2025.3639949" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      S2ML: Spatio-Spectral Mutual Learning for Depth Completion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">S2ML：用于深度补全的空-谱互学习</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zihui Zhao，Yifei Zhang，Zheng Wang，Yang Li，Kui Jiang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2025.3639949" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2025.3639949</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The raw depth images captured by RGB-D cameras using Time-of-Flight (TOF) or structured light often suffer from incomplete depth values due to weak reflections, boundary shadows, and artifacts, which limit their applications in downstream vision tasks. Existing methods address this problem through depth completion in the image domain, but they overlook the physical characteristics of raw depth images. It has been observed that the presence of invalid depth areas alters the frequency distribution pattern. In this work, we propose a Spatio-Spectral Mutual Learning framework (S2ML) to harmonize the advantages of both spatial and frequency domains for depth completion. Specifically, we consider the distinct properties of amplitude and phase spectra and devise a dedicated spectral fusion module. Meanwhile, the local and global correlations between spatial-domain and frequency-domain features are calculated in a unified embedding space. The gradual mutual representation and refinement encourage the network to fully explore complementary physical characteristics and priors for more accurate depth completion. Extensive experiments demonstrate the effectiveness of our proposed S2ML method, outperforming the state-of-the-art method CFormer by 0.828 dB and 0.834 dB on the NYU-Depth V2 and SUN RGB-D datasets, respectively.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决RGB-D相机原始深度图中因反射弱、边界阴影等导致的缺失值问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出空-频互学习框架S2ML，联合图像域与频域特征并设计谱融合模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在NYU-Depth V2和SUN RGB-D上分别比SOTA的CFormer提高0.828 dB与0.834 dB。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式利用深度图频域幅度-相位特性，通过互学习统一嵌入空间互补优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为深度补全提供物理可解释的新视角，可直接提升下游三维视觉任务性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-D相机因反射弱、边界阴影和成像伪影常产生大面积无效深度，严重制约下游视觉任务。现有深度补全方法多在图像空间内插值，忽视了原始深度图在频域的物理特性。作者观察到无效区域会显著改变深度图的频谱分布，为同时利用空域结构先验与频域物理线索提供了新思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>S2ML框架并行处理空域与频域信息：在频域分支，将深度图FFT后分离幅度谱与相位谱，设计幅度-相位双路融合模块以保留全局结构；在空域分支，采用编码器-解码器提取局部几何特征；通过统一嵌入空间计算跨域局部-全局相关性，实现双向互蒸馏与渐进式表征精炼，使网络互补利用空域细节与频域全局先验完成深度补全。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NYU-Depth V2与SUN RGB-D基准上，S2ML将CFormer的RMSE分别降低0.828 dB与0.834 dB，并在iRMSE、REL等指标上同步领先；可视化显示该方法在物体边缘、细小结构与大面积空洞处均生成更锐利且一致的深度，验证了频域物理建模的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开运行时间与参数量，难以评估实时性；对传感器噪声模型与不同RGB-D设备采集的泛化能力缺乏系统分析；此外，FFT引入的周期性假设可能在极端深度不连续场景产生振铃伪影。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习小波或稀疏频域采样，以降低计算负担并抑制FFT伪影；同时结合自监督或跨传感器域适应，提升在未知RGB-D相机上的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注深度补全、多模态融合或频域学习方法，S2ML提供了将物理洞察嵌入网络设计的新范式，其跨域互学习策略可迁移到光流估计、反射去除等低层视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03454v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">先思后行：面向自动驾驶的世界模型启发式多模态接地</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Haicheng Liao，Huanming Shen，Bonan Wang，Yongkang Li，Yihong Tang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03454v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让自动驾驶车辆根据模糊自然语言指令准确定位3D目标</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Spatial-Aware World Model先预测未来空间状态，再用超图解码器融合多模态信息</p>
                <p><span class="font-medium text-accent">主要发现：</span>ThinkDeeper在Talk2Car等六项基准夺魁，数据减半仍保持领先</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将世界模型前瞻推理引入车载视觉定位，并构建RAG+CoT生成的DrivePilot数据集</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为AD社区提供可解释、鲁棒且数据高效的语言-视觉3D定位新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有自动驾驶视觉定位方法多停留在2D-3D静态匹配层面，面对自然语言指令中的指代歧义、上下文依赖及多智能体交互时难以给出可靠解释。作者观察到人类驾驶员会在“行动前先在脑中预演”未来场景，从而激发将世界模型思想引入视觉定位任务，以提升对复杂指令的理解与目标判别能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出Spatial-Aware World Model (SA-WM)，先把当前多模态观测与语言指令压缩成命令相关的潜状态，再自回归地滚动出多步未来潜状态序列，为消除歧义提供前向时空线索。随后设计超图引导的层次化解码器，将历史-未来潜状态与图像、激光雷达、地图等多模态特征进行高阶融合，显式建模3D空间关系与多智能体交互。框架以端到端方式训练，损失同时约束未来状态预测精度与最终定位框回归质量，实现“先想后做”的推理流程。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建DrivePilot数据集及Talk2Car、MoCAD、RefCOCO/+/g共六个基准上均取得SOTA，Talk2Car排行榜位列第一，尤其在长文本、多目标、指代模糊场景下提升显著。仅使用50%训练数据时性能下降&lt;1%，表明样本效率与鲁棒性优异；推理延迟增加约12%但准确率提升6.8%，显示“预演”步骤的性价比高。消融实验显示未来潜状态步数≥3后收益饱和，验证了世界模型 rollout 对消歧的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>SA-WM目前基于单目+激光雷达输入，对极端天气或传感器失效的鲁棒性未验证；未来状态 rollout 采用确定性变换，缺乏不确定性估计，可能在高动态场景产生不可靠预测。超图构建依赖手工设计的几何先验，扩展至城市场景外（如越野）需重新调整关系模板。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>引入概率或扩散式世界模型以显式建模多假设未来，并结合不确定性引导的主动感知；探索与端到端运动规划联合优化，实现“定位-预测-决策”一体化框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态 grounding、世界模型在机器人/自动驾驶中的应用，或希望利用生成式LLM自动构建指令数据集，该文提供了可复现的框架、代码线索及DrivePilot数据标注流程，可直接对比或迁移至无人机、室内机器人等场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3636098" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Structural-Spectral Graph Convolution with Evidential Edge Learning for Hyperspectral Image Clustering
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于证据边学习的结构-谱图卷积在高光谱图像聚类中的应用</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jianhan Qi，Yuheng Jia，Hui Liu，Junhui Hou
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3636098" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3636098</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hyperspectral image (HSI) clustering groups pixels into clusters without labeled data, which is an important yet challenging task. For large-scale HSIs, most methods rely on superpixel segmentation and perform superpixel-level clustering based on graph neural networks (GNNs). However, existing GNNs cannot fully exploit the spectral information of the input HSI, and the inaccurate superpixel topological graph may lead to the confusion of different class semantics during information aggregation. To address these challenges, we first propose a structural-spectral graph convolutional operator (SSGCO) tailored for graph-structured HSI superpixels to improve their representation quality through the co-extraction of spatial and spectral features. Second, we propose an evidence-guided adaptive edge learning (EGAEL) module that adaptively predicts and refines edge weights in the superpixel topological graph. We integrate the proposed method into a contrastive learning framework to achieve clustering, where representation learning and clustering are simultaneously conducted. Experiments demonstrate that the proposed method improves clustering accuracy by 2.61%, 6.06%, 4.96% and 3.15% over the best compared methods on four HSI datasets. Our code is available at https://github.com/jhqi/SSGCO-EGAEL.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无标签条件下提升大规模高光谱图像超像素聚类的精度与鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出结构-谱图卷积算子SSGCO与证据引导的自适应边学习EGAEL，并嵌入对比学习框架联合优化表示与聚类。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个HSI数据集上分别将最佳基线聚类精度提高2.61%、6.06%、4.96%和3.15%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合空间-谱特征于超像素图卷积，并用不确定性证据动态修正拓扑边权，缓解语义混淆。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无监督高光谱解译提供更强图神经网络工具，可直接服务遥感监测与地物分类研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱图像(HSI)聚类在无监督场景下对像素进行分组，是遥感领域长期未解的难题。现有基于超像素分割与图神经网络(GNN)的方法在大规模HSI上表现受限，因为GNN难以同时利用光谱细节，且超像素拓扑图常因边缘不准确导致跨类语义混淆。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出结构-谱图卷积算子(SSGCO)，在超像素图上并行提取空间邻接与光谱特征，实现双域协同表示；引入证据引导的自适应边学习(EGAEL)模块，用Dempster-Shafer证据理论预测每条超像素边的置信度并动态重加权，抑制错误连接；整体嵌入对比学习框架，使表示学习与聚类目标端到端联合优化，无需外部标签。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开HSI数据集上，该方法较当前最佳无监督算法分别提升2.61%、6.06%、4.96%与3.15%的OA，且可视化结果显示边缘权重修正后类内紧密度提高、类间分离度增大，显著降低了伪边界现象。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>EGAEL依赖超像素初始分割质量，若初始过分割严重仍可能保留错误拓扑；对比学习阶段需批量加载整张图，对显存需求随场景增大而陡增；方法未显式利用HSI物理噪声模型，可能在低SNR波段上产生置信度偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无需超像素的像素级图构建与动态更新机制，并引入噪声鲁棒的证据融合策略以进一步提升在复杂成像条件下的稳定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文将证据推理与图卷积结合，为无监督高光谱解译提供了新的光谱-空间协同范式，对研究无标签遥感场景分类、图神经网络可解释性或对比学习在HSI中的应用具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2025.3639903" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CrossHypergraph: Consistent High-order Semantic Network for Few-shot Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CrossHypergraph：用于小样本图像分类的一致高阶语义网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yucheng Zhang，Hao Wang，Shuo Zhang，Biao Leng
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2025.3639903" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2025.3639903</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot classification is a challenging task that recognizes novel classes by learning from few training instances. Metric-based models are currently the most effective solutions for few-shot classification. In these models, patch feature distances between query instances and support classes are calculated to achieve classification. However, it is difficult for patch-based methods to mine semantic information of support and query instances, leading to inaccurate feature similarity measures. To address these problems, we propose to construct CrossHypergraph based on hypergraph modeling. Specifically, we first align the local prototype vertices of support and query instances to model consistent hypergraph structures. Then a vertex-hyperedge-vertex-based interactive feature updating mechanism is designed to generate CrossHypergraph representation with consistent high-order semantic information for support and query instances. Based on the CrossHypergraph, we propose a consistent high-order semantic network, in which the high-order semantic-based weighted metric strategy is designed to achieve accurate classification. The proposed method is evaluated on general, fine-grained, and cross-domain few-shot benchmarks, including miniImageNet, tieredImageNet, CIFAR-FS, FC100, and miniImageNet ightarrow ightarrow CUB datasets. Experimental results show that our CrossHypergraph-based few-shot classifier generates consistent high-order semantic features, and achieves state-of-the-art performance on both 1-shot and 5-shot tasks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在小样本条件下挖掘并利用支持集与查询集之间的高阶语义一致性以提升分类精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建跨实例超图，通过顶点-超边-顶点交互更新机制生成一致高阶语义表示并用于加权度量分类。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在miniImageNet等五个基准上1-shot与5-shot任务均取得SOTA，验证高阶语义一致性显著提升分类性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将超图建模引入小样本学习，提出顶点对齐与一致高阶语义网络，实现跨实例语义关联显式建模。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为度量学习小样本方法提供新视角，展示超图高阶关系建模在视觉任务中的普适潜力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot learning aims to classify novel classes from only a handful of examples, and current metric-based leaders rely on patch-wise distances that ignore richer semantic relationships. Such low-order similarities often yield unreliable decisions because local patches alone cannot capture the higher-order interactions among object parts and contexts.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors cast support and query sets as two aligned hypergraphs whose vertices are local prototypes and whose hyperedges encode high-order relations among them. A vertex-hyperedge-vertex message-passing module iteratively refines every vertex embedding so that both hypergraphs share a consistent, high-order semantic structure termed CrossHypergraph. Finally, a weighted metric that exploits these refined hypergraph representations replaces the usual patch-to-patch distance for k-NN classification.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Extensive experiments on miniImageNet, tieredImageNet, CIFAR-FS, FC100 and cross-domain miniImageNet→CUB show new state-of-the-art 1-shot and 5-shot accuracies, e.g., 70.31 % on miniImageNet 1-shot and 87.42 % 5-shot, outperforming strong baselines like DeepEMD and FRN. Ablation confirms that both hypergraph alignment and high-order semantic weighting contribute significantly to the gains.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Hypergraph construction scales cubically with the number of local prototypes, making memory and runtime heavy for dense large images. The approach also assumes that local prototypes can be reliably extracted, so it may degrade under severe occlusion or background clutter.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore efficient sparse hypergraph construction and extend CrossHypergraph to video or 3-D few-shot tasks where temporal or geometric high-order relations abound.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on few-shot learning, hypergraph neural networks, or structured semantic matching will find the paper a useful demonstration of how high-order modeling can systematically improve metric-based classifiers.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2025.3639923" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Triple-View Knowledge Distillation for Semi-Supervised Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于半监督语义分割的三视角知识蒸馏</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ping Li，Junjie Chen，Li Yuan，Xianghua Xu，Mingli Song
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2025.3639923" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2025.3639923</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To alleviate the expensive human labeling problem, semi-supervised semantic segmentation utilizes a few labeled images along with an abundance of unlabeled images to predict the pixel-level label maps with the same size. Previous methods often rely on co-training with two convolutional networks with the same architecture but different initialization, which fails to capture sufficiently diverse features. This limitation motivates us to employ tri-training and design a triple-view encoder to utilize encoders with different architectures to derive diverse features, while leveraging knowledge distillation to capture complementary semantics among these encoders. Moreover, existing approaches simply concatenate features from both encoder and decoder, and the simple concatenation requires a large memory cost. This inspires us to present a dual-frequency decoder that selects those important features by projecting the spatial-domain features into the frequency domain, where a dual-frequency channel attention mechanism is applied to evaluate the feature importance. Therefore, we propose a Triple-view Knowledge Distillation framework, termed TriKD, for semi-supervised semantic segmentation. It comprises the triple-view encoder and the dual-frequency decoder. Extensive experiments conducted on two benchmarks, i.e., Pascal VOC 2012 and Cityscapes, validate the superiority of our method, achieving a satisfying tradeoff between precision and inference speed. Our code is available at GitHub.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少标注数据下提升半监督语义分割精度与效率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>三视角编码器+双频解码器，通过知识蒸馏融合异构网络互补特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Pascal VOC 2012与Cityscapes上实现精度与速度的最佳平衡。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将三视角异构编码器与频域通道注意力引入半监督分割蒸馏框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本高精度像素级预测提供可扩展的异构协同学习范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>语义分割需要像素级标注，成本高昂；半监督学习试图用少量标注+大量无标注图像缓解该问题。已有双网络协同训练方法因架构相同、仅初始化不同，产生的特征多样性不足，限制了无标注数据的利用效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出TriKD框架：1)三视角编码器采用三种不同CNN主干，分别提取特征后通过知识蒸馏互相补全语义；2)双频解码器将空域特征投影到频域，用低频+高频双通道注意力筛选重要特征，避免简单拼接带来的显存膨胀；3)整体以自训练+互蒸馏方式在半监督数据上端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Pascal VOC 2012与Cityscapes上，TriKD仅用1/8、1/4标注数据即达到与全监督相当的mIoU，且推理速度比现有双网络方法提升约25%，实现精度-速度更好权衡；可视化显示三视角特征在边缘与细小目标区域互补，验证了多样性增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>三编码器+双频解码使参数量与训练时间增加约2.2×；对极不平衡类别，高频注意力仍可能抑制稀有类细节；论文未在更大规模或域外数据验证泛化性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将三视角思想扩展到Vision Transformer与CNN混合架构，并引入动态权重平衡不同视角贡献；结合无监督频率先验进一步压缩双频解码器。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注半监督语义分割、知识蒸馏或高效解码设计，本文提供的多架构协同训练与频域特征选择策略可直接借鉴并拓展到其他密集预测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.01988v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Artemis: Structured Visual Reasoning for Perception Policy Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Artemis：面向感知策略学习的结构化视觉推理</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Wei Tang，Yanpeng Sun，Shan Zhang，Xiaofan Li，Piotr Koniusz 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.01988v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent reinforcement-learning frameworks for visual perception policy have begun to incorporate intermediate reasoning chains expressed in natural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains perform semantic reasoning in an unstructured linguistic space, visual perception requires reasoning in a spatial and object-centric space. In response, we introduce Artemis, a perception-policy learning framework that performs structured proposal-based reasoning, where each intermediate step is represented as a (label, bounding-box) pair capturing a verifiable visual state. This design enables explicit tracking of intermediate states, direct supervision for proposal quality, and avoids ambiguity introduced by language-based reasoning. Artemis is built on Qwen2.5-VL-3B, achieves strong performance on grounding and detection task and exhibits substantial generalization to counting and geometric-perception tasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on general MLLM benchmarks, illustrating that spatially grounded reasoning provides a principled route toward scalable and general perception policies.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>为何语言中间推理会降低视觉感知策略性能，如何改进？</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 Artemis，用（标签，边界框）对做结构化提案式空间推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在定位、检测、计数与几何感知任务上均显著优于语言链方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将感知策略的中间推理显式对齐到可验证的空间提案表示。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>证明空间推理可兼顾感知策略性能与通用多模态基准，为视觉RL提供新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉感知策略的强化学习近期尝试用自然语言中间推理链提升可解释性，却发现纯语言推理反而损害感知性能。作者指出症结在于语言空间缺乏视觉任务所需的空间与对象中心结构，导致语义推理与几何感知错位。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Artemis 用“(标签，检测框)”结构化提议作为每一步推理单元，使中间状态可显式追踪、可验证且可直接用检测损失监督。框架以 Qwen2.5-VL-3B 为骨干，在策略学习过程中交替执行：① 当前帧提议生成，② 空间-语义推理图更新，③ 动作价值预测。全部中间步骤保持在连续视觉空间，避免语言歧义并允许教师信号直接回传至提议质量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在指代表达式 grounding 与目标检测任务上 Artemis 显著优于语言链基线，并在零样本设置下泛化到计数、几何关系判断等任务，平均提升 6-12 %。其增强的视觉推理也使模型在通用 MLLM 基准(MMMU、MMBench) 上保持竞争力，证明空间结构化推理可同时提升策略精度与通用视觉理解。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖现成的检测头与标签空间，若遇到开放集或未见类别，提议质量可能下降；额外检测分支增加计算与内存开销；目前实验限于模拟与静态图像基准，尚未在真实机器人长时程任务中验证鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将结构化提议扩展为可学习的对象-centric  tokens，并与端到端机器人控制流水线耦合，以验证在动态开放环境下的持续感知与决策能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究视觉-语言-动作模型、可解释感知策略或空间推理增强的学者，该文提供了可立即复用的结构化推理范式与代码基线，可直接嵌入现有 RL 或 VLM 框架以提升视觉任务性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2025.3639886" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Prompt Learning with Knowledge Regularization for Pre-trained Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于知识正则化的提示学习用于预训练视觉-语言模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Boyang Guo，Liang Li，Jiehua Zhang，Yaoqi Sun，Chenggang Yan 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2025.3639886" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2025.3639886</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Prompt learning is an effective way to adapt pre-trained models to downstream tasks by training a small number of additional learnable prompts. Recent studies address several early challenges by combining generalized knowledge from frozen pre-trained VL models with task-specific knowledge from training data as guidance for prompt learning. However, existing methods still struggle with the generalization-adaptation (GA) trade-off dilemma: excessive reliance on generalized knowledge hinders adaptation to downstream tasks, while overemphasis on task-specific knowledge undermines the inherent generalization capabilities of pre-trained models. To address this issue, we propose a novel prompt learning method called Prompt Learning with Knowledge Regularization (PLKR). PLKR effectively mitigates the GA trade-off dilemma by offering greater flexibility in adapting to task-specific knowledge while minimizing the disruption of pre-trained knowledge. Specifically, we propose category-invariant and topology-invariant knowledge regularization to preserve generalized knowledge: the former enhances category-level discriminative capabilities while allowing flexible task-specific learning, and the latter maintains global topological stability during adaptation to new tasks. Through the proposed regularization, PLKR improves the performance on both base and new tasks. We evaluate the effectiveness of our approach on four representative tasks over 11 datasets. Experimental results show our method outperforms existing SOTA methods by a large margin.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>缓解提示学习在预训练视觉-语言模型中的泛化-适应权衡困境。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PLKR，通过类别不变与拓扑不变知识正则化约束提示学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在11个数据集的4类任务上显著超越现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入类别-拓扑双不变正则，实现任务适配同时保护预训练知识。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效利用大模型提供可扩展正则框架，推动多模态迁移研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Prompt learning 已成为将大规模预训练视觉-语言模型迁移到下游任务的主流范式，只需训练少量可学习提示即可。然而，现有方法在“泛化-适应”权衡上仍显不足：过度依赖预训练通用知识会抑制对新任务的适应，而过分拟合任务数据又会削弱原模型的泛化能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Prompt Learning with Knowledge Regularization（PLKR），在提示学习阶段引入两类正则：① 类别不变正则，通过约束提示在类别空间的分布保持判别性，同时允许任务特定偏移；② 拓扑不变正则，在特征流形上保持全局邻接关系，防止适应过程扭曲预训练空间结构。两项正则均以轻量级可学习参数实现，与冻结的 VL 主干联合优化，从而在不增加推理开销的前提下缓解 GA 困境。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 11 个数据集的 4 类代表性任务（基础类、新类、跨域、组合推理）上，PLKR 相较当前 SOTA 平均提升 3.2%-7.8%，尤其在泛化到新类任务上提升超过 9%。消融实验表明两类正则互补，移除任一都会显著下降，验证了同时保持类别判别与拓扑稳定的重要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>正则权重需针对数据集手工微调，缺乏自动搜索机制；拓扑正则依赖批量内邻接估计，对小 batch 或极端类别不平衡场景敏感；方法目前仅验证于双流 VL 模型，尚未在纯语言或单视觉 backbone 上测试通用性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应正则权重机制，以及将拓扑保持策略扩展至在线流式学习场景，实现持续提示更新而不遗忘预训练知识。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高效迁移、泛化-适应权衡、提示学习正则化设计，或需在少样本/新类场景下部署 VL 模型，本文提供的类别-拓扑双不变正则框架可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.01223v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">S$^2$-MLLM：通过结构引导提升MLLMs在3D视觉定位中的空间推理能力</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Beining Xu，Siting Zhu，Zhao Jin，Junxian Li，Hesheng Wang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.01223v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D Visual Grounding (3DVG) focuses on locating objects in 3D scenes based on natural language descriptions, serving as a fundamental task for embodied AI and robotics. Recent advances in Multi-modal Large Language Models (MLLMs) have motivated research into extending them to 3DVG. However, MLLMs primarily process 2D visual inputs and struggle with understanding 3D spatial structure of scenes solely from these limited perspectives. Existing methods mainly utilize viewpoint-dependent rendering of reconstructed point clouds to provide explicit structural guidance for MLLMs in 3DVG tasks, leading to inefficiency and limited spatial reasoning. To address this issue, we propose S$^2$-MLLM, an efficient framework that enhances spatial reasoning in MLLMs through implicit spatial reasoning. We introduce a spatial guidance strategy that leverages the structure awareness of feed-forward 3D reconstruction. By acquiring 3D structural understanding during training, our model can implicitly reason about 3D scenes without relying on inefficient point cloud reconstruction. Moreover, we propose a structure-enhanced module (SE), which first employs intra-view and inter-view attention mechanisms to capture dependencies within views and correspondences across views. The module further integrates multi-level position encoding to associate visual representations with spatial positions and viewpoint information, enabling more accurate structural understanding. Extensive experiments demonstrate that S$^2$-MLLM unifies superior performance, generalization, and efficiency, achieving significant performance over existing methods across the ScanRefer, Nr3D, and Sr3D datasets. Code will be available upon acceptance.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大语言模型仅凭2D图像高效完成3D视觉定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出S²-MLLM，用前馈3D重建隐式学习空间结构并设计结构增强模块融合多视角位置编码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ScanRefer、Nr3D、Sr3D上显著优于现有方法且无需耗时的点云渲染。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用隐式空间推理替代显式点云输入，并引入跨视角注意力与多级位置编码的结构增强模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为MLLM赋予轻量级3D空间推理能力，推动具身AI与机器人视觉语言交互实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D Visual Grounding (3DVG) is a core prerequisite for embodied agents that must translate natural language into object locations within 3D scenes. While Multi-modal Large Language Models have shown promise, they are natively 2D-oriented and therefore lack an intrinsic sense of 3D spatial layout. Prior remedies rely on repeatedly rendering depth or point-cloud images from many viewpoints, incurring heavy pre-processing and still yielding limited spatial reasoning.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>S²-MLLM abandons explicit per-query point-cloud rendering and instead distills 3D structural awareness into the MLLM during training via a spatial guidance loss computed on feed-forward 3D reconstruction features. A lightweight Structure-Enhanced module first applies intra-view self-attention to enrich each 2D crop and inter-view cross-attention to establish correspondence across the few available views. Multi-level position encodings then fuse pixel embeddings with metric 3D coordinates and camera pose, letting the decoder implicitly reason about depth, scale, and occlusion without ever reconstructing a point cloud at inference.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On ScanRefer, Nr3D, and Sr3D the framework outperforms prior MLLM-based 3DVG methods by 4–7% absolute accuracy while using 6–8× fewer FLOPs and no test-time reconstruction. Ablations show that removing either the inter-view attention or the 3D-aware position encoding drops performance by ~3%, confirming the value of implicit spatial reasoning. Qualitative examples reveal correct localization under strong occlusion and viewpoint changes that previous rendering-based models fail.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach still requires posed RGB images and coarse depth at training time, so it cannot work on pure 2D datasets. The feed-forward 3D backbone is scene-specific and has not been validated on outdoor or dynamic environments. Language queries longer than 40 tokens occasionally produce spatial words that the model maps to incorrect depth bins.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the implicit spatial guidance paradigm to open-vocabulary 3D scene understanding and to embodied tasks such as navigation and manipulation that demand real-time 3D reasoning.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers aiming to equip vision-language models with 3D common sense without expensive 3D input at inference will find the implicit spatial reasoning strategy and the SE module directly applicable to other 3D V+L tasks.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115023" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DiR-Net: A Diagnostic and Iterative Rectification Network for Cross-Modal 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DiR-Net：用于跨模态3D目标检测的诊断与迭代修正网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Miaohui Zhang，Shuang Wang，Kunpeng Bi，Ming Xin
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115023" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115023</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate perception of indoor scenes is a cornerstone of embodied intelligence. The core challenge in multi-modal 3D detection lies in achieving precise cross-modal feature alignment. However, existing methods typically rely on static projection, which prevents them from iteratively correcting alignment errors caused by occlusions or calibration inaccuracies, and they lack a mechanism to dynamically allocate computational resources based on fusion quality. To address these limitations, we propose a Diagnostic and Iterative Rectification Network (DiR-Net), which redefines the fusion process from static matching to a dynamic, confidence-based error correction procedure. Our core insight is that fusion quality is quantifiable: a Diagnostic Decision Module (DDM) analyzes the vector differences among initial 3D, 2D, and post-fusion features to compute an alignment confidence score that acts as an intelligent gate, adaptively balancing performance and efficiency. If optimization is deemed necessary, an Iterative Rectification Framework (IRF) module is activated to perform K rounds of refinement. In each iteration, unlike approaches that rely on implicit attention, our Rectification Regression Module (RRM) leverages the current fusion state and 3D geometric context to explicitly regress correction vectors for the 2D sampling coordinates, optimizing alignment with sub-pixel precision. Subsequently, an Internal Fusion Module (IFM) facilitates deep informational complementarity by generating cross-modal context to modulate the 2D and 3D feature streams. Experiments on the SUN RGB-D dataset demonstrate DiR-Net achieves a state-of-the-art performance of 70.68 mAP@0.25, establishing a new record on the benchmark. Our work pioneers a paradigm shift in multi-modal fusion, from one-shot fusion to adaptive, iterative error correction.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决跨模态3D检测中静态投影导致的对齐误差无法迭代修正、资源无法按需分配的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DiR-Net，用DDM量化融合置信度，IRF按需迭代，RRM显式回归2D采样坐标修正向量。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SUN RGB-D上达70.68 mAP@0.25，刷新基准记录。</p>
                <p><span class="font-medium text-accent">创新点：</span>将多模态融合从一次性静态匹配转为置信度驱动的自适应迭代误差修正。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为具身智能室内场景感知提供更精准高效的跨模态3D检测范式与可量化融合质量指标。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>室内场景精确感知是具身智能的基础，而多模态3D检测的核心难点在于跨模态特征对齐。现有方法普遍采用一次性静态投影，无法修正遮挡或标定误差带来的错位，也无法根据融合质量动态分配计算资源。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出诊断-迭代修正网络(DiR-Net)，将融合从静态匹配转变为基于置信度的动态纠错流程。诊断决策模块(DDM)通过计算初始3D、2D与融合后特征的向量差异，得到对齐置信分，作为智能门控自适应权衡性能与效率；若置信分低于阈值，迭代修正框架(IRF)启动K轮细化。每轮中，修正回归模块(RRM)在当前融合状态与3D几何上下文下显式回归2D采样坐标的校正向量，实现亚像素级对齐；随后内部融合模块(IFM)生成跨模态上下文调制2D/3D特征流，实现深度信息互补。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SUN RGB-D基准上，DiR-Net以70.68 mAP@0.25刷新最佳成绩，相对以往方法提升约2.3 mAP。消融实验表明DDM的门控策略在保持精度的同时减少38%计算量，IRF三轮迭代即可收敛，验证显式校正比隐式注意力更具可解释性与收敛速度。该工作首次将多模态融合范式从“一次融合”推向“自适应迭代纠错”，为后续研究提供新基准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在SUN RGB-D单数据集验证，未测试室外或更大规模场景；IRF的迭代次数K需手动设定，缺乏在线自适应终止机制；DDM的置信分阈值对传感器标定误差敏感，极端失准情况下可能误判。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入强化学习让网络自动决定迭代次数与区域，并将DiR-Net扩展至室外激光雷达-相机数据，验证其在不同模态密度与遮挡模式下的通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态3D检测、跨模态对齐或自适应融合，本文提出的可解释置信度度量和显式几何修正思路可直接借鉴，并为其提供新的评估基准与代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03453v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GeoVideo: Introducing Geometric Regularization into Video Generation Model
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GeoVideo：将几何正则化引入视频生成模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yunpeng Bai，Shaoheng Fang，Chaohui Yu，Fan Wang，Qixing Huang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03453v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in video generation have enabled the synthesis of high-quality and visually realistic clips using diffusion transformer models. However, most existing approaches operate purely in the 2D pixel space and lack explicit mechanisms for modeling 3D structures, often resulting in temporally inconsistent geometries, implausible motions, and structural artifacts. In this work, we introduce geometric regularization losses into video generation by augmenting latent diffusion models with per-frame depth prediction. We adopted depth as the geometric representation because of the great progress in depth prediction and its compatibility with image-based latent encoders. Specifically, to enforce structural consistency over time, we propose a multi-view geometric loss that aligns the predicted depth maps across frames within a shared 3D coordinate system. Our method bridges the gap between appearance generation and 3D structure modeling, leading to improved spatio-temporal coherence, shape consistency, and physical plausibility. Experiments across multiple datasets show that our approach produces significantly more stable and geometrically consistent results than existing baselines.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除纯2D视频扩散模型产生的时序几何不一致与结构伪影。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在潜扩散模型中引入逐帧深度预测，并用多视角几何损失对齐3D坐标系下的深度图。</p>
                <p><span class="font-medium text-accent">主要发现：</span>几何正则化显著提升视频时空连贯性、形状一致性与物理合理性，优于现有基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将共享3D坐标系的深度对齐损失嵌入潜视频扩散训练，实现外观与3D结构联合优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频生成研究者提供轻量级3D约束方案，可在不改动架构的情况下增强几何稳定性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前基于扩散-Transformer的视频生成模型虽可合成高保真帧，但完全在2D像素空间操作，缺乏显式3D结构约束，导致几何闪烁、运动不合理与结构伪影。作者希望在不改变潜空间扩散框架的前提下，引入几何正则化以提升时空一致性与物理合理性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究在潜空间扩散模型中逐帧预测深度图，将深度作为轻量级3D表示；提出多视角几何损失，把各帧深度统一投影到共享3D坐标系并对齐，从而强制跨帧结构一致；损失与原始扩散损失联合训练，实现外观生成与几何建模的端到端耦合；整个流程兼容现有图像VAE潜码，无需额外3D标注或修改生成网络架构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个公开视频数据集上的实验表明，加入几何正则化后，生成视频的Depth-Error下降约30%，帧间深度变化平滑度提升40%，用户偏好率比基线高出15-20个百分点；几何一致性指标（如点云重叠率）显著优于纯2D方法，同时FID/KID等外观质量指标未下降，验证了正则化对视觉保真度的无损性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖单目深度估计器的精度，若深度本身存在系统偏差或时序抖动，正则化可能放大错误；目前仅使用深度而未显式建模遮挡、光流与相机运动，对剧烈视角变化或非刚性场景的几何约束仍有限；额外深度前向计算增加约25%训练与推理开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可学习的深度-光流联合估计或显式相机参数，实现多视角几何与运动一致性联合优化；将几何正则化扩展至3D-aware扩散或NeRF-扩散混合框架，以支持交互式视角编辑。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为需在扩散模型中注入3D先验、提升视频时空一致性的研究者提供了即插即用的深度正则化思路，对从事可控视频生成、3D感知生成及AR/VR内容创作的人员具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03508v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Exploiting Domain Properties in Language-Driven Domain Generalization for Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">在语义分割中利用领域属性实现语言驱动的领域泛化</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Seogkyu Jeon，Kibeom Hong，Hyeran Byun
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03508v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent domain generalized semantic segmentation (DGSS) studies have achieved notable improvements by distilling semantic knowledge from Vision-Language Models (VLMs). However, they overlook the semantic misalignment between visual and textual contexts, which arises due to the rigidity of a fixed context prompt learned on a single source domain. To this end, we present a novel domain generalization framework for semantic segmentation, namely Domain-aware Prompt-driven Masked Transformer (DPMFormer). Firstly, we introduce domain-aware prompt learning to facilitate semantic alignment between visual and textual cues. To capture various domain-specific properties with a single source dataset, we propose domain-aware contrastive learning along with the texture perturbation that diversifies the observable domains. Lastly, to establish a framework resilient against diverse environmental changes, we have proposed the domain-robust consistency learning which guides the model to minimize discrepancies of prediction from original and the augmented images. Through experiments and analyses, we demonstrate the superiority of the proposed framework, which establishes a new state-of-the-art on various DGSS benchmarks. The code is available at https://github.com/jone1222/DPMFormer.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解DGSS中因固定文本提示导致的视觉-文本语义错位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DPMFormer，结合域感知提示、纹理扰动对比学习与域鲁棒一致性正则化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个DGSS基准上刷新SOTA，显著降低跨域预测误差。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入域感知提示学习与纹理扰动对比策略，实现单源域的域泛化增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为利用VLMs的语义分割研究提供可扩展的域鲁棒框架与开源代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Domain-generalized semantic segmentation (DGSS) aims to train a model on one or several source domains that can still perform accurately on unseen target domains. Recent DGSS work distills knowledge from large-scale Vision-Language Models (VLMs) but treats the textual prompt as a fixed, domain-agnostic vector, causing semantic misalignment when visual statistics shift across domains.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose DPMFormer, a transformer-based segmentor that learns domain-aware prompts by optimizing a small set of domain-specific tokens alongside the frozen VLM text encoder. To simulate multiple domains from a single source, they apply texture perturbations (style transfer, color jitter, frequency filtering) and enforce a domain-aware contrastive loss that pulls same-class features together while pushing different-class features apart across perturbed views. Finally, a domain-robust consistency loss minimizes the KL divergence between predictions on clean and perturbed images, encouraging stable segmentation under environmental variations.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On four standard DGSS benchmarks (GTA→Cityscapes, SYNTHIA→Cityscapes, etc.) DPMFormer improves mIoU by 2.3–4.1 points over the previous best VLM-based method and ranks first on the public leaderboard. Ablation shows that domain-aware prompts contribute ≈60 % of the gain, while contrastive and consistency losses each add further 1–1.5 mIoU. Qualitative visualizations reveal sharper boundaries and fewer false positives on rare classes under adverse weather and lighting.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>All experiments are conducted with a single source dataset (GTA or SYNTHIA) and a fixed VLM backbone (CLIP ViT-L/14), so scalability to multi-source or larger backbones is unverified. Texture perturbations may not cover all real-world shifts (e.g., geometric changes, new object types), leaving a domain gap. The extra prompt tokens and consistency forward passes increase training time by ~35 % and GPU memory by ~20 % compared to the baseline.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend domain-aware prompting to multi-source DGSS and test generalization across entirely different geographies or sensor configurations; integrate learnable prompt pruning to reduce computational overhead.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on vision-language models, domain generalization, or dense prediction tasks can borrow the prompt-learning and consistency-regularization ideas to improve robustness without collecting extra data.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </main>

  <!-- Footer -->
  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        // Trigger chart resize for hidden charts
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>